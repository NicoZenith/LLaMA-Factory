5: ➤ Node 5 / 8 starting training…
2: ➤ Node 2 / 8 starting training…
1: ➤ Node 1 / 8 starting training…
6: ➤ Node 6 / 8 starting training…
4: ➤ Node 4 / 8 starting training…
3: ➤ Node 3 / 8 starting training…
0: ➤ Node 0 / 8 starting training…
7: ➤ Node 7 / 8 starting training…
5: [2025-07-11 11:47:15,705] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
4: [2025-07-11 11:47:15,711] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
3: [2025-07-11 11:47:15,712] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
6: [2025-07-11 11:47:15,717] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
7: [2025-07-11 11:47:15,729] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
5: df: /users/ndeperr/.triton/autotune: No such file or directory
6: df: /users/ndeperr/.triton/autotune: No such file or directory
3: df: /users/ndeperr/.triton/autotune: No such file or directory
7: df: /users/ndeperr/.triton/autotune: No such file or directory
4: df: /users/ndeperr/.triton/autotune: No such file or directory
0: [2025-07-11 11:47:16,294] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2: [2025-07-11 11:47:16,315] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
0: df: /users/ndeperr/.triton/autotune: No such file or directory
2: df: /users/ndeperr/.triton/autotune: No such file or directory
1: [2025-07-11 11:47:16,896] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
1: df: /users/ndeperr/.triton/autotune: No such file or directory
0: [INFO|2025-07-11 11:47:22] llamafactory.cli:143 >> Initializing 4 distributed tasks at: nid007116:29500
0: [INFO|2025-07-11 11:47:22] llamafactory.cli:143 >> Multi-node training enabled: num nodes: 8, node rank: 0
4: [INFO|2025-07-11 11:47:22] llamafactory.cli:143 >> Initializing 4 distributed tasks at: nid007116:29500
4: [INFO|2025-07-11 11:47:22] llamafactory.cli:143 >> Multi-node training enabled: num nodes: 8, node rank: 4
5: [INFO|2025-07-11 11:47:23] llamafactory.cli:143 >> Initializing 4 distributed tasks at: nid007116:29500
5: [INFO|2025-07-11 11:47:23] llamafactory.cli:143 >> Multi-node training enabled: num nodes: 8, node rank: 5
2: [INFO|2025-07-11 11:47:23] llamafactory.cli:143 >> Initializing 4 distributed tasks at: nid007116:29500
2: [INFO|2025-07-11 11:47:23] llamafactory.cli:143 >> Multi-node training enabled: num nodes: 8, node rank: 2
7: [INFO|2025-07-11 11:47:23] llamafactory.cli:143 >> Initializing 4 distributed tasks at: nid007116:29500
7: [INFO|2025-07-11 11:47:23] llamafactory.cli:143 >> Multi-node training enabled: num nodes: 8, node rank: 7
6: [INFO|2025-07-11 11:47:23] llamafactory.cli:143 >> Initializing 4 distributed tasks at: nid007116:29500
6: [INFO|2025-07-11 11:47:23] llamafactory.cli:143 >> Multi-node training enabled: num nodes: 8, node rank: 6
3: [INFO|2025-07-11 11:47:23] llamafactory.cli:143 >> Initializing 4 distributed tasks at: nid007116:29500
3: [INFO|2025-07-11 11:47:23] llamafactory.cli:143 >> Multi-node training enabled: num nodes: 8, node rank: 3
0: W0711 11:47:23.749000 70368977946720 torch/distributed/run.py:793] 
0: W0711 11:47:23.749000 70368977946720 torch/distributed/run.py:793] *****************************************
0: W0711 11:47:23.749000 70368977946720 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
0: W0711 11:47:23.749000 70368977946720 torch/distributed/run.py:793] *****************************************
4: W0711 11:47:23.935000 70369684949088 torch/distributed/run.py:793] 
4: W0711 11:47:23.935000 70369684949088 torch/distributed/run.py:793] *****************************************
4: W0711 11:47:23.935000 70369684949088 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
4: W0711 11:47:23.935000 70369684949088 torch/distributed/run.py:793] *****************************************
5: W0711 11:47:23.942000 70369139099744 torch/distributed/run.py:793] 
5: W0711 11:47:23.942000 70369139099744 torch/distributed/run.py:793] *****************************************
5: W0711 11:47:23.942000 70369139099744 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
5: W0711 11:47:23.942000 70369139099744 torch/distributed/run.py:793] *****************************************
2: W0711 11:47:23.950000 70369729251424 torch/distributed/run.py:793] 
2: W0711 11:47:23.950000 70369729251424 torch/distributed/run.py:793] *****************************************
2: W0711 11:47:23.950000 70369729251424 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
2: W0711 11:47:23.950000 70369729251424 torch/distributed/run.py:793] *****************************************
7: W0711 11:47:24.196000 70369193887840 torch/distributed/run.py:793] 
7: W0711 11:47:24.196000 70369193887840 torch/distributed/run.py:793] *****************************************
7: W0711 11:47:24.196000 70369193887840 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
7: W0711 11:47:24.196000 70369193887840 torch/distributed/run.py:793] *****************************************
6: W0711 11:47:24.452000 70369706707040 torch/distributed/run.py:793] 
6: W0711 11:47:24.452000 70369706707040 torch/distributed/run.py:793] *****************************************
6: W0711 11:47:24.452000 70369706707040 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
6: W0711 11:47:24.452000 70369706707040 torch/distributed/run.py:793] *****************************************
1: [INFO|2025-07-11 11:47:24] llamafactory.cli:143 >> Initializing 4 distributed tasks at: nid007116:29500
1: [INFO|2025-07-11 11:47:24] llamafactory.cli:143 >> Multi-node training enabled: num nodes: 8, node rank: 1
3: W0711 11:47:24.565000 70369175865440 torch/distributed/run.py:793] 
3: W0711 11:47:24.565000 70369175865440 torch/distributed/run.py:793] *****************************************
3: W0711 11:47:24.565000 70369175865440 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
3: W0711 11:47:24.565000 70369175865440 torch/distributed/run.py:793] *****************************************
1: W0711 11:47:25.484000 70369164920928 torch/distributed/run.py:793] 
1: W0711 11:47:25.484000 70369164920928 torch/distributed/run.py:793] *****************************************
1: W0711 11:47:25.484000 70369164920928 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
1: W0711 11:47:25.484000 70369164920928 torch/distributed/run.py:793] *****************************************
5: [2025-07-11 11:47:31,541] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
5: [2025-07-11 11:47:31,543] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2: [2025-07-11 11:47:31,788] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2: [2025-07-11 11:47:31,792] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2: [2025-07-11 11:47:31,793] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
0: [2025-07-11 11:47:31,968] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
0: [2025-07-11 11:47:31,970] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
0: [2025-07-11 11:47:31,971] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
7: [2025-07-11 11:47:32,003] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
7: [2025-07-11 11:47:32,016] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
7: [2025-07-11 11:47:32,016] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
4: [2025-07-11 11:47:32,183] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
4: [2025-07-11 11:47:32,186] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
4: [2025-07-11 11:47:32,187] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
1: [2025-07-11 11:47:32,374] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
1: [2025-07-11 11:47:32,379] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
1: [2025-07-11 11:47:32,380] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2: [2025-07-11 11:47:32,804] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
5: [2025-07-11 11:47:32,822] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
5: [2025-07-11 11:47:32,824] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
6: [2025-07-11 11:47:32,841] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
6: [2025-07-11 11:47:32,841] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
6: [2025-07-11 11:47:32,843] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
6: [2025-07-11 11:47:32,845] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
3: [2025-07-11 11:47:33,000] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
3: [2025-07-11 11:47:33,001] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
3: [2025-07-11 11:47:33,002] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
3: [2025-07-11 11:47:33,004] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
0: [2025-07-11 11:47:33,125] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
4: [2025-07-11 11:47:33,268] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
7: [2025-07-11 11:47:33,275] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
1: [2025-07-11 11:47:33,408] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
5: [2025-07-11 11:47:34,150] [INFO] [comm.py:669:init_distributed] cdb=None
5: [2025-07-11 11:47:34,150] [INFO] [comm.py:669:init_distributed] cdb=None
5: [W711 11:47:34.070120131 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
5: [W711 11:47:34.070417530 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
2: [2025-07-11 11:47:34,451] [INFO] [comm.py:669:init_distributed] cdb=None
2: [2025-07-11 11:47:34,452] [INFO] [comm.py:669:init_distributed] cdb=None
2: [W711 11:47:34.461122115 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
2: [W711 11:47:34.461737682 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
2: [2025-07-11 11:47:34,557] [INFO] [comm.py:669:init_distributed] cdb=None
2: [W711 11:47:34.567645150 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
0: [2025-07-11 11:47:35,546] [INFO] [comm.py:669:init_distributed] cdb=None
0: [2025-07-11 11:47:35,546] [INFO] [comm.py:669:init_distributed] cdb=None
0: [2025-07-11 11:47:35,546] [INFO] [comm.py:669:init_distributed] cdb=None
7: [2025-07-11 11:47:35,548] [INFO] [comm.py:669:init_distributed] cdb=None
7: [2025-07-11 11:47:35,548] [INFO] [comm.py:669:init_distributed] cdb=None
0: [W711 11:47:35.465233282 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
0: [W711 11:47:35.465233346 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
0: [W711 11:47:35.465297920 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
7: [W711 11:47:35.484576149 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
7: [W711 11:47:35.484644147 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
7: [2025-07-11 11:47:35,552] [INFO] [comm.py:669:init_distributed] cdb=None
4: [2025-07-11 11:47:35,555] [INFO] [comm.py:669:init_distributed] cdb=None
4: [2025-07-11 11:47:35,555] [INFO] [comm.py:669:init_distributed] cdb=None
4: [2025-07-11 11:47:35,555] [INFO] [comm.py:669:init_distributed] cdb=None
7: [W711 11:47:35.489437791 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
4: [W711 11:47:35.235421659 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
4: [W711 11:47:35.235421563 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
4: [W711 11:47:35.235421947 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
1: [2025-07-11 11:47:35,644] [INFO] [comm.py:669:init_distributed] cdb=None
1: [2025-07-11 11:47:35,644] [INFO] [comm.py:669:init_distributed] cdb=None
1: [W711 11:47:35.040757250 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
1: [W711 11:47:35.040949693 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
2: [2025-07-11 11:47:35,649] [INFO] [comm.py:669:init_distributed] cdb=None
2: [W711 11:47:35.657897729 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
1: [2025-07-11 11:47:35,666] [INFO] [comm.py:669:init_distributed] cdb=None
1: [W711 11:47:35.062449703 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
6: [2025-07-11 11:47:35,772] [INFO] [comm.py:669:init_distributed] cdb=None
6: [2025-07-11 11:47:35,772] [INFO] [comm.py:669:init_distributed] cdb=None
6: [2025-07-11 11:47:35,772] [INFO] [comm.py:669:init_distributed] cdb=None
6: [W711 11:47:35.504044754 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
6: [W711 11:47:35.504298603 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
6: [W711 11:47:35.504682560 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
5: [2025-07-11 11:47:35,862] [INFO] [comm.py:669:init_distributed] cdb=None
5: [2025-07-11 11:47:35,866] [INFO] [comm.py:669:init_distributed] cdb=None
5: [W711 11:47:35.781397810 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
5: [W711 11:47:35.785104927 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
3: [2025-07-11 11:47:36,096] [INFO] [comm.py:669:init_distributed] cdb=None
3: [2025-07-11 11:47:36,096] [INFO] [comm.py:669:init_distributed] cdb=None
3: [W711 11:47:36.572080884 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
3: [W711 11:47:36.572657986 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
3: [2025-07-11 11:47:36,130] [INFO] [comm.py:669:init_distributed] cdb=None
3: [W711 11:47:36.606875653 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
7: [INFO|2025-07-11 11:47:36] llamafactory.hparams.parser:406 >> Process rank: 28, world size: 32, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
6: [2025-07-11 11:47:36,254] [INFO] [comm.py:669:init_distributed] cdb=None
3: [2025-07-11 11:47:36,256] [INFO] [comm.py:669:init_distributed] cdb=None
6: [W711 11:47:36.985517714 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
3: [W711 11:47:36.732966722 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
7: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,427 >> loading file vocab.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
7: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,427 >> loading file merges.txt from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
7: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,427 >> loading file tokenizer.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
7: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,427 >> loading file added_tokens.json from cache at None
7: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,427 >> loading file special_tokens_map.json from cache at None
7: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,427 >> loading file tokenizer_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
7: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,427 >> loading file chat_template.jinja from cache at None
0: [2025-07-11 11:47:36,487] [INFO] [comm.py:669:init_distributed] cdb=None
0: [2025-07-11 11:47:36,487] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
0: [W711 11:47:36.405289127 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
1: [2025-07-11 11:47:36,500] [INFO] [comm.py:669:init_distributed] cdb=None
1: [W711 11:47:36.897576520 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
5: [INFO|2025-07-11 11:47:36] llamafactory.hparams.parser:406 >> Process rank: 20, world size: 32, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
4: [2025-07-11 11:47:36,603] [INFO] [comm.py:669:init_distributed] cdb=None
4: [W711 11:47:36.283983004 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
1: [INFO|2025-07-11 11:47:36] llamafactory.hparams.parser:406 >> Process rank: 4, world size: 32, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
7: [INFO|tokenization_utils_base.py:2323] 2025-07-11 11:47:36,625 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
5: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,687 >> loading file vocab.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
5: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,687 >> loading file merges.txt from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
5: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,687 >> loading file tokenizer.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
5: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,687 >> loading file added_tokens.json from cache at None
5: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,687 >> loading file special_tokens_map.json from cache at None
5: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,687 >> loading file tokenizer_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
5: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,687 >> loading file chat_template.jinja from cache at None
6: [INFO|2025-07-11 11:47:36] llamafactory.hparams.parser:406 >> Process rank: 24, world size: 32, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
1: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,774 >> loading file vocab.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
1: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,774 >> loading file merges.txt from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
1: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,774 >> loading file tokenizer.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
1: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,774 >> loading file added_tokens.json from cache at None
1: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,774 >> loading file special_tokens_map.json from cache at None
1: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,774 >> loading file tokenizer_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
1: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,774 >> loading file chat_template.jinja from cache at None
7: [2025-07-11 11:47:36,830] [INFO] [comm.py:669:init_distributed] cdb=None
7: [W711 11:47:36.767620568 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
6: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,880 >> loading file vocab.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
6: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,880 >> loading file merges.txt from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
6: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,880 >> loading file tokenizer.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
6: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,880 >> loading file added_tokens.json from cache at None
6: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,880 >> loading file special_tokens_map.json from cache at None
6: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,880 >> loading file tokenizer_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
6: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:36,880 >> loading file chat_template.jinja from cache at None
5: [INFO|tokenization_utils_base.py:2323] 2025-07-11 11:47:36,902 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
7: [INFO|image_processing_base.py:380] 2025-07-11 11:47:37,001 >> loading configuration file preprocessor_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
1: [INFO|tokenization_utils_base.py:2323] 2025-07-11 11:47:37,003 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
3: [INFO|2025-07-11 11:47:37] llamafactory.hparams.parser:406 >> Process rank: 12, world size: 32, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
6: [INFO|tokenization_utils_base.py:2323] 2025-07-11 11:47:37,069 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
7: [INFO|image_processing_base.py:380] 2025-07-11 11:47:37,119 >> loading configuration file preprocessor_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
7: [INFO|image_processing_base.py:433] 2025-07-11 11:47:37,120 >> Image processor Qwen2VLImageProcessorFast {
7:   "crop_size": null,
7:   "data_format": "channels_first",
7:   "default_to_square": true,
7:   "device": null,
7:   "do_center_crop": null,
7:   "do_convert_rgb": true,
7:   "do_normalize": true,
7:   "do_rescale": true,
7:   "do_resize": true,
7:   "image_mean": [
7:     0.48145466,
7:     0.4578275,
7:     0.40821073
7:   ],
7:   "image_processor_type": "Qwen2VLImageProcessorFast",
7:   "image_std": [
7:     0.26862954,
7:     0.26130258,
7:     0.27577711
7:   ],
7:   "input_data_format": null,
7:   "max_pixels": 12845056,
7:   "merge_size": 2,
7:   "min_pixels": 3136,
7:   "patch_size": 14,
7:   "processor_class": "Qwen2_5_VLProcessor",
7:   "resample": 3,
7:   "rescale_factor": 0.00392156862745098,
7:   "return_tensors": null,
7:   "size": {
7:     "longest_edge": 12845056,
7:     "shortest_edge": 3136
7:   },
7:   "temporal_patch_size": 2
7: }
7: 
7: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,241 >> loading file vocab.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
7: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,241 >> loading file merges.txt from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
7: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,241 >> loading file tokenizer.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
7: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,241 >> loading file added_tokens.json from cache at None
7: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,241 >> loading file special_tokens_map.json from cache at None
7: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,241 >> loading file tokenizer_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
7: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,241 >> loading file chat_template.jinja from cache at None
2: [INFO|2025-07-11 11:47:37] llamafactory.hparams.parser:406 >> Process rank: 8, world size: 32, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
3: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,347 >> loading file vocab.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
3: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,347 >> loading file merges.txt from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
3: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,348 >> loading file tokenizer.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
3: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,348 >> loading file added_tokens.json from cache at None
3: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,348 >> loading file special_tokens_map.json from cache at None
3: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,348 >> loading file tokenizer_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
3: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,348 >> loading file chat_template.jinja from cache at None
7: [INFO|tokenization_utils_base.py:2323] 2025-07-11 11:47:37,410 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,413 >> loading file vocab.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
2: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,413 >> loading file merges.txt from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
2: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,413 >> loading file tokenizer.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
2: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,413 >> loading file added_tokens.json from cache at None
2: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,413 >> loading file special_tokens_map.json from cache at None
2: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,413 >> loading file tokenizer_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
2: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,413 >> loading file chat_template.jinja from cache at None
1: [INFO|image_processing_base.py:380] 2025-07-11 11:47:37,467 >> loading configuration file preprocessor_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
5: [INFO|image_processing_base.py:380] 2025-07-11 11:47:37,467 >> loading configuration file preprocessor_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
6: [INFO|image_processing_base.py:380] 2025-07-11 11:47:37,467 >> loading configuration file preprocessor_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
5: [INFO|2025-07-11 11:47:37] llamafactory.hparams.parser:406 >> Process rank: 22, world size: 32, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
2: [INFO|2025-07-11 11:47:37] llamafactory.hparams.parser:406 >> Process rank: 9, world size: 32, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
2: [INFO|2025-07-11 11:47:37] llamafactory.hparams.parser:406 >> Process rank: 11, world size: 32, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
3: [INFO|tokenization_utils_base.py:2323] 2025-07-11 11:47:37,554 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
1: [INFO|image_processing_base.py:380] 2025-07-11 11:47:37,588 >> loading configuration file preprocessor_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
6: [INFO|image_processing_base.py:380] 2025-07-11 11:47:37,589 >> loading configuration file preprocessor_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
1: [INFO|image_processing_base.py:433] 2025-07-11 11:47:37,590 >> Image processor Qwen2VLImageProcessorFast {
1:   "crop_size": null,
1:   "data_format": "channels_first",
1:   "default_to_square": true,
1:   "device": null,
1:   "do_center_crop": null,
1:   "do_convert_rgb": true,
1:   "do_normalize": true,
1:   "do_rescale": true,
1:   "do_resize": true,
1:   "image_mean": [
1:     0.48145466,
1:     0.4578275,
1:     0.40821073
1:   ],
1:   "image_processor_type": "Qwen2VLImageProcessorFast",
1:   "image_std": [
1:     0.26862954,
1:     0.26130258,
1:     0.27577711
1:   ],
1:   "input_data_format": null,
1:   "max_pixels": 12845056,
1:   "merge_size": 2,
1:   "min_pixels": 3136,
1:   "patch_size": 14,
1:   "processor_class": "Qwen2_5_VLProcessor",
1:   "resample": 3,
1:   "rescale_factor": 0.00392156862745098,
1:   "return_tensors": null,
1:   "size": {
1:     "longest_edge": 12845056,
1:     "shortest_edge": 3136
1:   },
1:   "temporal_patch_size": 2
1: }
1: 
6: [INFO|image_processing_base.py:433] 2025-07-11 11:47:37,591 >> Image processor Qwen2VLImageProcessorFast {
6:   "crop_size": null,
6:   "data_format": "channels_first",
6:   "default_to_square": true,
6:   "device": null,
6:   "do_center_crop": null,
6:   "do_convert_rgb": true,
6:   "do_normalize": true,
6:   "do_rescale": true,
6:   "do_resize": true,
6:   "image_mean": [
6:     0.48145466,
6:     0.4578275,
6:     0.40821073
6:   ],
6:   "image_processor_type": "Qwen2VLImageProcessorFast",
6:   "image_std": [
6:     0.26862954,
6:     0.26130258,
6:     0.27577711
6:   ],
6:   "input_data_format": null,
6:   "max_pixels": 12845056,
6:   "merge_size": 2,
6:   "min_pixels": 3136,
6:   "patch_size": 14,
6:   "processor_class": "Qwen2_5_VLProcessor",
6:   "resample": 3,
6:   "rescale_factor": 0.00392156862745098,
6:   "return_tensors": null,
6:   "size": {
6:     "longest_edge": 12845056,
6:     "shortest_edge": 3136
6:   },
6:   "temporal_patch_size": 2
6: }
6: 
2: [INFO|tokenization_utils_base.py:2323] 2025-07-11 11:47:37,609 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
5: [INFO|image_processing_base.py:380] 2025-07-11 11:47:37,621 >> loading configuration file preprocessor_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
5: [INFO|image_processing_base.py:433] 2025-07-11 11:47:37,623 >> Image processor Qwen2VLImageProcessorFast {
5:   "crop_size": null,
5:   "data_format": "channels_first",
5:   "default_to_square": true,
5:   "device": null,
5:   "do_center_crop": null,
5:   "do_convert_rgb": true,
5:   "do_normalize": true,
5:   "do_rescale": true,
5:   "do_resize": true,
5:   "image_mean": [
5:     0.48145466,
5:     0.4578275,
5:     0.40821073
5:   ],
5:   "image_processor_type": "Qwen2VLImageProcessorFast",
5:   "image_std": [
5:     0.26862954,
5:     0.26130258,
5:     0.27577711
5:   ],
5:   "input_data_format": null,
5:   "max_pixels": 12845056,
5:   "merge_size": 2,
5:   "min_pixels": 3136,
5:   "patch_size": 14,
5:   "processor_class": "Qwen2_5_VLProcessor",
5:   "resample": 3,
5:   "rescale_factor": 0.00392156862745098,
5:   "return_tensors": null,
5:   "size": {
5:     "longest_edge": 12845056,
5:     "shortest_edge": 3136
5:   },
5:   "temporal_patch_size": 2
5: }
5: 
1: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,709 >> loading file vocab.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
1: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,709 >> loading file merges.txt from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
1: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,709 >> loading file tokenizer.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
1: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,709 >> loading file added_tokens.json from cache at None
1: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,710 >> loading file special_tokens_map.json from cache at None
1: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,710 >> loading file tokenizer_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
1: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,710 >> loading file chat_template.jinja from cache at None
6: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,713 >> loading file vocab.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
6: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,713 >> loading file merges.txt from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
6: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,713 >> loading file tokenizer.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
6: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,713 >> loading file added_tokens.json from cache at None
6: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,713 >> loading file special_tokens_map.json from cache at None
6: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,713 >> loading file tokenizer_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
6: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,713 >> loading file chat_template.jinja from cache at None
5: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,745 >> loading file vocab.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
5: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,745 >> loading file merges.txt from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
5: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,746 >> loading file tokenizer.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
5: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,746 >> loading file added_tokens.json from cache at None
5: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,746 >> loading file special_tokens_map.json from cache at None
5: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,746 >> loading file tokenizer_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
5: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:37,746 >> loading file chat_template.jinja from cache at None
1: [INFO|tokenization_utils_base.py:2323] 2025-07-11 11:47:37,876 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
6: [INFO|tokenization_utils_base.py:2323] 2025-07-11 11:47:37,882 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
5: [INFO|2025-07-11 11:47:37] llamafactory.hparams.parser:406 >> Process rank: 23, world size: 32, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
5: [INFO|tokenization_utils_base.py:2323] 2025-07-11 11:47:37,916 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2: [INFO|image_processing_base.py:380] 2025-07-11 11:47:37,964 >> loading configuration file preprocessor_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
0: [INFO|2025-07-11 11:47:37] llamafactory.hparams.parser:406 >> Process rank: 0, world size: 32, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
7: [INFO|processing_utils.py:884] 2025-07-11 11:47:38,080 >> Processor Qwen2_5_VLProcessor:
7: - image_processor: Qwen2VLImageProcessorFast {
7:   "crop_size": null,
7:   "data_format": "channels_first",
7:   "default_to_square": true,
7:   "device": null,
7:   "do_center_crop": null,
7:   "do_convert_rgb": true,
7:   "do_normalize": true,
7:   "do_rescale": true,
7:   "do_resize": true,
7:   "image_mean": [
7:     0.48145466,
7:     0.4578275,
7:     0.40821073
7:   ],
7:   "image_processor_type": "Qwen2VLImageProcessorFast",
7:   "image_std": [
7:     0.26862954,
7:     0.26130258,
7:     0.27577711
7:   ],
7:   "input_data_format": null,
7:   "max_pixels": 12845056,
7:   "merge_size": 2,
7:   "min_pixels": 3136,
7:   "patch_size": 14,
7:   "processor_class": "Qwen2_5_VLProcessor",
7:   "resample": 3,
7:   "rescale_factor": 0.00392156862745098,
7:   "return_tensors": null,
7:   "size": {
7:     "longest_edge": 12845056,
7:     "shortest_edge": 3136
7:   },
7:   "temporal_patch_size": 2
7: }
7: 
7: - tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
7: 	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
7: 	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
7: 	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
7: 	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
7: 	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
7: 	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
7: 	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
7: 	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
7: 	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
7: 	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
7: 	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
7: 	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
7: 	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
7: 	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
7: 	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
7: 	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
7: 	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
7: 	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
7: 	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
7: 	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
7: 	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
7: 	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
7: }
7: )
7: 
7: {
7:   "processor_class": "Qwen2_5_VLProcessor"
7: }
7: 
2: [INFO|image_processing_base.py:380] 2025-07-11 11:47:38,085 >> loading configuration file preprocessor_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
2: [INFO|image_processing_base.py:433] 2025-07-11 11:47:38,087 >> Image processor Qwen2VLImageProcessorFast {
2:   "crop_size": null,
2:   "data_format": "channels_first",
2:   "default_to_square": true,
2:   "device": null,
2:   "do_center_crop": null,
2:   "do_convert_rgb": true,
2:   "do_normalize": true,
2:   "do_rescale": true,
2:   "do_resize": true,
2:   "image_mean": [
2:     0.48145466,
2:     0.4578275,
2:     0.40821073
2:   ],
2:   "image_processor_type": "Qwen2VLImageProcessorFast",
2:   "image_std": [
2:     0.26862954,
2:     0.26130258,
2:     0.27577711
2:   ],
2:   "input_data_format": null,
2:   "max_pixels": 12845056,
2:   "merge_size": 2,
2:   "min_pixels": 3136,
2:   "patch_size": 14,
2:   "processor_class": "Qwen2_5_VLProcessor",
2:   "resample": 3,
2:   "rescale_factor": 0.00392156862745098,
2:   "return_tensors": null,
2:   "size": {
2:     "longest_edge": 12845056,
2:     "shortest_edge": 3136
2:   },
2:   "temporal_patch_size": 2
2: }
2: 
7: [WARNING|2025-07-11 11:47:38] llamafactory.data.loader:148 >> Loading dataset from disk will ignore other data arguments.
4: [INFO|2025-07-11 11:47:38] llamafactory.hparams.parser:406 >> Process rank: 16, world size: 32, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
2: [INFO|2025-07-11 11:47:38] llamafactory.hparams.parser:406 >> Process rank: 10, world size: 32, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
0: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,162 >> loading file vocab.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
0: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,162 >> loading file merges.txt from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
0: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,162 >> loading file tokenizer.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
0: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,162 >> loading file added_tokens.json from cache at None
0: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,162 >> loading file special_tokens_map.json from cache at None
0: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,162 >> loading file tokenizer_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
0: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,162 >> loading file chat_template.jinja from cache at None
2: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,210 >> loading file vocab.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
2: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,210 >> loading file merges.txt from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
2: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,210 >> loading file tokenizer.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
2: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,210 >> loading file added_tokens.json from cache at None
2: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,210 >> loading file special_tokens_map.json from cache at None
2: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,210 >> loading file tokenizer_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
2: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,210 >> loading file chat_template.jinja from cache at None
3: [INFO|image_processing_base.py:380] 2025-07-11 11:47:38,257 >> loading configuration file preprocessor_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
4: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,297 >> loading file vocab.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
4: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,297 >> loading file merges.txt from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
4: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,297 >> loading file tokenizer.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
4: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,297 >> loading file added_tokens.json from cache at None
4: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,297 >> loading file special_tokens_map.json from cache at None
4: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,297 >> loading file tokenizer_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
4: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,297 >> loading file chat_template.jinja from cache at None
5: [INFO|2025-07-11 11:47:38] llamafactory.hparams.parser:406 >> Process rank: 21, world size: 32, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
0: [INFO|tokenization_utils_base.py:2323] 2025-07-11 11:47:38,353 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
3: [INFO|image_processing_base.py:380] 2025-07-11 11:47:38,379 >> loading configuration file preprocessor_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
3: [INFO|image_processing_base.py:433] 2025-07-11 11:47:38,381 >> Image processor Qwen2VLImageProcessorFast {
3:   "crop_size": null,
3:   "data_format": "channels_first",
3:   "default_to_square": true,
3:   "device": null,
3:   "do_center_crop": null,
3:   "do_convert_rgb": true,
3:   "do_normalize": true,
3:   "do_rescale": true,
3:   "do_resize": true,
3:   "image_mean": [
3:     0.48145466,
3:     0.4578275,
3:     0.40821073
3:   ],
3:   "image_processor_type": "Qwen2VLImageProcessorFast",
3:   "image_std": [
3:     0.26862954,
3:     0.26130258,
3:     0.27577711
3:   ],
3:   "input_data_format": null,
3:   "max_pixels": 12845056,
3:   "merge_size": 2,
3:   "min_pixels": 3136,
3:   "patch_size": 14,
3:   "processor_class": "Qwen2_5_VLProcessor",
3:   "resample": 3,
3:   "rescale_factor": 0.00392156862745098,
3:   "return_tensors": null,
3:   "size": {
3:     "longest_edge": 12845056,
3:     "shortest_edge": 3136
3:   },
3:   "temporal_patch_size": 2
3: }
3: 
2: [INFO|tokenization_utils_base.py:2323] 2025-07-11 11:47:38,381 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
4: [INFO|tokenization_utils_base.py:2323] 2025-07-11 11:47:38,488 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
3: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,504 >> loading file vocab.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
3: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,504 >> loading file merges.txt from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
3: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,504 >> loading file tokenizer.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
3: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,504 >> loading file added_tokens.json from cache at None
3: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,504 >> loading file special_tokens_map.json from cache at None
3: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,504 >> loading file tokenizer_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
3: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,504 >> loading file chat_template.jinja from cache at None
1: [INFO|processing_utils.py:884] 2025-07-11 11:47:38,519 >> Processor Qwen2_5_VLProcessor:
1: - image_processor: Qwen2VLImageProcessorFast {
1:   "crop_size": null,
1:   "data_format": "channels_first",
1:   "default_to_square": true,
1:   "device": null,
1:   "do_center_crop": null,
1:   "do_convert_rgb": true,
1:   "do_normalize": true,
1:   "do_rescale": true,
1:   "do_resize": true,
1:   "image_mean": [
1:     0.48145466,
1:     0.4578275,
1:     0.40821073
1:   ],
1:   "image_processor_type": "Qwen2VLImageProcessorFast",
1:   "image_std": [
1:     0.26862954,
1:     0.26130258,
1:     0.27577711
1:   ],
1:   "input_data_format": null,
1:   "max_pixels": 12845056,
1:   "merge_size": 2,
1:   "min_pixels": 3136,
1:   "patch_size": 14,
1:   "processor_class": "Qwen2_5_VLProcessor",
1:   "resample": 3,
1:   "rescale_factor": 0.00392156862745098,
1:   "return_tensors": null,
1:   "size": {
1:     "longest_edge": 12845056,
1:     "shortest_edge": 3136
1:   },
1:   "temporal_patch_size": 2
1: }
1: 
1: - tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
1: 	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
1: 	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
1: 	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
1: 	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
1: 	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
1: 	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
1: 	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
1: 	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
1: 	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
1: 	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
1: 	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
1: 	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
1: 	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
1: 	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
1: 	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
1: 	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
1: 	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
1: 	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
1: 	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
1: 	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
1: 	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
1: 	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
1: }
1: )
1: 
1: {
1:   "processor_class": "Qwen2_5_VLProcessor"
1: }
1: 
6: [INFO|processing_utils.py:884] 2025-07-11 11:47:38,533 >> Processor Qwen2_5_VLProcessor:
6: - image_processor: Qwen2VLImageProcessorFast {
6:   "crop_size": null,
6:   "data_format": "channels_first",
6:   "default_to_square": true,
6:   "device": null,
6:   "do_center_crop": null,
6:   "do_convert_rgb": true,
6:   "do_normalize": true,
6:   "do_rescale": true,
6:   "do_resize": true,
6:   "image_mean": [
6:     0.48145466,
6:     0.4578275,
6:     0.40821073
6:   ],
6:   "image_processor_type": "Qwen2VLImageProcessorFast",
6:   "image_std": [
6:     0.26862954,
6:     0.26130258,
6:     0.27577711
6:   ],
6:   "input_data_format": null,
6:   "max_pixels": 12845056,
6:   "merge_size": 2,
6:   "min_pixels": 3136,
6:   "patch_size": 14,
6:   "processor_class": "Qwen2_5_VLProcessor",
6:   "resample": 3,
6:   "rescale_factor": 0.00392156862745098,
6:   "return_tensors": null,
6:   "size": {
6:     "longest_edge": 12845056,
6:     "shortest_edge": 3136
6:   },
6:   "temporal_patch_size": 2
6: }
6: 
6: - tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
6: 	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
6: 	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
6: 	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
6: 	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
6: 	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
6: 	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
6: 	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
6: 	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
6: 	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
6: 	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
6: 	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
6: 	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
6: 	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
6: 	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
6: 	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
6: 	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
6: 	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
6: 	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
6: 	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
6: 	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
6: 	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
6: 	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
6: }
6: )
6: 
6: {
6:   "processor_class": "Qwen2_5_VLProcessor"
6: }
6: 
1: [WARNING|2025-07-11 11:47:38] llamafactory.data.loader:148 >> Loading dataset from disk will ignore other data arguments.
6: [WARNING|2025-07-11 11:47:38] llamafactory.data.loader:148 >> Loading dataset from disk will ignore other data arguments.
5: [INFO|processing_utils.py:884] 2025-07-11 11:47:38,603 >> Processor Qwen2_5_VLProcessor:
5: - image_processor: Qwen2VLImageProcessorFast {
5:   "crop_size": null,
5:   "data_format": "channels_first",
5:   "default_to_square": true,
5:   "device": null,
5:   "do_center_crop": null,
5:   "do_convert_rgb": true,
5:   "do_normalize": true,
5:   "do_rescale": true,
5:   "do_resize": true,
5:   "image_mean": [
5:     0.48145466,
5:     0.4578275,
5:     0.40821073
5:   ],
5:   "image_processor_type": "Qwen2VLImageProcessorFast",
5:   "image_std": [
5:     0.26862954,
5:     0.26130258,
5:     0.27577711
5:   ],
5:   "input_data_format": null,
5:   "max_pixels": 12845056,
5:   "merge_size": 2,
5:   "min_pixels": 3136,
5:   "patch_size": 14,
5:   "processor_class": "Qwen2_5_VLProcessor",
5:   "resample": 3,
5:   "rescale_factor": 0.00392156862745098,
5:   "return_tensors": null,
5:   "size": {
5:     "longest_edge": 12845056,
5:     "shortest_edge": 3136
5:   },
5:   "temporal_patch_size": 2
5: }
5: 
5: - tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
5: 	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
5: 	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
5: 	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
5: 	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
5: 	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
5: 	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
5: 	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
5: 	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
5: 	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
5: 	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
5: 	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
5: 	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
5: 	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
5: 	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
5: 	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
5: 	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
5: 	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
5: 	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
5: 	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
5: 	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
5: 	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
5: 	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
5: }
5: )
5: 
5: {
5:   "processor_class": "Qwen2_5_VLProcessor"
5: }
5: 
5: [WARNING|2025-07-11 11:47:38] llamafactory.data.loader:148 >> Loading dataset from disk will ignore other data arguments.
3: [INFO|tokenization_utils_base.py:2323] 2025-07-11 11:47:38,679 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
1: [INFO|2025-07-11 11:47:38] llamafactory.data.loader:143 >> Loaded tokenized dataset from /capstor/scratch/cscs/ndeperr/checkpoints/qwen2vl_cs_long/tokenizer.
6: [INFO|2025-07-11 11:47:38] llamafactory.data.loader:143 >> Loaded tokenized dataset from /capstor/scratch/cscs/ndeperr/checkpoints/qwen2vl_cs_long/tokenizer.
7: [INFO|2025-07-11 11:47:38] llamafactory.data.loader:143 >> Loaded tokenized dataset from /capstor/scratch/cscs/ndeperr/checkpoints/qwen2vl_cs_long/tokenizer.
5: [INFO|2025-07-11 11:47:38] llamafactory.data.loader:143 >> Loaded tokenized dataset from /capstor/scratch/cscs/ndeperr/checkpoints/qwen2vl_cs_long/tokenizer.
0: [INFO|image_processing_base.py:380] 2025-07-11 11:47:38,730 >> loading configuration file preprocessor_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
6: [INFO|configuration_utils.py:693] 2025-07-11 11:47:38,832 >> loading configuration file config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
5: [INFO|configuration_utils.py:693] 2025-07-11 11:47:38,832 >> loading configuration file config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
7: [INFO|configuration_utils.py:693] 2025-07-11 11:47:38,832 >> loading configuration file config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
1: [INFO|configuration_utils.py:693] 2025-07-11 11:47:38,833 >> loading configuration file config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
6: [INFO|configuration_utils.py:765] 2025-07-11 11:47:38,834 >> Model config Qwen2_5_VLConfig {
6:   "architectures": [
6:     "Qwen2_5_VLForConditionalGeneration"
6:   ],
6:   "attention_dropout": 0.0,
6:   "bos_token_id": 151643,
6:   "eos_token_id": 151645,
6:   "hidden_act": "silu",
6:   "hidden_size": 3584,
6:   "image_token_id": 151655,
6:   "initializer_range": 0.02,
6:   "intermediate_size": 18944,
6:   "max_position_embeddings": 128000,
6:   "max_window_layers": 28,
6:   "model_type": "qwen2_5_vl",
6:   "num_attention_heads": 28,
6:   "num_hidden_layers": 28,
6:   "num_key_value_heads": 4,
6:   "rms_norm_eps": 1e-06,
6:   "rope_scaling": {
6:     "mrope_section": [
6:       16,
6:       24,
6:       24
6:     ],
6:     "rope_type": "default",
6:     "type": "default"
6:   },
6:   "rope_theta": 1000000.0,
6:   "sliding_window": 32768,
6:   "tie_word_embeddings": false,
6:   "torch_dtype": "bfloat16",
6:   "transformers_version": "4.51.3",
6:   "use_cache": true,
6:   "use_sliding_window": false,
6:   "video_token_id": 151656,
6:   "vision_config": {
6:     "depth": 32,
6:     "fullatt_block_indexes": [
6:       7,
6:       15,
6:       23,
6:       31
6:     ],
6:     "hidden_act": "silu",
6:     "hidden_size": 1280,
6:     "in_channels": 3,
6:     "in_chans": 3,
6:     "intermediate_size": 3420,
6:     "model_type": "qwen2_5_vl",
6:     "num_heads": 16,
6:     "out_hidden_size": 3584,
6:     "patch_size": 14,
6:     "spatial_merge_size": 2,
6:     "spatial_patch_size": 14,
6:     "temporal_patch_size": 2,
6:     "tokens_per_second": 2,
6:     "window_size": 112
6:   },
6:   "vision_end_token_id": 151653,
6:   "vision_start_token_id": 151652,
6:   "vision_token_id": 151654,
6:   "vocab_size": 152064
6: }
6: 
6: [INFO|2025-07-11 11:47:38] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
1: [INFO|configuration_utils.py:765] 2025-07-11 11:47:38,835 >> Model config Qwen2_5_VLConfig {
1:   "architectures": [
1:     "Qwen2_5_VLForConditionalGeneration"
1:   ],
1:   "attention_dropout": 0.0,
1:   "bos_token_id": 151643,
1:   "eos_token_id": 151645,
1:   "hidden_act": "silu",
1:   "hidden_size": 3584,
1:   "image_token_id": 151655,
1:   "initializer_range": 0.02,
1:   "intermediate_size": 18944,
1:   "max_position_embeddings": 128000,
1:   "max_window_layers": 28,
1:   "model_type": "qwen2_5_vl",
1:   "num_attention_heads": 28,
1:   "num_hidden_layers": 28,
1:   "num_key_value_heads": 4,
1:   "rms_norm_eps": 1e-06,
1:   "rope_scaling": {
1:     "mrope_section": [
1:       16,
1:       24,
1:       24
1:     ],
1:     "rope_type": "default",
1:     "type": "default"
1:   },
1:   "rope_theta": 1000000.0,
1:   "sliding_window": 32768,
1:   "tie_word_embeddings": false,
1:   "torch_dtype": "bfloat16",
1:   "transformers_version": "4.51.3",
1:   "use_cache": true,
1:   "use_sliding_window": false,
1:   "video_token_id": 151656,
1:   "vision_config": {
1:     "depth": 32,
1:     "fullatt_block_indexes": [
1:       7,
5: [INFO|configuration_utils.py:765] 2025-07-11 11:47:38,835 >> Model config Qwen2_5_VLConfig {
5:   "architectures": [
5:     "Qwen2_5_VLForConditionalGeneration"
5:   ],
5:   "attention_dropout": 0.0,
5:   "bos_token_id": 151643,
5:   "eos_token_id": 151645,
5:   "hidden_act": "silu",
5:   "hidden_size": 3584,
5:   "image_token_id": 151655,
5:   "initializer_range": 0.02,
5:   "intermediate_size": 18944,
5:   "max_position_embeddings": 128000,
5:   "max_window_layers": 28,
5:   "model_type": "qwen2_5_vl",
5:   "num_attention_heads": 28,
5:   "num_hidden_layers": 28,
5:   "num_key_value_heads": 4,
5:   "rms_norm_eps": 1e-06,
5:   "rope_scaling": {
5:     "mrope_section": [
5:       16,
5:       24,
5:       24
5:     ],
5:     "rope_type": "default",
5:     "type": "default"
5:   },
5:   "rope_theta": 1000000.0,
5:   "sliding_window": 32768,
5:   "tie_word_embeddings": false,
5:   "torch_dtype": "bfloat16",
5:   "transformers_version": "4.51.3",
5:   "use_cache": true,
5:   "use_sliding_window": false,
5:   "video_token_id": 151656,
5:   "vision_config": {
5:     "depth": 32,
5:     "fullatt_block_indexes": [
5:       7,
1: [INFO|2025-07-11 11:47:38] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
7: [INFO|configuration_utils.py:765] 2025-07-11 11:47:38,835 >> Model config Qwen2_5_VLConfig {
7:   "architectures": [
7:     "Qwen2_5_VLForConditionalGeneration"
7:   ],
7:   "attention_dropout": 0.0,
7:   "bos_token_id": 151643,
7:   "eos_token_id": 151645,
7:   "hidden_act": "silu",
7:   "hidden_size": 3584,
7:   "image_token_id": 151655,
7:   "initializer_range": 0.02,
7:   "intermediate_size": 18944,
7:   "max_position_embeddings": 128000,
7:   "max_window_layers": 28,
7:   "model_type": "qwen2_5_vl",
7:   "num_attention_heads": 28,
7:   "num_hidden_layers": 28,
7:   "num_key_value_heads": 4,
7:   "rms_norm_eps": 1e-06,
7:   "rope_scaling": {
7:     "mrope_section": [
7:       16,
7:       24,
7:       24
7:     ],
7:     "rope_type": "default",
7:     "type": "default"
7:   },
7:   "rope_theta": 1000000.0,
7:   "sliding_window": 32768,
7:   "tie_word_embeddings": false,
7:   "torch_dtype": "bfloat16",
7:   "transformers_version": "4.51.3",
7:   "use_cache": true,
7:   "use_sliding_window": false,
7:   "video_token_id": 151656,
7:   "vision_config": {
7:     "depth": 32,
7:     "fullatt_block_indexes": [
7:       7,
5: [INFO|2025-07-11 11:47:38] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
1:       15,
1:       23,
1:       31
1:     ],
1:     "hidden_act": "silu",
1:     "hidden_size": 1280,
1:     "in_channels": 3,
1:     "in_chans": 3,
1:     "intermediate_size": 3420,
1:     "model_type": "qwen2_5_vl",
1:     "num_heads": 16,
1:     "out_hidden_size": 3584,
1:     "patch_size": 14,
1:     "spatial_merge_size": 2,
1:     "spatial_patch_size": 14,
1:     "temporal_patch_size": 2,
1:     "tokens_per_second": 2,
1:     "window_size": 112
1:   },
1:   "vision_end_token_id": 151653,
1:   "vision_start_token_id": 151652,
1:   "vision_token_id": 151654,
1:   "vocab_size": 152064
1: }
1: 
7: [INFO|2025-07-11 11:47:38] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
5:       15,
5:       23,
5:       31
5:     ],
5:     "hidden_act": "silu",
5:     "hidden_size": 1280,
5:     "in_channels": 3,
5:     "in_chans": 3,
5:     "intermediate_size": 3420,
5:     "model_type": "qwen2_5_vl",
5:     "num_heads": 16,
5:     "out_hidden_size": 3584,
5:     "patch_size": 14,
5:     "spatial_merge_size": 2,
5:     "spatial_patch_size": 14,
5:     "temporal_patch_size": 2,
5:     "tokens_per_second": 2,
5:     "window_size": 112
5:   },
5:   "vision_end_token_id": 151653,
5:   "vision_start_token_id": 151652,
5:   "vision_token_id": 151654,
5:   "vocab_size": 152064
5: }
5: 
7:       15,
7:       23,
7:       31
7:     ],
7:     "hidden_act": "silu",
7:     "hidden_size": 1280,
7:     "in_channels": 3,
7:     "in_chans": 3,
7:     "intermediate_size": 3420,
7:     "model_type": "qwen2_5_vl",
7:     "num_heads": 16,
7:     "out_hidden_size": 3584,
7:     "patch_size": 14,
7:     "spatial_merge_size": 2,
7:     "spatial_patch_size": 14,
7:     "temporal_patch_size": 2,
7:     "tokens_per_second": 2,
7:     "window_size": 112
7:   },
7:   "vision_end_token_id": 151653,
7:   "vision_start_token_id": 151652,
7:   "vision_token_id": 151654,
7:   "vocab_size": 152064
7: }
7: 
0: [INFO|image_processing_base.py:380] 2025-07-11 11:47:38,851 >> loading configuration file preprocessor_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
6: [INFO|modeling_utils.py:1124] 2025-07-11 11:47:38,852 >> loading weights file model.safetensors from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/model.safetensors.index.json
0: [INFO|image_processing_base.py:433] 2025-07-11 11:47:38,853 >> Image processor Qwen2VLImageProcessorFast {
0:   "crop_size": null,
0:   "data_format": "channels_first",
0:   "default_to_square": true,
0:   "device": null,
0:   "do_center_crop": null,
0:   "do_convert_rgb": true,
0:   "do_normalize": true,
0:   "do_rescale": true,
0:   "do_resize": true,
0:   "image_mean": [
0:     0.48145466,
0:     0.4578275,
0:     0.40821073
0:   ],
0:   "image_processor_type": "Qwen2VLImageProcessorFast",
0:   "image_std": [
0:     0.26862954,
0:     0.26130258,
0:     0.27577711
0:   ],
0:   "input_data_format": null,
0:   "max_pixels": 12845056,
0:   "merge_size": 2,
0:   "min_pixels": 3136,
0:   "patch_size": 14,
0:   "processor_class": "Qwen2_5_VLProcessor",
0:   "resample": 3,
0:   "rescale_factor": 0.00392156862745098,
0:   "return_tensors": null,
0:   "size": {
0:     "longest_edge": 12845056,
0:     "shortest_edge": 3136
0:   },
0:   "temporal_patch_size": 2
0: }
0: 
1: [INFO|modeling_utils.py:1124] 2025-07-11 11:47:38,855 >> loading weights file model.safetensors from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/model.safetensors.index.json
5: [INFO|modeling_utils.py:1124] 2025-07-11 11:47:38,855 >> loading weights file model.safetensors from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/model.safetensors.index.json
7: [INFO|modeling_utils.py:1124] 2025-07-11 11:47:38,855 >> loading weights file model.safetensors from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/model.safetensors.index.json
1: [INFO|modeling_utils.py:3726] 2025-07-11 11:47:38,869 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
5: [INFO|modeling_utils.py:3726] 2025-07-11 11:47:38,869 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
7: [INFO|modeling_utils.py:3726] 2025-07-11 11:47:38,869 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
6: [INFO|modeling_utils.py:3726] 2025-07-11 11:47:38,869 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
1: [2025-07-11 11:47:38,869] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
5: [2025-07-11 11:47:38,869] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
4: [INFO|image_processing_base.py:380] 2025-07-11 11:47:38,869 >> loading configuration file preprocessor_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
7: [2025-07-11 11:47:38,869] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
6: [2025-07-11 11:47:38,869] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
6: [INFO|configuration_utils.py:1142] 2025-07-11 11:47:38,874 >> Generate config GenerationConfig {
6:   "bos_token_id": 151643,
6:   "eos_token_id": 151645,
6:   "use_cache": false
6: }
6: 
1: [INFO|configuration_utils.py:1142] 2025-07-11 11:47:38,875 >> Generate config GenerationConfig {
1:   "bos_token_id": 151643,
1:   "eos_token_id": 151645,
1:   "use_cache": false
1: }
1: 
6: [INFO|modeling_utils.py:2167] 2025-07-11 11:47:38,874 >> Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.float32.
7: [INFO|configuration_utils.py:1142] 2025-07-11 11:47:38,875 >> Generate config GenerationConfig {
7:   "bos_token_id": 151643,
7:   "eos_token_id": 151645,
7:   "use_cache": false
7: }
7: 
1: [INFO|modeling_utils.py:2167] 2025-07-11 11:47:38,875 >> Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.float32.
7: [INFO|modeling_utils.py:2167] 2025-07-11 11:47:38,875 >> Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.float32.
5: [INFO|configuration_utils.py:1142] 2025-07-11 11:47:38,876 >> Generate config GenerationConfig {
5:   "bos_token_id": 151643,
5:   "eos_token_id": 151645,
5:   "use_cache": false
5: }
5: 
5: [INFO|modeling_utils.py:2167] 2025-07-11 11:47:38,876 >> Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.float32.
0: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,974 >> loading file vocab.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
0: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,974 >> loading file merges.txt from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
0: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,974 >> loading file tokenizer.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
0: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,974 >> loading file added_tokens.json from cache at None
0: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,974 >> loading file special_tokens_map.json from cache at None
0: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,974 >> loading file tokenizer_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
0: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:38,974 >> loading file chat_template.jinja from cache at None
4: [INFO|image_processing_base.py:380] 2025-07-11 11:47:38,987 >> loading configuration file preprocessor_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/preprocessor_config.json
4: [INFO|image_processing_base.py:433] 2025-07-11 11:47:38,989 >> Image processor Qwen2VLImageProcessorFast {
4:   "crop_size": null,
4:   "data_format": "channels_first",
4:   "default_to_square": true,
4:   "device": null,
4:   "do_center_crop": null,
4:   "do_convert_rgb": true,
4:   "do_normalize": true,
4:   "do_rescale": true,
4:   "do_resize": true,
4:   "image_mean": [
4:     0.48145466,
4:     0.4578275,
4:     0.40821073
4:   ],
4:   "image_processor_type": "Qwen2VLImageProcessorFast",
4:   "image_std": [
4:     0.26862954,
4:     0.26130258,
4:     0.27577711
4:   ],
4:   "input_data_format": null,
4:   "max_pixels": 12845056,
4:   "merge_size": 2,
4:   "min_pixels": 3136,
4:   "patch_size": 14,
4:   "processor_class": "Qwen2_5_VLProcessor",
4:   "resample": 3,
4:   "rescale_factor": 0.00392156862745098,
4:   "return_tensors": null,
4:   "size": {
4:     "longest_edge": 12845056,
4:     "shortest_edge": 3136
4:   },
4:   "temporal_patch_size": 2
4: }
4: 
0: [INFO|2025-07-11 11:47:39] llamafactory.hparams.parser:406 >> Process rank: 1, world size: 32, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
0: [INFO|2025-07-11 11:47:39] llamafactory.hparams.parser:406 >> Process rank: 2, world size: 32, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
0: [INFO|2025-07-11 11:47:39] llamafactory.hparams.parser:406 >> Process rank: 3, world size: 32, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
4: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:39,109 >> loading file vocab.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/vocab.json
4: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:39,109 >> loading file merges.txt from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/merges.txt
4: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:39,109 >> loading file tokenizer.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer.json
4: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:39,109 >> loading file added_tokens.json from cache at None
4: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:39,109 >> loading file special_tokens_map.json from cache at None
4: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:39,109 >> loading file tokenizer_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/tokenizer_config.json
4: [INFO|tokenization_utils_base.py:2060] 2025-07-11 11:47:39,109 >> loading file chat_template.jinja from cache at None
0: [INFO|tokenization_utils_base.py:2323] 2025-07-11 11:47:39,147 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
4: [INFO|2025-07-11 11:47:39] llamafactory.hparams.parser:406 >> Process rank: 18, world size: 32, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
4: [INFO|2025-07-11 11:47:39] llamafactory.hparams.parser:406 >> Process rank: 19, world size: 32, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
7: [INFO|2025-07-11 11:47:39] llamafactory.hparams.parser:406 >> Process rank: 30, world size: 32, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
4: [INFO|2025-07-11 11:47:39] llamafactory.hparams.parser:406 >> Process rank: 17, world size: 32, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
7: [INFO|2025-07-11 11:47:39] llamafactory.hparams.parser:406 >> Process rank: 31, world size: 32, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
2: [INFO|processing_utils.py:884] 2025-07-11 11:47:39,257 >> Processor Qwen2_5_VLProcessor:
2: - image_processor: Qwen2VLImageProcessorFast {
2:   "crop_size": null,
2:   "data_format": "channels_first",
2:   "default_to_square": true,
2:   "device": null,
2:   "do_center_crop": null,
2:   "do_convert_rgb": true,
2:   "do_normalize": true,
2:   "do_rescale": true,
2:   "do_resize": true,
2:   "image_mean": [
2:     0.48145466,
2:     0.4578275,
2:     0.40821073
2:   ],
2:   "image_processor_type": "Qwen2VLImageProcessorFast",
2:   "image_std": [
2:     0.26862954,
2:     0.26130258,
2:     0.27577711
2:   ],
2:   "input_data_format": null,
2:   "max_pixels": 12845056,
2:   "merge_size": 2,
2:   "min_pixels": 3136,
2:   "patch_size": 14,
2:   "processor_class": "Qwen2_5_VLProcessor",
2:   "resample": 3,
2:   "rescale_factor": 0.00392156862745098,
2:   "return_tensors": null,
2:   "size": {
2:     "longest_edge": 12845056,
2:     "shortest_edge": 3136
2:   },
2:   "temporal_patch_size": 2
2: }
2: 
2: - tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
2: 	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
2: 	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
2: 	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
2: 	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
2: 	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
2: 	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
2: 	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
2: 	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
2: 	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
2: 	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
2: 	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
2: 	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
2: 	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
2: 	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
2: 	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
2: 	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
2: 	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
2: 	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
2: 	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
2: 	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
2: 	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
2: 	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
2: }
2: )
2: 
2: {
2:   "processor_class": "Qwen2_5_VLProcessor"
2: }
2: 
2: [WARNING|2025-07-11 11:47:39] llamafactory.data.loader:148 >> Loading dataset from disk will ignore other data arguments.
4: [INFO|tokenization_utils_base.py:2323] 2025-07-11 11:47:39,283 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2: [INFO|2025-07-11 11:47:39] llamafactory.data.loader:143 >> Loaded tokenized dataset from /capstor/scratch/cscs/ndeperr/checkpoints/qwen2vl_cs_long/tokenizer.
6: [INFO|2025-07-11 11:47:39] llamafactory.hparams.parser:406 >> Process rank: 27, world size: 32, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
5: [2025-07-11 11:47:39,391] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
1: [INFO|2025-07-11 11:47:39] llamafactory.hparams.parser:406 >> Process rank: 5, world size: 32, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
1: [INFO|2025-07-11 11:47:39] llamafactory.hparams.parser:406 >> Process rank: 6, world size: 32, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
2: [INFO|configuration_utils.py:693] 2025-07-11 11:47:39,414 >> loading configuration file config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
2: [INFO|configuration_utils.py:765] 2025-07-11 11:47:39,416 >> Model config Qwen2_5_VLConfig {
2:   "architectures": [
2:     "Qwen2_5_VLForConditionalGeneration"
2:   ],
2:   "attention_dropout": 0.0,
2:   "bos_token_id": 151643,
2:   "eos_token_id": 151645,
2:   "hidden_act": "silu",
2:   "hidden_size": 3584,
2:   "image_token_id": 151655,
2:   "initializer_range": 0.02,
2:   "intermediate_size": 18944,
2:   "max_position_embeddings": 128000,
2:   "max_window_layers": 28,
2:   "model_type": "qwen2_5_vl",
2:   "num_attention_heads": 28,
2:   "num_hidden_layers": 28,
2:   "num_key_value_heads": 4,
2:   "rms_norm_eps": 1e-06,
2:   "rope_scaling": {
2:     "mrope_section": [
2:       16,
2:       24,
2:       24
2:     ],
2:     "rope_type": "default",
2:     "type": "default"
2:   },
2:   "rope_theta": 1000000.0,
2:   "sliding_window": 32768,
2:   "tie_word_embeddings": false,
2:   "torch_dtype": "bfloat16",
2:   "transformers_version": "4.51.3",
2:   "use_cache": true,
2:   "use_sliding_window": false,
2:   "video_token_id": 151656,
2:   "vision_config": {
2:     "depth": 32,
2:     "fullatt_block_indexes": [
2:       7,
2:       15,
2:       23,
2:       31
2:     ],
2:     "hidden_act": "silu",
2:     "hidden_size": 1280,
2:     "in_channels": 3,
2:     "in_chans": 3,
2:     "intermediate_size": 3420,
2:     "model_type": "qwen2_5_vl",
2:     "num_heads": 16,
2:     "out_hidden_size": 3584,
2:     "patch_size": 14,
2:     "spatial_merge_size": 2,
2:     "spatial_patch_size": 14,
2:     "temporal_patch_size": 2,
2:     "tokens_per_second": 2,
2:     "window_size": 112
2:   },
2:   "vision_end_token_id": 151653,
2:   "vision_start_token_id": 151652,
2:   "vision_token_id": 151654,
2:   "vocab_size": 152064
2: }
2: 
2: [INFO|2025-07-11 11:47:39] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
2: [INFO|modeling_utils.py:1124] 2025-07-11 11:47:39,431 >> loading weights file model.safetensors from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/model.safetensors.index.json
6: [INFO|2025-07-11 11:47:39] llamafactory.hparams.parser:406 >> Process rank: 26, world size: 32, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
1: [INFO|2025-07-11 11:47:39] llamafactory.hparams.parser:406 >> Process rank: 7, world size: 32, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
2: [INFO|modeling_utils.py:3726] 2025-07-11 11:47:39,437 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
2: [2025-07-11 11:47:39,437] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
2: [2025-07-11 11:47:39,437] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
2: [INFO|configuration_utils.py:1142] 2025-07-11 11:47:39,444 >> Generate config GenerationConfig {
2:   "bos_token_id": 151643,
2:   "eos_token_id": 151645,
2:   "use_cache": false
2: }
2: 
2: [INFO|modeling_utils.py:2167] 2025-07-11 11:47:39,444 >> Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.float32.
6: [INFO|2025-07-11 11:47:39] llamafactory.hparams.parser:406 >> Process rank: 25, world size: 32, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
3: [INFO|2025-07-11 11:47:39] llamafactory.hparams.parser:406 >> Process rank: 15, world size: 32, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
7: [INFO|2025-07-11 11:47:39] llamafactory.hparams.parser:406 >> Process rank: 29, world size: 32, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
3: [INFO|processing_utils.py:884] 2025-07-11 11:47:39,597 >> Processor Qwen2_5_VLProcessor:
3: - image_processor: Qwen2VLImageProcessorFast {
3:   "crop_size": null,
3:   "data_format": "channels_first",
3:   "default_to_square": true,
3:   "device": null,
3:   "do_center_crop": null,
3:   "do_convert_rgb": true,
3:   "do_normalize": true,
3:   "do_rescale": true,
3:   "do_resize": true,
3:   "image_mean": [
3:     0.48145466,
3:     0.4578275,
3:     0.40821073
3:   ],
3:   "image_processor_type": "Qwen2VLImageProcessorFast",
3:   "image_std": [
3:     0.26862954,
3:     0.26130258,
3:     0.27577711
3:   ],
3:   "input_data_format": null,
3:   "max_pixels": 12845056,
3:   "merge_size": 2,
3:   "min_pixels": 3136,
3:   "patch_size": 14,
3:   "processor_class": "Qwen2_5_VLProcessor",
3:   "resample": 3,
3:   "rescale_factor": 0.00392156862745098,
3:   "return_tensors": null,
3:   "size": {
3:     "longest_edge": 12845056,
3:     "shortest_edge": 3136
3:   },
3:   "temporal_patch_size": 2
3: }
3: 
3: - tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
3: 	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
3: 	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
3: 	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
3: 	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
3: 	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
3: 	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
3: 	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
3: 	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
3: 	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
3: 	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
3: 	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
3: 	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
3: 	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
3: 	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
3: 	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
3: 	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
3: 	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
3: 	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
3: 	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
3: 	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
3: 	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
3: 	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
3: }
3: )
3: 
3: {
3:   "processor_class": "Qwen2_5_VLProcessor"
3: }
3: 
3: [WARNING|2025-07-11 11:47:39] llamafactory.data.loader:148 >> Loading dataset from disk will ignore other data arguments.
2: [2025-07-11 11:47:39,643] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
5: [2025-07-11 11:47:39,869] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
3: [INFO|2025-07-11 11:47:39] llamafactory.data.loader:143 >> Loaded tokenized dataset from /capstor/scratch/cscs/ndeperr/checkpoints/qwen2vl_cs_long/tokenizer.
0: [INFO|processing_utils.py:884] 2025-07-11 11:47:39,953 >> Processor Qwen2_5_VLProcessor:
0: - image_processor: Qwen2VLImageProcessorFast {
0:   "crop_size": null,
0:   "data_format": "channels_first",
0:   "default_to_square": true,
0:   "device": null,
0:   "do_center_crop": null,
0:   "do_convert_rgb": true,
0:   "do_normalize": true,
0:   "do_rescale": true,
0:   "do_resize": true,
0:   "image_mean": [
0:     0.48145466,
0:     0.4578275,
0:     0.40821073
0:   ],
0:   "image_processor_type": "Qwen2VLImageProcessorFast",
0:   "image_std": [
0:     0.26862954,
0:     0.26130258,
0:     0.27577711
0:   ],
0:   "input_data_format": null,
0:   "max_pixels": 12845056,
0:   "merge_size": 2,
0:   "min_pixels": 3136,
0:   "patch_size": 14,
0:   "processor_class": "Qwen2_5_VLProcessor",
0:   "resample": 3,
0:   "rescale_factor": 0.00392156862745098,
0:   "return_tensors": null,
0:   "size": {
0:     "longest_edge": 12845056,
0:     "shortest_edge": 3136
0:   },
0:   "temporal_patch_size": 2
0: }
0: 
0: - tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
0: 	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
0: 	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
0: 	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
0: 	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
0: 	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
0: 	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
0: 	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
0: 	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
0: 	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
0: 	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
0: 	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
0: 	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
0: 	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
0: 	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
0: 	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
0: 	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
0: 	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
0: 	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
0: 	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
0: 	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
0: 	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
0: 	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
0: }
0: )
0: 
0: {
0:   "processor_class": "Qwen2_5_VLProcessor"
0: }
0: 
3: [INFO|2025-07-11 11:47:39] llamafactory.hparams.parser:406 >> Process rank: 13, world size: 32, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
3: [INFO|2025-07-11 11:47:39] llamafactory.hparams.parser:406 >> Process rank: 14, world size: 32, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
4: [INFO|processing_utils.py:884] 2025-07-11 11:47:39,966 >> Processor Qwen2_5_VLProcessor:
4: - image_processor: Qwen2VLImageProcessorFast {
4:   "crop_size": null,
4:   "data_format": "channels_first",
4:   "default_to_square": true,
4:   "device": null,
4:   "do_center_crop": null,
4:   "do_convert_rgb": true,
4:   "do_normalize": true,
4:   "do_rescale": true,
4:   "do_resize": true,
4:   "image_mean": [
4:     0.48145466,
4:     0.4578275,
4:     0.40821073
4:   ],
4:   "image_processor_type": "Qwen2VLImageProcessorFast",
4:   "image_std": [
4:     0.26862954,
4:     0.26130258,
4:     0.27577711
4:   ],
4:   "input_data_format": null,
4:   "max_pixels": 12845056,
4:   "merge_size": 2,
4:   "min_pixels": 3136,
4:   "patch_size": 14,
4:   "processor_class": "Qwen2_5_VLProcessor",
4:   "resample": 3,
4:   "rescale_factor": 0.00392156862745098,
4:   "return_tensors": null,
4:   "size": {
4:     "longest_edge": 12845056,
4:     "shortest_edge": 3136
4:   },
4:   "temporal_patch_size": 2
4: }
4: 
4: - tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
4: 	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
4: 	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
4: 	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
4: 	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
4: 	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
4: 	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
4: 	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
4: 	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
4: 	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
4: 	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
4: 	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
4: 	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
4: 	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
4: 	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
4: 	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
4: 	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
4: 	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
4: 	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
4: 	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
4: 	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
4: 	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
4: 	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
4: }
4: )
4: 
4: {
4:   "processor_class": "Qwen2_5_VLProcessor"
4: }
4: 
0: [WARNING|2025-07-11 11:47:39] llamafactory.data.loader:148 >> Loading dataset from disk will ignore other data arguments.
4: [WARNING|2025-07-11 11:47:39] llamafactory.data.loader:148 >> Loading dataset from disk will ignore other data arguments.
3: [INFO|configuration_utils.py:693] 2025-07-11 11:47:40,037 >> loading configuration file config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
3: [INFO|configuration_utils.py:765] 2025-07-11 11:47:40,040 >> Model config Qwen2_5_VLConfig {
3:   "architectures": [
3:     "Qwen2_5_VLForConditionalGeneration"
3:   ],
3:   "attention_dropout": 0.0,
3:   "bos_token_id": 151643,
3:   "eos_token_id": 151645,
3:   "hidden_act": "silu",
3:   "hidden_size": 3584,
3:   "image_token_id": 151655,
3:   "initializer_range": 0.02,
3:   "intermediate_size": 18944,
3:   "max_position_embeddings": 128000,
3:   "max_window_layers": 28,
3:   "model_type": "qwen2_5_vl",
3:   "num_attention_heads": 28,
3:   "num_hidden_layers": 28,
3:   "num_key_value_heads": 4,
3:   "rms_norm_eps": 1e-06,
3:   "rope_scaling": {
3:     "mrope_section": [
3:       16,
3:       24,
3:       24
3:     ],
3:     "rope_type": "default",
3:     "type": "default"
3:   },
3:   "rope_theta": 1000000.0,
3:   "sliding_window": 32768,
3:   "tie_word_embeddings": false,
3:   "torch_dtype": "bfloat16",
3:   "transformers_version": "4.51.3",
3:   "use_cache": true,
3:   "use_sliding_window": false,
3:   "video_token_id": 151656,
3:   "vision_config": {
3:     "depth": 32,
3:     "fullatt_block_indexes": [
3:       7,
3:       15,
3:       23,
3:       31
3:     ],
3:     "hidden_act": "silu",
3:     "hidden_size": 1280,
3:     "in_channels": 3,
3:     "in_chans": 3,
3:     "intermediate_size": 3420,
3:     "model_type": "qwen2_5_vl",
3:     "num_heads": 16,
3:     "out_hidden_size": 3584,
3:     "patch_size": 14,
3:     "spatial_merge_size": 2,
3:     "spatial_patch_size": 14,
3:     "temporal_patch_size": 2,
3:     "tokens_per_second": 2,
3:     "window_size": 112
3:   },
3:   "vision_end_token_id": 151653,
3:   "vision_start_token_id": 151652,
3:   "vision_token_id": 151654,
3:   "vocab_size": 152064
3: }
3: 
3: [INFO|2025-07-11 11:47:40] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
3: [INFO|modeling_utils.py:1124] 2025-07-11 11:47:40,061 >> loading weights file model.safetensors from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/model.safetensors.index.json
3: [INFO|modeling_utils.py:3726] 2025-07-11 11:47:40,066 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
3: [2025-07-11 11:47:40,066] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
3: [INFO|configuration_utils.py:1142] 2025-07-11 11:47:40,075 >> Generate config GenerationConfig {
3:   "bos_token_id": 151643,
3:   "eos_token_id": 151645,
3:   "use_cache": false
3: }
3: 
3: [INFO|modeling_utils.py:2167] 2025-07-11 11:47:40,075 >> Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.float32.
2: [2025-07-11 11:47:40,089] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
4: [INFO|2025-07-11 11:47:40] llamafactory.data.loader:143 >> Loaded tokenized dataset from /capstor/scratch/cscs/ndeperr/checkpoints/qwen2vl_cs_long/tokenizer.
0: [INFO|2025-07-11 11:47:40] llamafactory.data.loader:143 >> Loaded tokenized dataset from /capstor/scratch/cscs/ndeperr/checkpoints/qwen2vl_cs_long/tokenizer.
4: [INFO|configuration_utils.py:693] 2025-07-11 11:47:40,332 >> loading configuration file config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
4: [INFO|configuration_utils.py:765] 2025-07-11 11:47:40,335 >> Model config Qwen2_5_VLConfig {
4:   "architectures": [
4:     "Qwen2_5_VLForConditionalGeneration"
4:   ],
4:   "attention_dropout": 0.0,
4:   "bos_token_id": 151643,
4:   "eos_token_id": 151645,
4:   "hidden_act": "silu",
4:   "hidden_size": 3584,
4:   "image_token_id": 151655,
4:   "initializer_range": 0.02,
4:   "intermediate_size": 18944,
4:   "max_position_embeddings": 128000,
4:   "max_window_layers": 28,
4:   "model_type": "qwen2_5_vl",
4:   "num_attention_heads": 28,
4:   "num_hidden_layers": 28,
4:   "num_key_value_heads": 4,
4:   "rms_norm_eps": 1e-06,
4:   "rope_scaling": {
4:     "mrope_section": [
4:       16,
4:       24,
4:       24
4:     ],
4:     "rope_type": "default",
4:     "type": "default"
4:   },
4:   "rope_theta": 1000000.0,
4:   "sliding_window": 32768,
4:   "tie_word_embeddings": false,
4:   "torch_dtype": "bfloat16",
4:   "transformers_version": "4.51.3",
4:   "use_cache": true,
4:   "use_sliding_window": false,
4:   "video_token_id": 151656,
4:   "vision_config": {
4:     "depth": 32,
4:     "fullatt_block_indexes": [
4:       7,
4:       15,
4:       23,
4:       31
4:     ],
4:     "hidden_act": "silu",
4:     "hidden_size": 1280,
4:     "in_channels": 3,
4:     "in_chans": 3,
4:     "intermediate_size": 3420,
4:     "model_type": "qwen2_5_vl",
4:     "num_heads": 16,
4:     "out_hidden_size": 3584,
4:     "patch_size": 14,
4:     "spatial_merge_size": 2,
4:     "spatial_patch_size": 14,
4:     "temporal_patch_size": 2,
4:     "tokens_per_second": 2,
4:     "window_size": 112
4:   },
4:   "vision_end_token_id": 151653,
4:   "vision_start_token_id": 151652,
4:   "vision_token_id": 151654,
4:   "vocab_size": 152064
4: }
4: 
4: [INFO|2025-07-11 11:47:40] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
4: [INFO|modeling_utils.py:1124] 2025-07-11 11:47:40,357 >> loading weights file model.safetensors from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/model.safetensors.index.json
4: [INFO|modeling_utils.py:3726] 2025-07-11 11:47:40,362 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
4: [2025-07-11 11:47:40,362] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
4: [INFO|configuration_utils.py:1142] 2025-07-11 11:47:40,369 >> Generate config GenerationConfig {
4:   "bos_token_id": 151643,
4:   "eos_token_id": 151645,
4:   "use_cache": false
4: }
4: 
4: [INFO|modeling_utils.py:2167] 2025-07-11 11:47:40,369 >> Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.float32.
0: [INFO|configuration_utils.py:693] 2025-07-11 11:47:40,450 >> loading configuration file config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/config.json
0: [INFO|configuration_utils.py:765] 2025-07-11 11:47:40,452 >> Model config Qwen2_5_VLConfig {
0:   "architectures": [
0:     "Qwen2_5_VLForConditionalGeneration"
0:   ],
0:   "attention_dropout": 0.0,
0:   "bos_token_id": 151643,
0:   "eos_token_id": 151645,
0:   "hidden_act": "silu",
0:   "hidden_size": 3584,
0:   "image_token_id": 151655,
0:   "initializer_range": 0.02,
0:   "intermediate_size": 18944,
0:   "max_position_embeddings": 128000,
0:   "max_window_layers": 28,
0:   "model_type": "qwen2_5_vl",
0:   "num_attention_heads": 28,
0:   "num_hidden_layers": 28,
0:   "num_key_value_heads": 4,
0:   "rms_norm_eps": 1e-06,
0:   "rope_scaling": {
0:     "mrope_section": [
0:       16,
0:       24,
0:       24
0:     ],
0:     "rope_type": "default",
0:     "type": "default"
0:   },
0:   "rope_theta": 1000000.0,
0:   "sliding_window": 32768,
0:   "tie_word_embeddings": false,
0:   "torch_dtype": "bfloat16",
0:   "transformers_version": "4.51.3",
0:   "use_cache": true,
0:   "use_sliding_window": false,
0:   "video_token_id": 151656,
0:   "vision_config": {
0:     "depth": 32,
0:     "fullatt_block_indexes": [
0:       7,
0:       15,
0:       23,
0:       31
0:     ],
0:     "hidden_act": "silu",
0:     "hidden_size": 1280,
0:     "in_channels": 3,
0:     "in_chans": 3,
0:     "intermediate_size": 3420,
0:     "model_type": "qwen2_5_vl",
0:     "num_heads": 16,
0:     "out_hidden_size": 3584,
0:     "patch_size": 14,
0:     "spatial_merge_size": 2,
0:     "spatial_patch_size": 14,
0:     "temporal_patch_size": 2,
0:     "tokens_per_second": 2,
0:     "window_size": 112
0:   },
0:   "vision_end_token_id": 151653,
0:   "vision_start_token_id": 151652,
0:   "vision_token_id": 151654,
0:   "vocab_size": 152064
0: }
0: 
0: [INFO|2025-07-11 11:47:40] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
0: [INFO|modeling_utils.py:1124] 2025-07-11 11:47:40,475 >> loading weights file model.safetensors from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/model.safetensors.index.json
0: [INFO|modeling_utils.py:3726] 2025-07-11 11:47:40,480 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
0: [2025-07-11 11:47:40,480] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
0: [INFO|configuration_utils.py:1142] 2025-07-11 11:47:40,487 >> Generate config GenerationConfig {
0:   "bos_token_id": 151643,
0:   "eos_token_id": 151645,
0:   "use_cache": false
0: }
0: 
0: [INFO|modeling_utils.py:2167] 2025-07-11 11:47:40,487 >> Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.float32.
5: [2025-07-11 11:47:40,533] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
0: nid007116:202900:202900 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
0: nid007116:202900:202900 [0] NCCL INFO Bootstrap : Using hsn0:172.28.39.104<0>
0: nid007116:202900:202900 [0] NCCL INFO cudaDriverVersion 12060
0: nid007116:202900:202900 [0] NCCL INFO NCCL version 2.22.3+cuda12.6
3: nid007120:197036:197036 [0] NCCL INFO cudaDriverVersion 12060
3: nid007120:197036:197036 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
7: nid007124:126395:126395 [0] NCCL INFO cudaDriverVersion 12060
7: nid007124:126395:126395 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
3: nid007120:197036:197036 [0] NCCL INFO Bootstrap : Using hsn0:172.28.39.120<0>
3: nid007120:197036:197036 [0] NCCL INFO NCCL version 2.22.3+cuda12.6
6: nid007123:108289:108289 [0] NCCL INFO cudaDriverVersion 12060
6: nid007123:108289:108289 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
4: nid007121:116201:116201 [0] NCCL INFO cudaDriverVersion 12060
4: nid007121:116201:116201 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
7: nid007124:126395:126395 [0] NCCL INFO Bootstrap : Using hsn0:172.28.39.136<0>
7: nid007124:126395:126395 [0] NCCL INFO NCCL version 2.22.3+cuda12.6
6: nid007123:108289:108289 [0] NCCL INFO Bootstrap : Using hsn0:172.28.39.132<0>
6: nid007123:108289:108289 [0] NCCL INFO NCCL version 2.22.3+cuda12.6
4: nid007121:116201:116201 [0] NCCL INFO Bootstrap : Using hsn0:172.28.39.124<0>
4: nid007121:116201:116201 [0] NCCL INFO NCCL version 2.22.3+cuda12.6
1: nid007117:210405:210405 [0] NCCL INFO cudaDriverVersion 12060
1: nid007117:210405:210405 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
1: nid007117:210405:210405 [0] NCCL INFO Bootstrap : Using hsn0:172.28.39.108<0>
1: nid007117:210405:210405 [0] NCCL INFO NCCL version 2.22.3+cuda12.6
5: nid007122:66178:66178 [3] NCCL INFO cudaDriverVersion 12060
5: nid007122:66178:66178 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
5: nid007122:66178:66178 [3] NCCL INFO Bootstrap : Using hsn0:172.28.39.128<0>
5: nid007122:66178:66178 [3] NCCL INFO NCCL version 2.22.3+cuda12.6
5: nid007122:66177:66177 [2] NCCL INFO cudaDriverVersion 12060
5: nid007122:66175:66175 [0] NCCL INFO cudaDriverVersion 12060
2: nid007119:146139:146139 [3] NCCL INFO cudaDriverVersion 12060
2: nid007119:146139:146139 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
5: nid007122:66176:66176 [1] NCCL INFO cudaDriverVersion 12060
2: nid007119:146139:146139 [3] NCCL INFO Bootstrap : Using hsn0:172.28.39.116<0>
2: nid007119:146139:146139 [3] NCCL INFO NCCL version 2.22.3+cuda12.6
2: nid007119:146137:146137 [1] NCCL INFO cudaDriverVersion 12060
2: nid007119:146136:146136 [0] NCCL INFO cudaDriverVersion 12060
2: nid007119:146138:146138 [2] NCCL INFO cudaDriverVersion 12060
4: nid007121:116201:116666 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
4: nid007121:116201:116666 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
4: nid007121:116201:116666 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
4: nid007121:116201:116666 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
4: nid007121:116201:116666 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
4: nid007121:116201:116666 [0] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
4: 
4: nid007121:116201:116666 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
4: nid007121:116201:116666 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
4: nid007121:116201:116666 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
4: nid007121:116201:116666 [0] NCCL INFO NET/OFI Creating one domain per process
4: nid007121:116201:116666 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
4: nid007121:116201:116666 [0] NCCL INFO NET/OFI Support for global registrations: false
4: nid007121:116201:116666 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
4: nid007121:116201:116666 [0] NCCL INFO Using network AWS Libfabric
5: nid007122:66177:66177 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
5: nid007122:66177:66177 [2] NCCL INFO Bootstrap : Using hsn0:172.28.39.128<0>
5: nid007122:66177:66177 [2] NCCL INFO NCCL version 2.22.3+cuda12.6
2: nid007119:146137:146137 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
2: nid007119:146136:146136 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
2: nid007119:146138:146138 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
2: nid007119:146137:146137 [1] NCCL INFO Bootstrap : Using hsn0:172.28.39.116<0>
2: nid007119:146137:146137 [1] NCCL INFO NCCL version 2.22.3+cuda12.6
2: nid007119:146136:146136 [0] NCCL INFO Bootstrap : Using hsn0:172.28.39.116<0>
2: nid007119:146138:146138 [2] NCCL INFO Bootstrap : Using hsn0:172.28.39.116<0>
2: nid007119:146138:146138 [2] NCCL INFO NCCL version 2.22.3+cuda12.6
2: nid007119:146136:146136 [0] NCCL INFO NCCL version 2.22.3+cuda12.6
5: nid007122:66175:66175 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
5: nid007122:66176:66176 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
5: nid007122:66176:66176 [1] NCCL INFO Bootstrap : Using hsn0:172.28.39.128<0>
5: nid007122:66175:66175 [0] NCCL INFO Bootstrap : Using hsn0:172.28.39.128<0>
5: nid007122:66176:66176 [1] NCCL INFO NCCL version 2.22.3+cuda12.6
5: nid007122:66175:66175 [0] NCCL INFO NCCL version 2.22.3+cuda12.6
1: nid007117:210405:210942 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
1: nid007117:210405:210942 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
1: nid007117:210405:210942 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
1: nid007117:210405:210942 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
1: nid007117:210405:210942 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
1: nid007117:210405:210942 [0] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
1: 
1: nid007117:210405:210942 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
1: nid007117:210405:210942 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
1: nid007117:210405:210942 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
1: nid007117:210405:210942 [0] NCCL INFO NET/OFI Creating one domain per process
1: nid007117:210405:210942 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
1: nid007117:210405:210942 [0] NCCL INFO NET/OFI Support for global registrations: false
1: nid007117:210405:210942 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
1: nid007117:210405:210942 [0] NCCL INFO Using network AWS Libfabric
0: [2025-07-11 11:47:41,164] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
0: [2025-07-11 11:47:41,164] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
0: nid007116:202902:202902 [2] NCCL INFO cudaDriverVersion 12060
0: nid007116:202902:202902 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
4: [2025-07-11 11:47:41,193] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
0: nid007116:202902:202902 [2] NCCL INFO Bootstrap : Using hsn0:172.28.39.104<0>
0: nid007116:202902:202902 [2] NCCL INFO NCCL version 2.22.3+cuda12.6
0: nid007116:202903:202903 [3] NCCL INFO cudaDriverVersion 12060
0: nid007116:202903:202903 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
0: nid007116:202903:202903 [3] NCCL INFO Bootstrap : Using hsn0:172.28.39.104<0>
0: nid007116:202903:202903 [3] NCCL INFO NCCL version 2.22.3+cuda12.6
2: nid007119:146139:146624 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
2: nid007119:146139:146624 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
2: nid007119:146139:146624 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
2: nid007119:146139:146624 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
2: nid007119:146139:146624 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
2: nid007119:146139:146624 [3] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
2: nid007119:146137:146625 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
2: nid007119:146137:146625 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
2: nid007119:146137:146625 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
2: nid007119:146137:146625 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
2: nid007119:146137:146625 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
2: nid007119:146137:146625 [1] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
7: nid007124:126395:126878 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
7: nid007124:126395:126878 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
7: nid007124:126395:126878 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
7: nid007124:126395:126878 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
7: nid007124:126395:126878 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
7: nid007124:126395:126878 [0] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
2: nid007119:146136:146627 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
2: nid007119:146136:146627 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
2: 
2: nid007119:146137:146625 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
2: 
2: nid007119:146139:146624 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
2: nid007119:146136:146627 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
2: nid007119:146136:146627 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
2: nid007119:146136:146627 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
2: nid007119:146136:146627 [0] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
2: nid007119:146138:146626 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
2: nid007119:146138:146626 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
2: nid007119:146138:146626 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
2: nid007119:146138:146626 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
2: nid007119:146138:146626 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
2: nid007119:146138:146626 [2] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
7: 
7: nid007124:126395:126878 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
7: nid007124:126395:126878 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
7: nid007124:126395:126878 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
7: nid007124:126395:126878 [0] NCCL INFO NET/OFI Creating one domain per process
0: nid007116:202900:203363 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
0: nid007116:202900:203363 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
0: nid007116:202900:203363 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
7: nid007124:126395:126878 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
0: nid007116:202900:203363 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
0: nid007116:202900:203363 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
0: nid007116:202900:203363 [0] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
7: nid007124:126395:126878 [0] NCCL INFO NET/OFI Support for global registrations: false
7: nid007124:126395:126878 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
0: 
0: nid007116:202900:203363 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
2: nid007119:146137:146625 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
2: nid007119:146137:146625 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
2: nid007119:146137:146625 [1] NCCL INFO NET/OFI Creating one domain per process
2: nid007119:146139:146624 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
2: nid007119:146139:146624 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
2: nid007119:146139:146624 [3] NCCL INFO NET/OFI Creating one domain per process
2: 
2: nid007119:146138:146626 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
0: nid007116:202900:203363 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
0: nid007116:202900:203363 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
0: nid007116:202900:203363 [0] NCCL INFO NET/OFI Creating one domain per process
2: 
2: nid007119:146136:146627 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
7: nid007124:126395:126878 [0] NCCL INFO Using network AWS Libfabric
0: nid007116:202900:203363 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
2: nid007119:146137:146625 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
2: nid007119:146139:146624 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
5: nid007122:66177:66674 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
5: nid007122:66178:66673 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
5: nid007122:66178:66673 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
5: nid007122:66177:66674 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
5: nid007122:66177:66674 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
5: nid007122:66178:66673 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
5: nid007122:66177:66674 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
5: nid007122:66178:66673 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
5: nid007122:66177:66674 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
5: nid007122:66178:66673 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
5: nid007122:66178:66673 [3] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
5: nid007122:66177:66674 [2] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
5: nid007122:66175:66676 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
5: nid007122:66175:66676 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
5: nid007122:66175:66676 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
5: nid007122:66175:66676 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
5: nid007122:66175:66676 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
5: nid007122:66175:66676 [0] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
5: nid007122:66176:66675 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
5: nid007122:66176:66675 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
5: nid007122:66176:66675 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
5: nid007122:66176:66675 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
5: nid007122:66176:66675 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
5: nid007122:66176:66675 [1] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
0: nid007116:202900:203363 [0] NCCL INFO NET/OFI Support for global registrations: false
0: nid007116:202900:203363 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
6: nid007123:108289:108841 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
6: nid007123:108289:108841 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
6: nid007123:108289:108841 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
6: nid007123:108289:108841 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
6: nid007123:108289:108841 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
6: nid007123:108289:108841 [0] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
2: nid007119:146137:146625 [1] NCCL INFO NET/OFI Support for global registrations: false
2: nid007119:146137:146625 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
2: nid007119:146139:146624 [3] NCCL INFO NET/OFI Support for global registrations: false
2: nid007119:146139:146624 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
0: nid007116:202900:203363 [0] NCCL INFO Using network AWS Libfabric
2: nid007119:146138:146626 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
2: nid007119:146138:146626 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
2: nid007119:146138:146626 [2] NCCL INFO NET/OFI Creating one domain per process
2: nid007119:146138:146626 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
2: nid007119:146136:146627 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
2: nid007119:146136:146627 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
2: nid007119:146136:146627 [0] NCCL INFO NET/OFI Creating one domain per process
2: nid007119:146137:146625 [1] NCCL INFO Using network AWS Libfabric
2: nid007119:146136:146627 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
2: nid007119:146139:146624 [3] NCCL INFO Using network AWS Libfabric
7: [2025-07-11 11:47:41,239] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
6: 
6: nid007123:108289:108841 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
6: nid007123:108289:108841 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
6: nid007123:108289:108841 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
6: nid007123:108289:108841 [0] NCCL INFO NET/OFI Creating one domain per process
2: nid007119:146136:146627 [0] NCCL INFO NET/OFI Support for global registrations: false
2: nid007119:146136:146627 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
6: nid007123:108289:108841 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
2: nid007119:146138:146626 [2] NCCL INFO NET/OFI Support for global registrations: false
2: nid007119:146138:146626 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
2: nid007119:146136:146627 [0] NCCL INFO Using network AWS Libfabric
2: nid007119:146138:146626 [2] NCCL INFO Using network AWS Libfabric
6: nid007123:108289:108841 [0] NCCL INFO NET/OFI Support for global registrations: false
6: nid007123:108289:108841 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
5: 
5: nid007122:66177:66674 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
5: 
5: nid007122:66178:66673 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
5: 
5: nid007122:66176:66675 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
6: nid007123:108289:108841 [0] NCCL INFO Using network AWS Libfabric
5: 
5: nid007122:66175:66676 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
5: nid007122:66175:66676 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
5: nid007122:66175:66676 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
5: nid007122:66175:66676 [0] NCCL INFO NET/OFI Creating one domain per process
5: nid007122:66178:66673 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
5: nid007122:66178:66673 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
5: nid007122:66178:66673 [3] NCCL INFO NET/OFI Creating one domain per process
5: nid007122:66177:66674 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
5: nid007122:66177:66674 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
5: nid007122:66177:66674 [2] NCCL INFO NET/OFI Creating one domain per process
5: nid007122:66176:66675 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
5: nid007122:66176:66675 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
5: nid007122:66176:66675 [1] NCCL INFO NET/OFI Creating one domain per process
5: nid007122:66175:66676 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
5: nid007122:66178:66673 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
5: nid007122:66177:66674 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
5: nid007122:66176:66675 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
3: nid007120:197036:197518 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
3: nid007120:197036:197518 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
3: nid007120:197036:197518 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
3: nid007120:197036:197518 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
3: nid007120:197036:197518 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
3: nid007120:197036:197518 [0] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
5: nid007122:66176:66675 [1] NCCL INFO NET/OFI Support for global registrations: false
5: nid007122:66176:66675 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
5: nid007122:66175:66676 [0] NCCL INFO NET/OFI Support for global registrations: false
5: nid007122:66175:66676 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
5: nid007122:66178:66673 [3] NCCL INFO NET/OFI Support for global registrations: false
5: nid007122:66178:66673 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
5: nid007122:66177:66674 [2] NCCL INFO NET/OFI Support for global registrations: false
5: nid007122:66177:66674 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
5: nid007122:66176:66675 [1] NCCL INFO Using network AWS Libfabric
5: nid007122:66175:66676 [0] NCCL INFO Using network AWS Libfabric
5: nid007122:66178:66673 [3] NCCL INFO Using network AWS Libfabric
5: nid007122:66177:66674 [2] NCCL INFO Using network AWS Libfabric
3: 
3: nid007120:197036:197518 [0] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
3: nid007120:197036:197518 [0] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
3: nid007120:197036:197518 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
3: nid007120:197036:197518 [0] NCCL INFO NET/OFI Creating one domain per process
3: nid007120:197036:197518 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
3: nid007120:197036:197518 [0] NCCL INFO NET/OFI Support for global registrations: false
3: nid007120:197036:197518 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
3: nid007120:197036:197518 [0] NCCL INFO Using network AWS Libfabric
1: [2025-07-11 11:47:41,306] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
1: [2025-07-11 11:47:41,319] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
1: [2025-07-11 11:47:41,321] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
4: [2025-07-11 11:47:41,337] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
6: [2025-07-11 11:47:41,357] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
6: [2025-07-11 11:47:41,400] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
4: [2025-07-11 11:47:41,424] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
7: [2025-07-11 11:47:41,430] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
3: [2025-07-11 11:47:41,433] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
4: nid007121:116201:116666 [0] NCCL INFO DMA-BUF is available on GPU device 0
4: nid007121:116204:116204 [3] NCCL INFO cudaDriverVersion 12060
4: nid007121:116204:116204 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
4: nid007121:116204:116204 [3] NCCL INFO Bootstrap : Using hsn0:172.28.39.124<0>
4: nid007121:116204:116204 [3] NCCL INFO NCCL version 2.22.3+cuda12.6
4: nid007121:116202:116202 [1] NCCL INFO cudaDriverVersion 12060
4: nid007121:116202:116202 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
4: nid007121:116202:116202 [1] NCCL INFO Bootstrap : Using hsn0:172.28.39.124<0>
4: nid007121:116203:116203 [2] NCCL INFO cudaDriverVersion 12060
4: nid007121:116202:116202 [1] NCCL INFO NCCL version 2.22.3+cuda12.6
4: nid007121:116203:116203 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
4: nid007121:116203:116203 [2] NCCL INFO Bootstrap : Using hsn0:172.28.39.124<0>
4: nid007121:116203:116203 [2] NCCL INFO NCCL version 2.22.3+cuda12.6
6: [2025-07-11 11:47:41,480] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
4: nid007121:116202:116681 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
4: nid007121:116204:116680 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
4: nid007121:116203:116682 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
4: nid007121:116203:116682 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
4: nid007121:116202:116681 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
4: nid007121:116204:116680 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
4: nid007121:116203:116682 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
4: nid007121:116204:116680 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
4: nid007121:116202:116681 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
4: nid007121:116203:116682 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
4: nid007121:116202:116681 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
4: nid007121:116204:116680 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
4: nid007121:116204:116680 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
4: nid007121:116203:116682 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
4: nid007121:116202:116681 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
4: nid007121:116203:116682 [2] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
4: nid007121:116202:116681 [1] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
4: nid007121:116204:116680 [3] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
4: 
4: nid007121:116202:116681 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
4: 
4: nid007121:116204:116680 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
4: 
4: nid007121:116203:116682 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
4: nid007121:116204:116680 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
4: nid007121:116204:116680 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
4: nid007121:116204:116680 [3] NCCL INFO NET/OFI Creating one domain per process
4: nid007121:116202:116681 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
4: nid007121:116202:116681 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
4: nid007121:116202:116681 [1] NCCL INFO NET/OFI Creating one domain per process
4: nid007121:116203:116682 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
4: nid007121:116203:116682 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
4: nid007121:116203:116682 [2] NCCL INFO NET/OFI Creating one domain per process
4: nid007121:116204:116680 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
4: nid007121:116202:116681 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
4: nid007121:116203:116682 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
4: nid007121:116203:116682 [2] NCCL INFO NET/OFI Support for global registrations: false
4: nid007121:116203:116682 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
4: nid007121:116204:116680 [3] NCCL INFO NET/OFI Support for global registrations: false
4: nid007121:116204:116680 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
4: nid007121:116202:116681 [1] NCCL INFO NET/OFI Support for global registrations: false
4: nid007121:116202:116681 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
4: nid007121:116203:116682 [2] NCCL INFO Using network AWS Libfabric
4: nid007121:116204:116680 [3] NCCL INFO Using network AWS Libfabric
4: nid007121:116202:116681 [1] NCCL INFO Using network AWS Libfabric
0: [2025-07-11 11:47:41,606] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
7: [2025-07-11 11:47:41,659] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
1: nid007117:210405:210942 [0] NCCL INFO DMA-BUF is available on GPU device 0
1: nid007117:210407:210407 [2] NCCL INFO cudaDriverVersion 12060
1: nid007117:210407:210407 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
1: nid007117:210407:210407 [2] NCCL INFO Bootstrap : Using hsn0:172.28.39.108<0>
1: nid007117:210407:210407 [2] NCCL INFO NCCL version 2.22.3+cuda12.6
1: nid007117:210406:210406 [1] NCCL INFO cudaDriverVersion 12060
1: nid007117:210406:210406 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
1: nid007117:210408:210408 [3] NCCL INFO cudaDriverVersion 12060
1: nid007117:210408:210408 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
1: nid007117:210406:210406 [1] NCCL INFO Bootstrap : Using hsn0:172.28.39.108<0>
1: nid007117:210408:210408 [3] NCCL INFO Bootstrap : Using hsn0:172.28.39.108<0>
1: nid007117:210406:210406 [1] NCCL INFO NCCL version 2.22.3+cuda12.6
1: nid007117:210408:210408 [3] NCCL INFO NCCL version 2.22.3+cuda12.6
0: nid007116:202900:203363 [0] NCCL INFO DMA-BUF is available on GPU device 0
7: nid007124:126395:126878 [0] NCCL INFO DMA-BUF is available on GPU device 0
0: nid007116:202901:202901 [1] NCCL INFO cudaDriverVersion 12060
0: nid007116:202901:202901 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
0: nid007116:202901:202901 [1] NCCL INFO Bootstrap : Using hsn0:172.28.39.104<0>
0: nid007116:202901:202901 [1] NCCL INFO NCCL version 2.22.3+cuda12.6
7: nid007124:126398:126398 [3] NCCL INFO cudaDriverVersion 12060
7: nid007124:126398:126398 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
7: nid007124:126398:126398 [3] NCCL INFO Bootstrap : Using hsn0:172.28.39.136<0>
7: nid007124:126398:126398 [3] NCCL INFO NCCL version 2.22.3+cuda12.6
0: nid007116:202903:203373 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
0: nid007116:202902:203372 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
0: nid007116:202903:203373 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
0: nid007116:202902:203372 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
0: nid007116:202902:203372 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
0: nid007116:202903:203373 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
0: nid007116:202902:203372 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
0: nid007116:202903:203373 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
0: nid007116:202902:203372 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
0: nid007116:202903:203373 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
0: nid007116:202903:203373 [3] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
0: nid007116:202902:203372 [2] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
7: nid007124:126397:126397 [2] NCCL INFO cudaDriverVersion 12060
7: nid007124:126397:126397 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
7: nid007124:126397:126397 [2] NCCL INFO Bootstrap : Using hsn0:172.28.39.136<0>
7: nid007124:126397:126397 [2] NCCL INFO NCCL version 2.22.3+cuda12.6
7: nid007124:126396:126396 [1] NCCL INFO cudaDriverVersion 12060
7: nid007124:126396:126396 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
7: nid007124:126396:126396 [1] NCCL INFO Bootstrap : Using hsn0:172.28.39.136<0>
7: nid007124:126396:126396 [1] NCCL INFO NCCL version 2.22.3+cuda12.6
0: 
0: nid007116:202902:203372 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
0: 
0: nid007116:202903:203373 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
0: nid007116:202902:203372 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
0: nid007116:202902:203372 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
0: nid007116:202902:203372 [2] NCCL INFO NET/OFI Creating one domain per process
0: nid007116:202903:203373 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
0: nid007116:202903:203373 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
0: nid007116:202903:203373 [3] NCCL INFO NET/OFI Creating one domain per process
0: nid007116:202902:203372 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
0: nid007116:202903:203373 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
6: nid007123:108289:108841 [0] NCCL INFO DMA-BUF is available on GPU device 0
0: nid007116:202903:203373 [3] NCCL INFO NET/OFI Support for global registrations: false
0: nid007116:202902:203372 [2] NCCL INFO NET/OFI Support for global registrations: false
0: nid007116:202903:203373 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
0: nid007116:202902:203372 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
0: nid007116:202902:203372 [2] NCCL INFO Using network AWS Libfabric
0: nid007116:202903:203373 [3] NCCL INFO Using network AWS Libfabric
6: nid007123:108292:108292 [3] NCCL INFO cudaDriverVersion 12060
6: nid007123:108292:108292 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
6: nid007123:108292:108292 [3] NCCL INFO Bootstrap : Using hsn0:172.28.39.132<0>
6: nid007123:108292:108292 [3] NCCL INFO NCCL version 2.22.3+cuda12.6
1: nid007117:210407:210956 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
1: nid007117:210407:210956 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
1: nid007117:210407:210956 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
1: nid007117:210407:210956 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
1: nid007117:210407:210956 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
1: nid007117:210407:210956 [2] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
1: nid007117:210408:210957 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
1: nid007117:210408:210957 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
1: nid007117:210408:210957 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
1: nid007117:210408:210957 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
1: nid007117:210408:210957 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
1: nid007117:210408:210957 [3] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
1: nid007117:210406:210958 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
1: nid007117:210406:210958 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
1: nid007117:210406:210958 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
1: nid007117:210406:210958 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
1: nid007117:210406:210958 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
1: nid007117:210406:210958 [1] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
1: 
1: nid007117:210407:210956 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
1: 
1: nid007117:210408:210957 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
6: nid007123:108291:108291 [2] NCCL INFO cudaDriverVersion 12060
6: nid007123:108291:108291 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
6: nid007123:108291:108291 [2] NCCL INFO Bootstrap : Using hsn0:172.28.39.132<0>
6: nid007123:108291:108291 [2] NCCL INFO NCCL version 2.22.3+cuda12.6
6: nid007123:108290:108290 [1] NCCL INFO cudaDriverVersion 12060
6: nid007123:108290:108290 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
1: 
1: nid007117:210406:210958 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
6: nid007123:108290:108290 [1] NCCL INFO Bootstrap : Using hsn0:172.28.39.132<0>
6: nid007123:108290:108290 [1] NCCL INFO NCCL version 2.22.3+cuda12.6
1: nid007117:210407:210956 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
1: nid007117:210407:210956 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
1: nid007117:210407:210956 [2] NCCL INFO NET/OFI Creating one domain per process
1: nid007117:210407:210956 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
1: nid007117:210408:210957 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
1: nid007117:210408:210957 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
1: nid007117:210408:210957 [3] NCCL INFO NET/OFI Creating one domain per process
1: nid007117:210408:210957 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
1: nid007117:210407:210956 [2] NCCL INFO NET/OFI Support for global registrations: false
1: nid007117:210407:210956 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
1: nid007117:210406:210958 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
1: nid007117:210406:210958 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
1: nid007117:210406:210958 [1] NCCL INFO NET/OFI Creating one domain per process
1: nid007117:210406:210958 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
1: nid007117:210407:210956 [2] NCCL INFO Using network AWS Libfabric
1: nid007117:210408:210957 [3] NCCL INFO NET/OFI Support for global registrations: false
1: nid007117:210408:210957 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
1: nid007117:210406:210958 [1] NCCL INFO NET/OFI Support for global registrations: false
1: nid007117:210406:210958 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
1: nid007117:210408:210957 [3] NCCL INFO Using network AWS Libfabric
1: nid007117:210406:210958 [1] NCCL INFO Using network AWS Libfabric
3: nid007120:197036:197518 [0] NCCL INFO DMA-BUF is available on GPU device 0
7: nid007124:126398:126892 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
7: nid007124:126398:126892 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
7: nid007124:126398:126892 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
7: nid007124:126398:126892 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
7: nid007124:126398:126892 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
7: nid007124:126398:126892 [3] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
3: nid007120:197039:197039 [3] NCCL INFO cudaDriverVersion 12060
3: nid007120:197039:197039 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
3: nid007120:197039:197039 [3] NCCL INFO Bootstrap : Using hsn0:172.28.39.120<0>
3: nid007120:197039:197039 [3] NCCL INFO NCCL version 2.22.3+cuda12.6
7: nid007124:126396:126894 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
7: nid007124:126397:126893 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
7: nid007124:126396:126894 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
7: nid007124:126397:126893 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
7: nid007124:126396:126894 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
7: nid007124:126397:126893 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
7: nid007124:126396:126894 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
7: nid007124:126397:126893 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
7: nid007124:126396:126894 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
7: nid007124:126397:126893 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
7: 
7: nid007124:126398:126892 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
7: nid007124:126396:126894 [1] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
7: nid007124:126397:126893 [2] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
7: nid007124:126398:126892 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
7: nid007124:126398:126892 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
7: nid007124:126398:126892 [3] NCCL INFO NET/OFI Creating one domain per process
7: nid007124:126398:126892 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
7: 
7: nid007124:126397:126893 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
7: 
7: nid007124:126396:126894 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
7: nid007124:126398:126892 [3] NCCL INFO NET/OFI Support for global registrations: false
7: nid007124:126398:126892 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
7: nid007124:126398:126892 [3] NCCL INFO Using network AWS Libfabric
7: nid007124:126397:126893 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
7: nid007124:126397:126893 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
7: nid007124:126397:126893 [2] NCCL INFO NET/OFI Creating one domain per process
7: nid007124:126396:126894 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
7: nid007124:126396:126894 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
7: nid007124:126396:126894 [1] NCCL INFO NET/OFI Creating one domain per process
7: nid007124:126397:126893 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
7: nid007124:126396:126894 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
6: nid007123:108292:108857 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
6: nid007123:108292:108857 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
6: nid007123:108292:108857 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
6: nid007123:108292:108857 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
6: nid007123:108292:108857 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
6: nid007123:108292:108857 [3] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
6: 
6: nid007123:108292:108857 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
7: nid007124:126396:126894 [1] NCCL INFO NET/OFI Support for global registrations: false
7: nid007124:126396:126894 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
7: nid007124:126397:126893 [2] NCCL INFO NET/OFI Support for global registrations: false
7: nid007124:126397:126893 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
6: nid007123:108292:108857 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
6: nid007123:108292:108857 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
6: nid007123:108292:108857 [3] NCCL INFO NET/OFI Creating one domain per process
6: nid007123:108292:108857 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
7: nid007124:126396:126894 [1] NCCL INFO Using network AWS Libfabric
7: nid007124:126397:126893 [2] NCCL INFO Using network AWS Libfabric
6: nid007123:108292:108857 [3] NCCL INFO NET/OFI Support for global registrations: false
6: nid007123:108292:108857 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
6: nid007123:108292:108857 [3] NCCL INFO Using network AWS Libfabric
6: nid007123:108290:108859 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
6: nid007123:108290:108859 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
6: nid007123:108290:108859 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
6: nid007123:108290:108859 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
6: nid007123:108290:108859 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
6: nid007123:108290:108859 [1] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
6: 
6: nid007123:108290:108859 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
6: nid007123:108290:108859 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
6: nid007123:108290:108859 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
6: nid007123:108290:108859 [1] NCCL INFO NET/OFI Creating one domain per process
6: nid007123:108290:108859 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
6: nid007123:108290:108859 [1] NCCL INFO NET/OFI Support for global registrations: false
6: nid007123:108290:108859 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
6: nid007123:108290:108859 [1] NCCL INFO Using network AWS Libfabric
3: nid007120:197039:197528 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
3: nid007120:197039:197528 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
3: nid007120:197039:197528 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
3: nid007120:197039:197528 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
3: nid007120:197039:197528 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
3: nid007120:197039:197528 [3] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
3: 
3: nid007120:197039:197528 [3] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
3: [2025-07-11 11:47:41,984] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
3: nid007120:197039:197528 [3] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
3: nid007120:197039:197528 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
3: nid007120:197039:197528 [3] NCCL INFO NET/OFI Creating one domain per process
3: nid007120:197039:197528 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
3: nid007120:197039:197528 [3] NCCL INFO NET/OFI Support for global registrations: false
3: nid007120:197039:197528 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
3: nid007120:197039:197528 [3] NCCL INFO Using network AWS Libfabric
6: nid007123:108291:108858 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
6: nid007123:108291:108858 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
6: nid007123:108291:108858 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
6: nid007123:108291:108858 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
6: nid007123:108291:108858 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
6: nid007123:108291:108858 [2] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
6: 
6: nid007123:108291:108858 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
6: nid007123:108291:108858 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
6: nid007123:108291:108858 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
6: nid007123:108291:108858 [2] NCCL INFO NET/OFI Creating one domain per process
6: nid007123:108291:108858 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
6: nid007123:108291:108858 [2] NCCL INFO NET/OFI Support for global registrations: false
6: nid007123:108291:108858 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
6: nid007123:108291:108858 [2] NCCL INFO Using network AWS Libfabric
3: [2025-07-11 11:47:42,408] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
3: nid007120:197039:197528 [3] NCCL INFO DMA-BUF is available on GPU device 3
3: nid007120:197037:197037 [1] NCCL INFO cudaDriverVersion 12060
3: nid007120:197037:197037 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
3: nid007120:197037:197037 [1] NCCL INFO Bootstrap : Using hsn0:172.28.39.120<0>
3: nid007120:197037:197037 [1] NCCL INFO NCCL version 2.22.3+cuda12.6
3: nid007120:197038:197038 [2] NCCL INFO cudaDriverVersion 12060
3: nid007120:197038:197038 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to hsn
3: nid007120:197038:197038 [2] NCCL INFO Bootstrap : Using hsn0:172.28.39.120<0>
3: nid007120:197038:197038 [2] NCCL INFO NCCL version 2.22.3+cuda12.6
3: nid007120:197037:197534 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
3: nid007120:197037:197534 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
3: nid007120:197037:197534 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
3: nid007120:197037:197534 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
3: nid007120:197037:197534 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
3: nid007120:197037:197534 [1] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
3: 
3: nid007120:197037:197534 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
3: nid007120:197037:197534 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
3: nid007120:197037:197534 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
3: nid007120:197037:197534 [1] NCCL INFO NET/OFI Creating one domain per process
3: nid007120:197037:197534 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
3: nid007120:197037:197534 [1] NCCL INFO NET/OFI Support for global registrations: false
3: nid007120:197037:197534 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
3: nid007120:197037:197534 [1] NCCL INFO Using network AWS Libfabric
3: nid007120:197038:197535 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
3: nid007120:197038:197535 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
3: nid007120:197038:197535 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
3: nid007120:197038:197535 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
3: nid007120:197038:197535 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
3: nid007120:197038:197535 [2] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
3: 
3: nid007120:197038:197535 [2] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
3: nid007120:197038:197535 [2] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
3: nid007120:197038:197535 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
3: nid007120:197038:197535 [2] NCCL INFO NET/OFI Creating one domain per process
3: nid007120:197038:197535 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
3: nid007120:197038:197535 [2] NCCL INFO NET/OFI Support for global registrations: false
3: nid007120:197038:197535 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
3: nid007120:197038:197535 [2] NCCL INFO Using network AWS Libfabric
0: nid007116:202903:203373 [3] NCCL INFO DMA-BUF is available on GPU device 3
0: nid007116:202902:203372 [2] NCCL INFO DMA-BUF is available on GPU device 2
0: nid007116:202901:203379 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net-ofi.so
0: nid007116:202901:203379 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
0: nid007116:202901:203379 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
0: nid007116:202901:203379 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.14.0
0: nid007116:202901:203379 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
0: nid007116:202901:203379 [1] NCCL INFO NET/OFI Using CUDA driver version 12060 with runtime 12060
0: 
0: nid007116:202901:203379 [1] nccl_net_ofi_rdma_init:7978 NCCL WARN NET/OFI OFI fi_getinfo() call failed: No data available
0: nid007116:202901:203379 [1] NCCL INFO NET/OFI Selected provider is cxi, fabric is cxi (found 4 nics)
0: nid007116:202901:203379 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
0: nid007116:202901:203379 [1] NCCL INFO NET/OFI Creating one domain per process
0: nid007116:202901:203379 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
0: nid007116:202901:203379 [1] NCCL INFO NET/OFI Support for global registrations: false
0: nid007116:202901:203379 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
0: nid007116:202901:203379 [1] NCCL INFO Using network AWS Libfabric
4: nid007121:116203:116682 [2] NCCL INFO DMA-BUF is available on GPU device 2
4: nid007121:116202:116681 [1] NCCL INFO DMA-BUF is available on GPU device 1
4: nid007121:116204:116680 [3] NCCL INFO DMA-BUF is available on GPU device 3
2: nid007119:146137:146625 [1] NCCL INFO DMA-BUF is available on GPU device 1
6: nid007123:108292:108857 [3] NCCL INFO DMA-BUF is available on GPU device 3
2: nid007119:146139:146624 [3] NCCL INFO DMA-BUF is available on GPU device 3
2: nid007119:146136:146627 [0] NCCL INFO DMA-BUF is available on GPU device 0
5: nid007122:66176:66675 [1] NCCL INFO DMA-BUF is available on GPU device 1
2: nid007119:146138:146626 [2] NCCL INFO DMA-BUF is available on GPU device 2
5: nid007122:66175:66676 [0] NCCL INFO DMA-BUF is available on GPU device 0
5: nid007122:66177:66674 [2] NCCL INFO DMA-BUF is available on GPU device 2
5: nid007122:66178:66673 [3] NCCL INFO DMA-BUF is available on GPU device 3
1: nid007117:210407:210956 [2] NCCL INFO DMA-BUF is available on GPU device 2
1: nid007117:210408:210957 [3] NCCL INFO DMA-BUF is available on GPU device 3
1: nid007117:210406:210958 [1] NCCL INFO DMA-BUF is available on GPU device 1
0: nid007116:202901:203379 [1] NCCL INFO DMA-BUF is available on GPU device 1
7: nid007124:126398:126892 [3] NCCL INFO DMA-BUF is available on GPU device 3
6: nid007123:108290:108859 [1] NCCL INFO DMA-BUF is available on GPU device 1
7: nid007124:126397:126893 [2] NCCL INFO DMA-BUF is available on GPU device 2
7: nid007124:126396:126894 [1] NCCL INFO DMA-BUF is available on GPU device 1
6: nid007123:108291:108858 [2] NCCL INFO DMA-BUF is available on GPU device 2
3: nid007120:197037:197534 [1] NCCL INFO DMA-BUF is available on GPU device 1
3: nid007120:197038:197535 [2] NCCL INFO DMA-BUF is available on GPU device 2
0: nid007116:202902:203372 [2] NCCL INFO ncclCommInitRank comm 0xaaaaf1faa2e0 rank 2 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 commId 0x6d129679625a667c - Init START
0: nid007116:202901:203379 [1] NCCL INFO ncclCommInitRank comm 0xaaaaf4f9b440 rank 1 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 commId 0x6d129679625a667c - Init START
0: nid007116:202903:203373 [3] NCCL INFO ncclCommInitRank comm 0xaaab18df7db0 rank 3 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 commId 0x6d129679625a667c - Init START
0: nid007116:202900:203363 [0] NCCL INFO ncclCommInitRank comm 0xaaab0f53e270 rank 0 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 commId 0x6d129679625a667c - Init START
1: nid007117:210406:210958 [1] NCCL INFO ncclCommInitRank comm 0xaaaaf2c70c20 rank 5 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 commId 0x6d129679625a667c - Init START
1: nid007117:210407:210956 [2] NCCL INFO ncclCommInitRank comm 0xaaab2c3c27a0 rank 6 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 commId 0x6d129679625a667c - Init START
1: nid007117:210405:210942 [0] NCCL INFO ncclCommInitRank comm 0xaaab0fcf58a0 rank 4 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 commId 0x6d129679625a667c - Init START
7: nid007124:126398:126892 [3] NCCL INFO ncclCommInitRank comm 0xaaab12762370 rank 31 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 commId 0x6d129679625a667c - Init START
6: nid007123:108292:108857 [3] NCCL INFO ncclCommInitRank comm 0xaaaaf5620a20 rank 27 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 commId 0x6d129679625a667c - Init START
6: nid007123:108291:108858 [2] NCCL INFO ncclCommInitRank comm 0xaaab23840990 rank 26 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 commId 0x6d129679625a667c - Init START
7: nid007124:126397:126893 [2] NCCL INFO ncclCommInitRank comm 0xaaab10cea560 rank 30 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 commId 0x6d129679625a667c - Init START
4: nid007121:116204:116680 [3] NCCL INFO ncclCommInitRank comm 0xaaab22c089c0 rank 19 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 commId 0x6d129679625a667c - Init START
4: nid007121:116203:116682 [2] NCCL INFO ncclCommInitRank comm 0xaaab08232380 rank 18 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 commId 0x6d129679625a667c - Init START
4: nid007121:116201:116666 [0] NCCL INFO ncclCommInitRank comm 0xaaaacfcf80d0 rank 16 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 commId 0x6d129679625a667c - Init START
7: nid007124:126396:126894 [1] NCCL INFO ncclCommInitRank comm 0xaaab39820720 rank 29 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 commId 0x6d129679625a667c - Init START
6: nid007123:108289:108841 [0] NCCL INFO ncclCommInitRank comm 0xaaaaef3a5bf0 rank 24 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 commId 0x6d129679625a667c - Init START
7: nid007124:126395:126878 [0] NCCL INFO ncclCommInitRank comm 0xaaaaef48c330 rank 28 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 commId 0x6d129679625a667c - Init START
2: nid007119:146136:146627 [0] NCCL INFO ncclCommInitRank comm 0xaaab09168e10 rank 8 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 commId 0x6d129679625a667c - Init START
2: nid007119:146139:146624 [3] NCCL INFO ncclCommInitRank comm 0xaaab16111160 rank 11 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 commId 0x6d129679625a667c - Init START
2: nid007119:146137:146625 [1] NCCL INFO ncclCommInitRank comm 0xaaab1b0b7890 rank 9 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 commId 0x6d129679625a667c - Init START
2: nid007119:146138:146626 [2] NCCL INFO ncclCommInitRank comm 0xaaab072922f0 rank 10 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 commId 0x6d129679625a667c - Init START
6: nid007123:108290:108859 [1] NCCL INFO ncclCommInitRank comm 0xaaab083c2070 rank 25 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 commId 0x6d129679625a667c - Init START
1: nid007117:210408:210957 [3] NCCL INFO ncclCommInitRank comm 0xaaaaf8ea2d90 rank 7 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 commId 0x6d129679625a667c - Init START
4: nid007121:116202:116681 [1] NCCL INFO ncclCommInitRank comm 0xaaab2fe09390 rank 17 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 commId 0x6d129679625a667c - Init START
5: nid007122:66178:66673 [3] NCCL INFO ncclCommInitRank comm 0xaaab08462130 rank 23 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 commId 0x6d129679625a667c - Init START
5: nid007122:66175:66676 [0] NCCL INFO ncclCommInitRank comm 0xaaab0041ada0 rank 20 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 commId 0x6d129679625a667c - Init START
5: nid007122:66177:66674 [2] NCCL INFO ncclCommInitRank comm 0xaaaacb2d9e10 rank 22 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 commId 0x6d129679625a667c - Init START
5: nid007122:66176:66675 [1] NCCL INFO ncclCommInitRank comm 0xaaab1e05ac60 rank 21 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 commId 0x6d129679625a667c - Init START
3: nid007120:197038:197535 [2] NCCL INFO ncclCommInitRank comm 0xaaab036888d0 rank 14 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 commId 0x6d129679625a667c - Init START
3: nid007120:197039:197528 [3] NCCL INFO ncclCommInitRank comm 0xaaaaeadc27e0 rank 15 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 commId 0x6d129679625a667c - Init START
3: nid007120:197036:197518 [0] NCCL INFO ncclCommInitRank comm 0xaaaaffa2f9f0 rank 12 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 commId 0x6d129679625a667c - Init START
3: nid007120:197037:197534 [1] NCCL INFO ncclCommInitRank comm 0xaaaafddafcb0 rank 13 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 commId 0x6d129679625a667c - Init START
5: nid007122:66178:66673 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
5: nid007122:66178:66673 [3] NCCL INFO NVLS multicast support is not available on dev 3
5: nid007122:66178:66673 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
5: nid007122:66177:66674 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
5: nid007122:66176:66675 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
5: nid007122:66177:66674 [2] NCCL INFO NVLS multicast support is not available on dev 2
5: nid007122:66177:66674 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
5: nid007122:66176:66675 [1] NCCL INFO NVLS multicast support is not available on dev 1
5: nid007122:66176:66675 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
5: nid007122:66175:66676 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
5: nid007122:66175:66676 [0] NCCL INFO NVLS multicast support is not available on dev 0
5: nid007122:66175:66676 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
7: nid007124:126398:126892 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
7: nid007124:126398:126892 [3] NCCL INFO NVLS multicast support is not available on dev 3
7: nid007124:126398:126892 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
7: nid007124:126397:126893 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
7: nid007124:126396:126894 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
7: nid007124:126397:126893 [2] NCCL INFO NVLS multicast support is not available on dev 2
7: nid007124:126397:126893 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
7: nid007124:126396:126894 [1] NCCL INFO NVLS multicast support is not available on dev 1
7: nid007124:126396:126894 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
7: nid007124:126395:126878 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
7: nid007124:126395:126878 [0] NCCL INFO NVLS multicast support is not available on dev 0
2: nid007119:146136:146627 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
2: nid007119:146138:146626 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
2: nid007119:146139:146624 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
2: nid007119:146137:146625 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
2: nid007119:146139:146624 [3] NCCL INFO NVLS multicast support is not available on dev 3
2: nid007119:146139:146624 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
2: nid007119:146136:146627 [0] NCCL INFO NVLS multicast support is not available on dev 0
2: nid007119:146138:146626 [2] NCCL INFO NVLS multicast support is not available on dev 2
2: nid007119:146137:146625 [1] NCCL INFO NVLS multicast support is not available on dev 1
2: nid007119:146136:146627 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
2: nid007119:146138:146626 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
2: nid007119:146137:146625 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
1: nid007117:210406:210958 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
1: nid007117:210405:210942 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
1: nid007117:210407:210956 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
1: nid007117:210406:210958 [1] NCCL INFO NVLS multicast support is not available on dev 1
1: nid007117:210406:210958 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
1: nid007117:210408:210957 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
1: nid007117:210405:210942 [0] NCCL INFO NVLS multicast support is not available on dev 0
1: nid007117:210405:210942 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
1: nid007117:210407:210956 [2] NCCL INFO NVLS multicast support is not available on dev 2
1: nid007117:210407:210956 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
1: nid007117:210408:210957 [3] NCCL INFO NVLS multicast support is not available on dev 3
1: nid007117:210408:210957 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
0: nid007116:202903:203373 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
0: nid007116:202902:203372 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
0: nid007116:202903:203373 [3] NCCL INFO NVLS multicast support is not available on dev 3
0: nid007116:202903:203373 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
0: nid007116:202902:203372 [2] NCCL INFO NVLS multicast support is not available on dev 2
0: nid007116:202902:203372 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
0: nid007116:202900:203363 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
0: nid007116:202900:203363 [0] NCCL INFO NVLS multicast support is not available on dev 0
0: nid007116:202900:203363 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
0: nid007116:202901:203379 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
0: nid007116:202901:203379 [1] NCCL INFO NVLS multicast support is not available on dev 1
0: nid007116:202901:203379 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
7: nid007124:126395:126878 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
3: nid007120:197036:197518 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
3: nid007120:197036:197518 [0] NCCL INFO NVLS multicast support is not available on dev 0
3: nid007120:197036:197518 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
3: nid007120:197038:197535 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
3: nid007120:197039:197528 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
3: nid007120:197038:197535 [2] NCCL INFO NVLS multicast support is not available on dev 2
3: nid007120:197038:197535 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
3: nid007120:197039:197528 [3] NCCL INFO NVLS multicast support is not available on dev 3
3: nid007120:197039:197528 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
3: nid007120:197037:197534 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
3: nid007120:197037:197534 [1] NCCL INFO NVLS multicast support is not available on dev 1
3: nid007120:197037:197534 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
4: nid007121:116201:116666 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
4: nid007121:116201:116666 [0] NCCL INFO NVLS multicast support is not available on dev 0
4: nid007121:116201:116666 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
4: nid007121:116204:116680 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
4: nid007121:116204:116680 [3] NCCL INFO NVLS multicast support is not available on dev 3
4: nid007121:116204:116680 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
4: nid007121:116203:116682 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
4: nid007121:116203:116682 [2] NCCL INFO NVLS multicast support is not available on dev 2
4: nid007121:116203:116682 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
4: nid007121:116202:116681 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
4: nid007121:116202:116681 [1] NCCL INFO NVLS multicast support is not available on dev 1
4: nid007121:116202:116681 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
6: nid007123:108289:108841 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
6: nid007123:108292:108857 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
6: nid007123:108289:108841 [0] NCCL INFO NVLS multicast support is not available on dev 0
6: nid007123:108292:108857 [3] NCCL INFO NVLS multicast support is not available on dev 3
6: nid007123:108292:108857 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
6: nid007123:108289:108841 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
6: nid007123:108291:108858 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
6: nid007123:108290:108859 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
6: nid007123:108290:108859 [1] NCCL INFO NVLS multicast support is not available on dev 1
6: nid007123:108290:108859 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
6: nid007123:108291:108858 [2] NCCL INFO NVLS multicast support is not available on dev 2
6: nid007123:108291:108858 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
1: nid007117:210408:210957 [3] NCCL INFO comm 0xaaaaf8ea2d90 rank 7 nRanks 32 nNodes 8 localRanks 4 localRank 3 MNNVL 0
1: nid007117:210407:210956 [2] NCCL INFO comm 0xaaab2c3c27a0 rank 6 nRanks 32 nNodes 8 localRanks 4 localRank 2 MNNVL 0
0: nid007116:202902:203372 [2] NCCL INFO comm 0xaaaaf1faa2e0 rank 2 nRanks 32 nNodes 8 localRanks 4 localRank 2 MNNVL 0
0: nid007116:202903:203373 [3] NCCL INFO comm 0xaaab18df7db0 rank 3 nRanks 32 nNodes 8 localRanks 4 localRank 3 MNNVL 0
0: nid007116:202902:203372 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/18/-1->2->-1 [3] 3/18/-1->2->-1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->6 [7] 3/-1/-1->2->6
0: nid007116:202903:203373 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 0/-1/-1->3->2 [3] 0/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] 0/-1/-1->3->2 [7] 0/-1/-1->3->2
0: nid007116:202902:203372 [2] NCCL INFO P2P Chunksize set to 131072
1: nid007117:210408:210957 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] 4/-1/-1->7->6 [3] 4/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] 4/-1/-1->7->6 [7] 4/-1/-1->7->6
1: nid007117:210407:210956 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->10 [3] 7/-1/-1->6->10 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/10/2->6->14 [7] 7/10/2->6->14
1: nid007117:210405:210942 [0] NCCL INFO comm 0xaaab0fcf58a0 rank 4 nRanks 32 nNodes 8 localRanks 4 localRank 0 MNNVL 0
1: nid007117:210408:210957 [3] NCCL INFO P2P Chunksize set to 131072
1: nid007117:210406:210958 [1] NCCL INFO comm 0xaaaaf2c70c20 rank 5 nRanks 32 nNodes 8 localRanks 4 localRank 1 MNNVL 0
0: nid007116:202903:203373 [3] NCCL INFO P2P Chunksize set to 131072
0: nid007116:202900:203363 [0] NCCL INFO comm 0xaaab0f53e270 rank 0 nRanks 32 nNodes 8 localRanks 4 localRank 0 MNNVL 0
0: nid007116:202901:203379 [1] NCCL INFO comm 0xaaaaf4f9b440 rank 1 nRanks 32 nNodes 8 localRanks 4 localRank 1 MNNVL 0
1: nid007117:210407:210956 [2] NCCL INFO P2P Chunksize set to 131072
0: nid007116:202900:203363 [0] NCCL INFO Channel 00/08 :    0   1   2   3   7   6   5   4   8   9  10  11  15  14  13  12  16  17  18  19
0: nid007116:202900:203363 [0] NCCL INFO Channel 01/08 :    0   4   5   6   7  11  10   9   8  12  13  14  15  19  18  17  16  20  21  22
0: nid007116:202901:203379 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] -1/-1/-1->1->0 [3] -1/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] -1/-1/-1->1->0 [7] -1/-1/-1->1->0
0: nid007116:202900:203363 [0] NCCL INFO Channel 02/08 :    0   3   1   5   4   7   6  10   8  11   9  13  12  15  14  18  16  19  17  21
0: nid007116:202901:203379 [1] NCCL INFO P2P Chunksize set to 131072
5: nid007122:66178:66673 [3] NCCL INFO comm 0xaaab08462130 rank 23 nRanks 32 nNodes 8 localRanks 4 localRank 3 MNNVL 0
7: nid007124:126396:126894 [1] NCCL INFO comm 0xaaab39820720 rank 29 nRanks 32 nNodes 8 localRanks 4 localRank 1 MNNVL 0
7: nid007124:126397:126893 [2] NCCL INFO comm 0xaaab10cea560 rank 30 nRanks 32 nNodes 8 localRanks 4 localRank 2 MNNVL 0
7: nid007124:126398:126892 [3] NCCL INFO comm 0xaaab12762370 rank 31 nRanks 32 nNodes 8 localRanks 4 localRank 3 MNNVL 0
7: nid007124:126395:126878 [0] NCCL INFO comm 0xaaaaef48c330 rank 28 nRanks 32 nNodes 8 localRanks 4 localRank 0 MNNVL 0
3: nid007120:197039:197528 [3] NCCL INFO comm 0xaaaaeadc27e0 rank 15 nRanks 32 nNodes 8 localRanks 4 localRank 3 MNNVL 0
6: nid007123:108292:108857 [3] NCCL INFO comm 0xaaaaf5620a20 rank 27 nRanks 32 nNodes 8 localRanks 4 localRank 3 MNNVL 0
1: nid007117:210405:210942 [0] NCCL INFO Trees [0] 5/-1/-1->4->8 [1] 5/-1/-1->4->8 [2] 5/-1/-1->4->7 [3] 5/-1/-1->4->7 [4] 5/8/0->4->12 [5] 5/8/0->4->12 [6] 5/-1/-1->4->7 [7] 5/-1/-1->4->7
1: nid007117:210405:210942 [0] NCCL INFO P2P Chunksize set to 131072
0: nid007116:202900:203363 [0] NCCL INFO Channel 03/08 :    0   3   2   6   4   7   5   9   8  11  10  14  12  15  13  17  16  19  18  22
0: nid007116:202900:203363 [0] NCCL INFO Channel 04/08 :    0   1   2   3   7   6   5   4   8   9  10  11  15  14  13  12  16  17  18  19
7: nid007124:126396:126894 [1] NCCL INFO Trees [0] 30/-1/-1->29->28 [1] 30/-1/-1->29->28 [2] -1/-1/-1->29->28 [3] -1/-1/-1->29->28 [4] 30/-1/-1->29->28 [5] 30/-1/-1->29->28 [6] -1/-1/-1->29->28 [7] -1/-1/-1->29->28
7: nid007124:126396:126894 [1] NCCL INFO P2P Chunksize set to 131072
7: nid007124:126397:126893 [2] NCCL INFO Trees [0] 31/-1/-1->30->29 [1] 31/-1/-1->30->29 [2] 31/-1/-1->30->26 [3] 31/-1/-1->30->26 [4] 31/-1/-1->30->29 [5] 31/-1/-1->30->29 [6] 31/14/-1->30->-1 [7] 31/14/-1->30->-1
7: nid007124:126397:126893 [2] NCCL INFO P2P Chunksize set to 131072
3: nid007120:197039:197528 [3] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] -1/-1/-1->15->14 [2] 12/-1/-1->15->14 [3] 12/-1/-1->15->14 [4] -1/-1/-1->15->14 [5] -1/-1/-1->15->14 [6] 12/-1/-1->15->14 [7] 12/-1/-1->15->14
3: nid007120:197039:197528 [3] NCCL INFO P2P Chunksize set to 131072
6: nid007123:108290:108859 [1] NCCL INFO comm 0xaaab083c2070 rank 25 nRanks 32 nNodes 8 localRanks 4 localRank 1 MNNVL 0
6: nid007123:108291:108858 [2] NCCL INFO comm 0xaaab23840990 rank 26 nRanks 32 nNodes 8 localRanks 4 localRank 2 MNNVL 0
1: nid007117:210406:210958 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] -1/-1/-1->5->4 [3] -1/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] -1/-1/-1->5->4 [7] -1/-1/-1->5->4
1: nid007117:210406:210958 [1] NCCL INFO P2P Chunksize set to 131072
0: nid007116:202900:203363 [0] NCCL INFO Channel 05/08 :    0   4   5   6   7  11  10   9   8  12  13  14  15  19  18  17  16  20  21  22
0: nid007116:202900:203363 [0] NCCL INFO Channel 06/08 :    0   3   1   5   4   7   6  10   8  11   9  13  12  15  14  18  16  19  17  21
0: nid007116:202900:203363 [0] NCCL INFO Channel 07/08 :    0   3   2   6   4   7   5   9   8  11  10  14  12  15  13  17  16  19  18  22
4: nid007121:116202:116681 [1] NCCL INFO comm 0xaaab2fe09390 rank 17 nRanks 32 nNodes 8 localRanks 4 localRank 1 MNNVL 0
4: nid007121:116203:116682 [2] NCCL INFO comm 0xaaab08232380 rank 18 nRanks 32 nNodes 8 localRanks 4 localRank 2 MNNVL 0
4: nid007121:116204:116680 [3] NCCL INFO comm 0xaaab22c089c0 rank 19 nRanks 32 nNodes 8 localRanks 4 localRank 3 MNNVL 0
5: nid007122:66178:66673 [3] NCCL INFO Trees [0] -1/-1/-1->23->22 [1] -1/-1/-1->23->22 [2] 20/-1/-1->23->22 [3] 20/-1/-1->23->22 [4] -1/-1/-1->23->22 [5] -1/-1/-1->23->22 [6] 20/-1/-1->23->22 [7] 20/-1/-1->23->22
5: nid007122:66178:66673 [3] NCCL INFO P2P Chunksize set to 131072
7: nid007124:126398:126892 [3] NCCL INFO Trees [0] -1/-1/-1->31->30 [1] -1/-1/-1->31->30 [2] 28/-1/-1->31->30 [3] 28/-1/-1->31->30 [4] -1/-1/-1->31->30 [5] -1/-1/-1->31->30 [6] 28/-1/-1->31->30 [7] 28/-1/-1->31->30
7: nid007124:126398:126892 [3] NCCL INFO P2P Chunksize set to 131072
7: nid007124:126395:126878 [0] NCCL INFO Trees [0] 29/-1/-1->28->24 [1] 29/-1/-1->28->24 [2] 29/-1/-1->28->31 [3] 29/-1/-1->28->31 [4] 29/12/-1->28->-1 [5] 29/12/-1->28->-1 [6] 29/-1/-1->28->31 [7] 29/-1/-1->28->31
2: nid007119:146137:146625 [1] NCCL INFO comm 0xaaab1b0b7890 rank 9 nRanks 32 nNodes 8 localRanks 4 localRank 1 MNNVL 0
3: nid007120:197038:197535 [2] NCCL INFO comm 0xaaab036888d0 rank 14 nRanks 32 nNodes 8 localRanks 4 localRank 2 MNNVL 0
6: nid007123:108292:108857 [3] NCCL INFO Trees [0] -1/-1/-1->27->26 [1] -1/-1/-1->27->26 [2] 24/-1/-1->27->26 [3] 24/-1/-1->27->26 [4] -1/-1/-1->27->26 [5] -1/-1/-1->27->26 [6] 24/-1/-1->27->26 [7] 24/-1/-1->27->26
0: nid007116:202900:203363 [0] NCCL INFO Trees [0] 1/16/-1->0->-1 [1] 1/16/-1->0->-1 [2] 1/-1/-1->0->3 [3] 1/-1/-1->0->3 [4] 1/-1/-1->0->4 [5] 1/-1/-1->0->4 [6] 1/-1/-1->0->3 [7] 1/-1/-1->0->3
0: nid007116:202900:203363 [0] NCCL INFO P2P Chunksize set to 131072
4: nid007121:116202:116681 [1] NCCL INFO Trees [0] 18/-1/-1->17->16 [1] 18/-1/-1->17->16 [2] -1/-1/-1->17->16 [3] -1/-1/-1->17->16 [4] 18/-1/-1->17->16 [5] 18/-1/-1->17->16 [6] -1/-1/-1->17->16 [7] -1/-1/-1->17->16
4: nid007121:116203:116682 [2] NCCL INFO Trees [0] 19/-1/-1->18->17 [1] 19/-1/-1->18->17 [2] 19/10/26->18->2 [3] 19/10/26->18->2 [4] 19/-1/-1->18->17 [5] 19/-1/-1->18->17 [6] 19/-1/-1->18->22 [7] 19/-1/-1->18->22
4: nid007121:116204:116680 [3] NCCL INFO Trees [0] -1/-1/-1->19->18 [1] -1/-1/-1->19->18 [2] 16/-1/-1->19->18 [3] 16/-1/-1->19->18 [4] -1/-1/-1->19->18 [5] -1/-1/-1->19->18 [6] 16/-1/-1->19->18 [7] 16/-1/-1->19->18
4: nid007121:116202:116681 [1] NCCL INFO P2P Chunksize set to 131072
4: nid007121:116203:116682 [2] NCCL INFO P2P Chunksize set to 131072
5: nid007122:66177:66674 [2] NCCL INFO comm 0xaaaacb2d9e10 rank 22 nRanks 32 nNodes 8 localRanks 4 localRank 2 MNNVL 0
5: nid007122:66176:66675 [1] NCCL INFO comm 0xaaab1e05ac60 rank 21 nRanks 32 nNodes 8 localRanks 4 localRank 1 MNNVL 0
5: nid007122:66175:66676 [0] NCCL INFO comm 0xaaab0041ada0 rank 20 nRanks 32 nNodes 8 localRanks 4 localRank 0 MNNVL 0
7: nid007124:126395:126878 [0] NCCL INFO P2P Chunksize set to 131072
2: nid007119:146136:146627 [0] NCCL INFO comm 0xaaab09168e10 rank 8 nRanks 32 nNodes 8 localRanks 4 localRank 0 MNNVL 0
2: nid007119:146138:146626 [2] NCCL INFO comm 0xaaab072922f0 rank 10 nRanks 32 nNodes 8 localRanks 4 localRank 2 MNNVL 0
3: nid007120:197038:197535 [2] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->10 [3] 15/-1/-1->14->10 [4] 15/-1/-1->14->13 [5] 15/-1/-1->14->13 [6] 15/22/6->14->30 [7] 15/22/6->14->30
3: nid007120:197038:197535 [2] NCCL INFO P2P Chunksize set to 131072
3: nid007120:197037:197534 [1] NCCL INFO comm 0xaaaafddafcb0 rank 13 nRanks 32 nNodes 8 localRanks 4 localRank 1 MNNVL 0
6: nid007123:108292:108857 [3] NCCL INFO P2P Chunksize set to 131072
6: nid007123:108289:108841 [0] NCCL INFO comm 0xaaaaef3a5bf0 rank 24 nRanks 32 nNodes 8 localRanks 4 localRank 0 MNNVL 0
4: nid007121:116204:116680 [3] NCCL INFO P2P Chunksize set to 131072
4: nid007121:116201:116666 [0] NCCL INFO comm 0xaaaacfcf80d0 rank 16 nRanks 32 nNodes 8 localRanks 4 localRank 0 MNNVL 0
5: nid007122:66176:66675 [1] NCCL INFO Trees [0] 22/-1/-1->21->20 [1] 22/-1/-1->21->20 [2] -1/-1/-1->21->20 [3] -1/-1/-1->21->20 [4] 22/-1/-1->21->20 [5] 22/-1/-1->21->20 [6] -1/-1/-1->21->20 [7] -1/-1/-1->21->20
5: nid007122:66175:66676 [0] NCCL INFO Trees [0] 21/-1/-1->20->24 [1] 21/-1/-1->20->24 [2] 21/-1/-1->20->23 [3] 21/-1/-1->20->23 [4] 21/24/16->20->12 [5] 21/24/16->20->12 [6] 21/-1/-1->20->23 [7] 21/-1/-1->20->23
5: nid007122:66176:66675 [1] NCCL INFO P2P Chunksize set to 131072
5: nid007122:66177:66674 [2] NCCL INFO Trees [0] 23/-1/-1->22->21 [1] 23/-1/-1->22->21 [2] 23/-1/-1->22->26 [3] 23/-1/-1->22->26 [4] 23/-1/-1->22->21 [5] 23/-1/-1->22->21 [6] 23/26/18->22->14 [7] 23/26/18->22->14
5: nid007122:66175:66676 [0] NCCL INFO P2P Chunksize set to 131072
2: nid007119:146139:146624 [3] NCCL INFO comm 0xaaab16111160 rank 11 nRanks 32 nNodes 8 localRanks 4 localRank 3 MNNVL 0
3: nid007120:197036:197518 [0] NCCL INFO comm 0xaaaaffa2f9f0 rank 12 nRanks 32 nNodes 8 localRanks 4 localRank 0 MNNVL 0
6: nid007123:108290:108859 [1] NCCL INFO Trees [0] 26/-1/-1->25->24 [1] 26/-1/-1->25->24 [2] -1/-1/-1->25->24 [3] -1/-1/-1->25->24 [4] 26/-1/-1->25->24 [5] 26/-1/-1->25->24 [6] -1/-1/-1->25->24 [7] -1/-1/-1->25->24
4: nid007121:116201:116666 [0] NCCL INFO Trees [0] 17/8/24->16->0 [1] 17/8/24->16->0 [2] 17/-1/-1->16->19 [3] 17/-1/-1->16->19 [4] 17/-1/-1->16->20 [5] 17/-1/-1->16->20 [6] 17/-1/-1->16->19 [7] 17/-1/-1->16->19
4: nid007121:116201:116666 [0] NCCL INFO P2P Chunksize set to 131072
5: nid007122:66177:66674 [2] NCCL INFO P2P Chunksize set to 131072
2: nid007119:146137:146625 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] 10/-1/-1->9->8 [2] -1/-1/-1->9->8 [3] -1/-1/-1->9->8 [4] 10/-1/-1->9->8 [5] 10/-1/-1->9->8 [6] -1/-1/-1->9->8 [7] -1/-1/-1->9->8
2: nid007119:146137:146625 [1] NCCL INFO P2P Chunksize set to 131072
3: nid007120:197037:197534 [1] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12 [2] -1/-1/-1->13->12 [3] -1/-1/-1->13->12 [4] 14/-1/-1->13->12 [5] 14/-1/-1->13->12 [6] -1/-1/-1->13->12 [7] -1/-1/-1->13->12
6: nid007123:108291:108858 [2] NCCL INFO Trees [0] 27/-1/-1->26->25 [1] 27/-1/-1->26->25 [2] 27/22/30->26->18 [3] 27/22/30->26->18 [4] 27/-1/-1->26->25 [5] 27/-1/-1->26->25 [6] 27/-1/-1->26->22 [7] 27/-1/-1->26->22
6: nid007123:108290:108859 [1] NCCL INFO P2P Chunksize set to 131072
6: nid007123:108291:108858 [2] NCCL INFO P2P Chunksize set to 131072
2: nid007119:146139:146624 [3] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] -1/-1/-1->11->10 [2] 8/-1/-1->11->10 [3] 8/-1/-1->11->10 [4] -1/-1/-1->11->10 [5] -1/-1/-1->11->10 [6] 8/-1/-1->11->10 [7] 8/-1/-1->11->10
2: nid007119:146139:146624 [3] NCCL INFO P2P Chunksize set to 131072
3: nid007120:197037:197534 [1] NCCL INFO P2P Chunksize set to 131072
6: nid007123:108289:108841 [0] NCCL INFO Trees [0] 25/20/28->24->16 [1] 25/20/28->24->16 [2] 25/-1/-1->24->27 [3] 25/-1/-1->24->27 [4] 25/-1/-1->24->20 [5] 25/-1/-1->24->20 [6] 25/-1/-1->24->27 [7] 25/-1/-1->24->27
6: nid007123:108289:108841 [0] NCCL INFO P2P Chunksize set to 131072
2: nid007119:146138:146626 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9 [2] 11/6/14->10->18 [3] 11/6/14->10->18 [4] 11/-1/-1->10->9 [5] 11/-1/-1->10->9 [6] 11/-1/-1->10->6 [7] 11/-1/-1->10->6
3: nid007120:197036:197518 [0] NCCL INFO Trees [0] 13/-1/-1->12->8 [1] 13/-1/-1->12->8 [2] 13/-1/-1->12->15 [3] 13/-1/-1->12->15 [4] 13/20/4->12->28 [5] 13/20/4->12->28 [6] 13/-1/-1->12->15 [7] 13/-1/-1->12->15
3: nid007120:197036:197518 [0] NCCL INFO P2P Chunksize set to 131072
2: nid007119:146138:146626 [2] NCCL INFO P2P Chunksize set to 131072
2: nid007119:146136:146627 [0] NCCL INFO Trees [0] 9/4/12->8->16 [1] 9/4/12->8->16 [2] 9/-1/-1->8->11 [3] 9/-1/-1->8->11 [4] 9/-1/-1->8->4 [5] 9/-1/-1->8->4 [6] 9/-1/-1->8->11 [7] 9/-1/-1->8->11
2: nid007119:146136:146627 [0] NCCL INFO P2P Chunksize set to 131072
2: nid007119:146139:146624 [3] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
2: nid007119:146139:146624 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
1: nid007117:210407:210956 [2] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
1: nid007117:210407:210956 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
1: nid007117:210406:210958 [1] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
1: nid007117:210406:210958 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
1: nid007117:210408:210957 [3] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
1: nid007117:210408:210957 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
4: nid007121:116203:116682 [2] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
4: nid007121:116203:116682 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
0: nid007116:202902:203372 [2] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
0: nid007116:202902:203372 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
5: nid007122:66178:66673 [3] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
5: nid007122:66178:66673 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
6: nid007123:108292:108857 [3] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
6: nid007123:108292:108857 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
0: nid007116:202901:203379 [1] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
0: nid007116:202901:203379 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
0: nid007116:202900:203363 [0] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
0: nid007116:202900:203363 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
3: nid007120:197036:197518 [0] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
3: nid007120:197036:197518 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
6: nid007123:108290:108859 [1] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
6: nid007123:108290:108859 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
2: nid007119:146137:146625 [1] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
2: nid007119:146137:146625 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
3: nid007120:197038:197535 [2] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
3: nid007120:197038:197535 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
4: nid007121:116204:116680 [3] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
4: nid007121:116204:116680 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
2: nid007119:146138:146626 [2] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
2: nid007119:146138:146626 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
3: nid007120:197037:197534 [1] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
3: nid007120:197037:197534 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
4: nid007121:116202:116681 [1] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
4: nid007121:116202:116681 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
7: nid007124:126396:126894 [1] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
7: nid007124:126396:126894 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
5: nid007122:66176:66675 [1] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
5: nid007122:66176:66675 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
5: nid007122:66175:66676 [0] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
5: nid007122:66175:66676 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
5: nid007122:66177:66674 [2] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
5: nid007122:66177:66674 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
7: nid007124:126398:126892 [3] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
7: nid007124:126398:126892 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
7: nid007124:126397:126893 [2] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
7: nid007124:126397:126893 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
6: nid007123:108291:108858 [2] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
6: nid007123:108291:108858 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
2: nid007119:146136:146627 [0] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
2: nid007119:146136:146627 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
1: nid007117:210405:210942 [0] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
1: nid007117:210405:210942 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
3: nid007120:197039:197528 [3] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
3: nid007120:197039:197528 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
0: nid007116:202900:203363 [0] NCCL INFO CC Off, Multi-GPU CC Off, workFifoBytes 1048576
0: nid007116:202903:203373 [3] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
0: nid007116:202903:203373 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
4: nid007121:116201:116666 [0] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
4: nid007121:116201:116666 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
6: nid007123:108289:108841 [0] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
6: nid007123:108289:108841 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
7: nid007124:126395:126878 [0] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
7: nid007124:126395:126878 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
2: nid007119:146139:146624 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
2: nid007119:146139:146624 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
2: nid007119:146139:146624 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
2: nid007119:146139:146624 [3] NCCL INFO ncclCommInitRank comm 0xaaab16111160 rank 11 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 commId 0x6d129679625a667c - Init COMPLETE
2: nid007119:146139:146624 [3] NCCL INFO Init timings: rank 11 nranks 32 total 3.56 (kernels 0.40, bootstrap 2.66, allgathers 0.03, topo 0.43, graphs 0.02, connections 0.01, rest 0.00)
2: nid007119:146137:146625 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
2: nid007119:146137:146625 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
2: nid007119:146137:146625 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
2: nid007119:146137:146625 [1] NCCL INFO ncclCommInitRank comm 0xaaab1b0b7890 rank 9 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 commId 0x6d129679625a667c - Init COMPLETE
2: nid007119:146137:146625 [1] NCCL INFO Init timings: rank 9 nranks 32 total 3.44 (kernels 0.29, bootstrap 2.66, allgathers 0.03, topo 0.43, graphs 0.02, connections 0.01, rest 0.00)
2: nid007119:146138:146626 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
3: nid007120:197038:197535 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
3: nid007120:197038:197535 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
3: nid007120:197038:197535 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
2: nid007119:146138:146626 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
2: nid007119:146138:146626 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
3: nid007120:197038:197535 [2] NCCL INFO ncclCommInitRank comm 0xaaab036888d0 rank 14 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 commId 0x6d129679625a667c - Init COMPLETE
3: nid007120:197038:197535 [2] NCCL INFO Init timings: rank 14 nranks 32 total 1.76 (kernels 0.06, bootstrap 1.21, allgathers 0.02, topo 0.44, graphs 0.02, connections 0.01, rest 0.00)
2: nid007119:146136:146627 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
2: nid007119:146138:146626 [2] NCCL INFO ncclCommInitRank comm 0xaaab072922f0 rank 10 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 commId 0x6d129679625a667c - Init COMPLETE
2: nid007119:146136:146627 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
2: nid007119:146138:146626 [2] NCCL INFO Init timings: rank 10 nranks 32 total 3.44 (kernels 0.29, bootstrap 2.66, allgathers 0.03, topo 0.43, graphs 0.02, connections 0.01, rest 0.00)
2: nid007119:146136:146627 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
2: nid007119:146136:146627 [0] NCCL INFO ncclCommInitRank comm 0xaaab09168e10 rank 8 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 commId 0x6d129679625a667c - Init COMPLETE
2: nid007119:146136:146627 [0] NCCL INFO Init timings: rank 8 nranks 32 total 3.44 (kernels 0.29, bootstrap 2.66, allgathers 0.03, topo 0.43, graphs 0.02, connections 0.01, rest 0.00)
3: nid007120:197036:197518 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
3: nid007120:197036:197518 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
3: nid007120:197036:197518 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
3: nid007120:197036:197518 [0] NCCL INFO ncclCommInitRank comm 0xaaaaffa2f9f0 rank 12 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 commId 0x6d129679625a667c - Init COMPLETE
3: nid007120:197039:197528 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
3: nid007120:197036:197518 [0] NCCL INFO Init timings: rank 12 nranks 32 total 3.58 (kernels 0.50, bootstrap 2.59, allgathers 0.02, topo 0.44, graphs 0.02, connections 0.01, rest 0.00)
3: nid007120:197037:197534 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
3: nid007120:197039:197528 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
3: nid007120:197039:197528 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
3: nid007120:197037:197534 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
3: nid007120:197037:197534 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
3: nid007120:197039:197528 [3] NCCL INFO ncclCommInitRank comm 0xaaaaeadc27e0 rank 15 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 commId 0x6d129679625a667c - Init COMPLETE
3: nid007120:197037:197534 [1] NCCL INFO ncclCommInitRank comm 0xaaaafddafcb0 rank 13 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 commId 0x6d129679625a667c - Init COMPLETE
3: nid007120:197037:197534 [1] NCCL INFO Init timings: rank 13 nranks 32 total 1.77 (kernels 0.06, bootstrap 1.22, allgathers 0.02, topo 0.44, graphs 0.02, connections 0.01, rest 0.00)
3: nid007120:197039:197528 [3] NCCL INFO Init timings: rank 15 nranks 32 total 2.45 (kernels 0.06, bootstrap 1.89, allgathers 0.02, topo 0.44, graphs 0.02, connections 0.01, rest 0.00)
5: nid007122:66175:66676 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
5: nid007122:66175:66676 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
5: nid007122:66175:66676 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
5: nid007122:66175:66676 [0] NCCL INFO ncclCommInitRank comm 0xaaab0041ada0 rank 20 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 commId 0x6d129679625a667c - Init COMPLETE
5: nid007122:66175:66676 [0] NCCL INFO Init timings: rank 20 nranks 32 total 3.40 (kernels 0.27, bootstrap 2.64, allgathers 0.04, topo 0.42, graphs 0.02, connections 0.01, rest 0.00)
5: nid007122:66176:66675 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
5: nid007122:66176:66675 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
5: nid007122:66176:66675 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
5: nid007122:66176:66675 [1] NCCL INFO ncclCommInitRank comm 0xaaab1e05ac60 rank 21 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 commId 0x6d129679625a667c - Init COMPLETE
5: nid007122:66176:66675 [1] NCCL INFO Init timings: rank 21 nranks 32 total 3.40 (kernels 0.27, bootstrap 2.64, allgathers 0.04, topo 0.42, graphs 0.02, connections 0.01, rest 0.00)
1: nid007117:210407:210956 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
1: nid007117:210407:210956 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
1: nid007117:210407:210956 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
1: nid007117:210407:210956 [2] NCCL INFO ncclCommInitRank comm 0xaaab2c3c27a0 rank 6 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 commId 0x6d129679625a667c - Init COMPLETE
5: nid007122:66178:66673 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
1: nid007117:210407:210956 [2] NCCL INFO Init timings: rank 6 nranks 32 total 2.57 (kernels 0.06, bootstrap 2.01, allgathers 0.03, topo 0.43, graphs 0.02, connections 0.01, rest 0.00)
5: nid007122:66178:66673 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
5: nid007122:66178:66673 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
5: nid007122:66177:66674 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
5: nid007122:66178:66673 [3] NCCL INFO ncclCommInitRank comm 0xaaab08462130 rank 23 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 commId 0x6d129679625a667c - Init COMPLETE
5: nid007122:66178:66673 [3] NCCL INFO Init timings: rank 23 nranks 32 total 3.57 (kernels 0.43, bootstrap 2.64, allgathers 0.04, topo 0.42, graphs 0.02, connections 0.01, rest 0.00)
5: nid007122:66177:66674 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
5: nid007122:66177:66674 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
5: nid007122:66177:66674 [2] NCCL INFO ncclCommInitRank comm 0xaaaacb2d9e10 rank 22 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 commId 0x6d129679625a667c - Init COMPLETE
5: nid007122:66177:66674 [2] NCCL INFO Init timings: rank 22 nranks 32 total 3.47 (kernels 0.33, bootstrap 2.64, allgathers 0.04, topo 0.42, graphs 0.02, connections 0.01, rest 0.00)
1: nid007117:210406:210958 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
1: nid007117:210406:210958 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
1: nid007117:210406:210958 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
1: nid007117:210406:210958 [1] NCCL INFO ncclCommInitRank comm 0xaaaaf2c70c20 rank 5 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 commId 0x6d129679625a667c - Init COMPLETE
1: nid007117:210408:210957 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
1: nid007117:210406:210958 [1] NCCL INFO Init timings: rank 5 nranks 32 total 2.56 (kernels 0.06, bootstrap 2.01, allgathers 0.03, topo 0.43, graphs 0.02, connections 0.01, rest 0.00)
1: nid007117:210408:210957 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
1: nid007117:210408:210957 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
1: nid007117:210408:210957 [3] NCCL INFO ncclCommInitRank comm 0xaaaaf8ea2d90 rank 7 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 commId 0x6d129679625a667c - Init COMPLETE
1: nid007117:210408:210957 [3] NCCL INFO Init timings: rank 7 nranks 32 total 2.56 (kernels 0.06, bootstrap 2.01, allgathers 0.03, topo 0.43, graphs 0.02, connections 0.01, rest 0.00)
1: nid007117:210405:210942 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
1: nid007117:210405:210942 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
1: nid007117:210405:210942 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
1: nid007117:210405:210942 [0] NCCL INFO ncclCommInitRank comm 0xaaab0fcf58a0 rank 4 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 commId 0x6d129679625a667c - Init COMPLETE
1: nid007117:210405:210942 [0] NCCL INFO Init timings: rank 4 nranks 32 total 3.57 (kernels 0.36, bootstrap 2.73, allgathers 0.03, topo 0.43, graphs 0.02, connections 0.01, rest 0.00)
0: nid007116:202901:203379 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
0: nid007116:202901:203379 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
0: nid007116:202901:203379 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
0: nid007116:202901:203379 [1] NCCL INFO ncclCommInitRank comm 0xaaaaf4f9b440 rank 1 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 commId 0x6d129679625a667c - Init COMPLETE
0: nid007116:202901:203379 [1] NCCL INFO Init timings: rank 1 nranks 32 total 2.53 (kernels 1.21, bootstrap 0.82, allgathers 0.02, topo 0.43, graphs 0.02, connections 0.01, rest 0.00)
0: nid007116:202900:203363 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
0: nid007116:202900:203363 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
0: nid007116:202900:203363 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
6: nid007123:108292:108857 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
0: nid007116:202900:203363 [0] NCCL INFO ncclCommInitRank comm 0xaaab0f53e270 rank 0 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 commId 0x6d129679625a667c - Init COMPLETE
0: nid007116:202900:203363 [0] NCCL INFO Init timings: rank 0 nranks 32 total 3.59 (kernels 0.44, bootstrap 2.65, allgathers 0.02, topo 0.43, graphs 0.02, connections 0.01, rest 0.00)
6: nid007123:108292:108857 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
6: nid007123:108292:108857 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
6: nid007123:108292:108857 [3] NCCL INFO ncclCommInitRank comm 0xaaaaf5620a20 rank 27 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 commId 0x6d129679625a667c - Init COMPLETE
6: nid007123:108292:108857 [3] NCCL INFO Init timings: rank 27 nranks 32 total 2.50 (kernels 0.06, bootstrap 1.95, allgathers 0.00, topo 0.46, graphs 0.02, connections 0.01, rest 0.00)
0: nid007116:202902:203372 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
0: nid007116:202902:203372 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
0: nid007116:202902:203372 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
0: nid007116:202902:203372 [2] NCCL INFO ncclCommInitRank comm 0xaaaaf1faa2e0 rank 2 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 commId 0x6d129679625a667c - Init COMPLETE
0: nid007116:202902:203372 [2] NCCL INFO Init timings: rank 2 nranks 32 total 3.17 (kernels 0.64, bootstrap 2.03, allgathers 0.02, topo 0.43, graphs 0.02, connections 0.01, rest 0.00)
0: nid007116:202903:203373 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
6: nid007123:108290:108859 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
0: nid007116:202903:203373 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
0: nid007116:202903:203373 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
6: nid007123:108290:108859 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
6: nid007123:108290:108859 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
6: nid007123:108290:108859 [1] NCCL INFO ncclCommInitRank comm 0xaaab083c2070 rank 25 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 commId 0x6d129679625a667c - Init COMPLETE
6: nid007123:108290:108859 [1] NCCL INFO Init timings: rank 25 nranks 32 total 2.48 (kernels 0.06, bootstrap 1.93, allgathers 0.00, topo 0.46, graphs 0.02, connections 0.01, rest 0.00)
0: nid007116:202903:203373 [3] NCCL INFO ncclCommInitRank comm 0xaaab18df7db0 rank 3 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 commId 0x6d129679625a667c - Init COMPLETE
0: nid007116:202903:203373 [3] NCCL INFO Init timings: rank 3 nranks 32 total 3.17 (kernels 0.64, bootstrap 2.03, allgathers 0.02, topo 0.43, graphs 0.02, connections 0.01, rest 0.00)
4: nid007121:116204:116680 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
4: nid007121:116204:116680 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
4: nid007121:116204:116680 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
6: nid007123:108289:108841 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
4: nid007121:116204:116680 [3] NCCL INFO ncclCommInitRank comm 0xaaab22c089c0 rank 19 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 commId 0x6d129679625a667c - Init COMPLETE
4: nid007121:116204:116680 [3] NCCL INFO Init timings: rank 19 nranks 32 total 2.89 (kernels 0.06, bootstrap 2.33, allgathers 0.01, topo 0.45, graphs 0.02, connections 0.01, rest 0.00)
6: nid007123:108289:108841 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
6: nid007123:108289:108841 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
6: nid007123:108289:108841 [0] NCCL INFO ncclCommInitRank comm 0xaaaaef3a5bf0 rank 24 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 commId 0x6d129679625a667c - Init COMPLETE
6: nid007123:108289:108841 [0] NCCL INFO Init timings: rank 24 nranks 32 total 3.58 (kernels 0.44, bootstrap 2.64, allgathers 0.00, topo 0.46, graphs 0.02, connections 0.01, rest 0.00)
6: nid007123:108291:108858 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
6: nid007123:108291:108858 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
6: nid007123:108291:108858 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
6: nid007123:108291:108858 [2] NCCL INFO ncclCommInitRank comm 0xaaab23840990 rank 26 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 commId 0x6d129679625a667c - Init COMPLETE
6: nid007123:108291:108858 [2] NCCL INFO Init timings: rank 26 nranks 32 total 2.48 (kernels 0.29, bootstrap 1.70, allgathers 0.00, topo 0.46, graphs 0.02, connections 0.01, rest 0.00)
4: nid007121:116203:116682 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
4: nid007121:116203:116682 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
4: nid007121:116203:116682 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
4: nid007121:116203:116682 [2] NCCL INFO ncclCommInitRank comm 0xaaab08232380 rank 18 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 commId 0x6d129679625a667c - Init COMPLETE
4: nid007121:116201:116666 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
4: nid007121:116203:116682 [2] NCCL INFO Init timings: rank 18 nranks 32 total 2.89 (kernels 0.06, bootstrap 2.33, allgathers 0.01, topo 0.45, graphs 0.02, connections 0.01, rest 0.00)
4: nid007121:116201:116666 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
4: nid007121:116201:116666 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
4: nid007121:116201:116666 [0] NCCL INFO ncclCommInitRank comm 0xaaaacfcf80d0 rank 16 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 commId 0x6d129679625a667c - Init COMPLETE
4: nid007121:116201:116666 [0] NCCL INFO Init timings: rank 16 nranks 32 total 3.58 (kernels 0.06, bootstrap 3.02, allgathers 0.01, topo 0.45, graphs 0.02, connections 0.01, rest 0.00)
4: nid007121:116202:116681 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
4: nid007121:116202:116681 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
4: nid007121:116202:116681 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
4: nid007121:116202:116681 [1] NCCL INFO ncclCommInitRank comm 0xaaab2fe09390 rank 17 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 commId 0x6d129679625a667c - Init COMPLETE
4: nid007121:116202:116681 [1] NCCL INFO Init timings: rank 17 nranks 32 total 2.89 (kernels 0.06, bootstrap 2.33, allgathers 0.01, topo 0.45, graphs 0.02, connections 0.01, rest 0.00)
7: nid007124:126396:126894 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
7: nid007124:126396:126894 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
7: nid007124:126396:126894 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
7: nid007124:126396:126894 [1] NCCL INFO ncclCommInitRank comm 0xaaab39820720 rank 29 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 commId 0x6d129679625a667c - Init COMPLETE
7: nid007124:126396:126894 [1] NCCL INFO Init timings: rank 29 nranks 32 total 2.52 (kernels 0.06, bootstrap 1.96, allgathers 0.04, topo 0.42, graphs 0.02, connections 0.01, rest 0.00)
7: nid007124:126395:126878 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
7: nid007124:126395:126878 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
7: nid007124:126397:126893 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
7: nid007124:126395:126878 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
7: nid007124:126395:126878 [0] NCCL INFO ncclCommInitRank comm 0xaaaaef48c330 rank 28 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 commId 0x6d129679625a667c - Init COMPLETE
7: nid007124:126397:126893 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
7: nid007124:126397:126893 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
7: nid007124:126395:126878 [0] NCCL INFO Init timings: rank 28 nranks 32 total 3.58 (kernels 0.42, bootstrap 2.66, allgathers 0.04, topo 0.42, graphs 0.02, connections 0.01, rest 0.00)
7: nid007124:126397:126893 [2] NCCL INFO ncclCommInitRank comm 0xaaab10cea560 rank 30 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 commId 0x6d129679625a667c - Init COMPLETE
7: nid007124:126398:126892 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net-ofi.so
7: nid007124:126397:126893 [2] NCCL INFO Init timings: rank 30 nranks 32 total 2.52 (kernels 0.06, bootstrap 1.96, allgathers 0.04, topo 0.42, graphs 0.02, connections 0.01, rest 0.00)
7: nid007124:126398:126892 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v3 symbol.
7: nid007124:126398:126892 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2 symbol, using internal tuner instead.
7: nid007124:126398:126892 [3] NCCL INFO ncclCommInitRank comm 0xaaab12762370 rank 31 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 commId 0x6d129679625a667c - Init COMPLETE
7: nid007124:126398:126892 [3] NCCL INFO Init timings: rank 31 nranks 32 total 2.52 (kernels 0.06, bootstrap 1.97, allgathers 0.04, topo 0.42, graphs 0.02, connections 0.01, rest 0.00)
0: nid007116:202900:203413 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
0: nid007116:202901:203412 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM
4: nid007121:116201:116717 [0] NCCL INFO Channel 00/0 : 16[0] -> 17[1] via P2P/CUMEM
0: nid007116:202900:203413 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM
0: nid007116:202901:203412 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM
5: nid007122:66175:66709 [0] NCCL INFO Channel 01/0 : 20[0] -> 21[1] via P2P/CUMEM
4: nid007121:116201:116717 [0] NCCL INFO Channel 04/0 : 16[0] -> 17[1] via P2P/CUMEM
0: nid007116:202900:203413 [0] NCCL INFO Channel 02/0 : 0[0] -> 3[3] via P2P/CUMEM
0: nid007116:202902:203415 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM
0: nid007116:202900:203413 [0] NCCL INFO Channel 03/0 : 0[0] -> 3[3] via P2P/CUMEM
4: nid007121:116203:116715 [2] NCCL INFO Channel 00/0 : 18[2] -> 19[3] via P2P/CUMEM
2: nid007119:146136:146664 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[1] via P2P/CUMEM
4: nid007121:116202:116716 [1] NCCL INFO Channel 00/0 : 17[1] -> 18[2] via P2P/CUMEM
5: nid007122:66175:66709 [0] NCCL INFO Channel 05/0 : 20[0] -> 21[1] via P2P/CUMEM
2: nid007119:146137:146663 [1] NCCL INFO Channel 00/0 : 9[1] -> 10[2] via P2P/CUMEM
0: nid007116:202902:203415 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/CUMEM
0: nid007116:202900:203413 [0] NCCL INFO Channel 06/0 : 0[0] -> 3[3] via P2P/CUMEM
2: nid007119:146137:146663 [1] NCCL INFO Channel 04/0 : 9[1] -> 10[2] via P2P/CUMEM
4: nid007121:116203:116715 [2] NCCL INFO Channel 04/0 : 18[2] -> 19[3] via P2P/CUMEM
4: nid007121:116202:116716 [1] NCCL INFO Channel 04/0 : 17[1] -> 18[2] via P2P/CUMEM
0: nid007116:202900:203413 [0] NCCL INFO Channel 07/0 : 0[0] -> 3[3] via P2P/CUMEM
2: nid007119:146136:146664 [0] NCCL INFO Channel 04/0 : 8[0] -> 9[1] via P2P/CUMEM
4: nid007121:116201:116717 [0] NCCL INFO Channel 02/0 : 16[0] -> 19[3] via P2P/CUMEM
3: nid007120:197036:197547 [0] NCCL INFO Channel 01/0 : 12[0] -> 13[1] via P2P/CUMEM
2: nid007119:146138:146665 [2] NCCL INFO Channel 00/0 : 10[2] -> 11[3] via P2P/CUMEM
4: nid007121:116201:116717 [0] NCCL INFO Channel 03/0 : 16[0] -> 19[3] via P2P/CUMEM
2: nid007119:146138:146665 [2] NCCL INFO Channel 04/0 : 10[2] -> 11[3] via P2P/CUMEM
2: nid007119:146136:146664 [0] NCCL INFO Channel 02/0 : 8[0] -> 11[3] via P2P/CUMEM
4: nid007121:116201:116717 [0] NCCL INFO Channel 06/0 : 16[0] -> 19[3] via P2P/CUMEM
2: nid007119:146136:146664 [0] NCCL INFO Channel 03/0 : 8[0] -> 11[3] via P2P/CUMEM
4: nid007121:116201:116717 [0] NCCL INFO Channel 07/0 : 16[0] -> 19[3] via P2P/CUMEM
3: nid007120:197036:197547 [0] NCCL INFO Channel 05/0 : 12[0] -> 13[1] via P2P/CUMEM
2: nid007119:146136:146664 [0] NCCL INFO Channel 06/0 : 8[0] -> 11[3] via P2P/CUMEM
5: nid007122:66177:66710 [2] NCCL INFO Channel 01/0 : 22[2] -> 23[3] via P2P/CUMEM
5: nid007122:66176:66712 [1] NCCL INFO Channel 01/0 : 21[1] -> 22[2] via P2P/CUMEM
2: nid007119:146136:146664 [0] NCCL INFO Channel 07/0 : 8[0] -> 11[3] via P2P/CUMEM
5: nid007122:66177:66710 [2] NCCL INFO Channel 05/0 : 22[2] -> 23[3] via P2P/CUMEM
5: nid007122:66176:66712 [1] NCCL INFO Channel 05/0 : 21[1] -> 22[2] via P2P/CUMEM
7: nid007124:126395:126909 [0] NCCL INFO Channel 01/0 : 28[0] -> 29[1] via P2P/CUMEM
3: nid007120:197037:197549 [1] NCCL INFO Channel 01/0 : 13[1] -> 14[2] via P2P/CUMEM
3: nid007120:197038:197550 [2] NCCL INFO Channel 01/0 : 14[2] -> 15[3] via P2P/CUMEM
1: nid007117:210405:210971 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[1] via P2P/CUMEM
5: nid007122:66175:66709 [0] NCCL INFO Channel 02/0 : 20[0] -> 23[3] via P2P/CUMEM
3: nid007120:197037:197549 [1] NCCL INFO Channel 05/0 : 13[1] -> 14[2] via P2P/CUMEM
3: nid007120:197038:197550 [2] NCCL INFO Channel 05/0 : 14[2] -> 15[3] via P2P/CUMEM
5: nid007122:66175:66709 [0] NCCL INFO Channel 03/0 : 20[0] -> 23[3] via P2P/CUMEM
3: nid007120:197036:197547 [0] NCCL INFO Channel 02/0 : 12[0] -> 15[3] via P2P/CUMEM
5: nid007122:66175:66709 [0] NCCL INFO Channel 06/0 : 20[0] -> 23[3] via P2P/CUMEM
1: nid007117:210405:210971 [0] NCCL INFO Channel 05/0 : 4[0] -> 5[1] via P2P/CUMEM
7: nid007124:126395:126909 [0] NCCL INFO Channel 05/0 : 28[0] -> 29[1] via P2P/CUMEM
3: nid007120:197036:197547 [0] NCCL INFO Channel 03/0 : 12[0] -> 15[3] via P2P/CUMEM
1: nid007117:210407:210970 [2] NCCL INFO Channel 01/0 : 6[2] -> 7[3] via P2P/CUMEM
5: nid007122:66175:66709 [0] NCCL INFO Channel 07/0 : 20[0] -> 23[3] via P2P/CUMEM
3: nid007120:197036:197547 [0] NCCL INFO Channel 06/0 : 12[0] -> 15[3] via P2P/CUMEM
7: nid007124:126396:126906 [1] NCCL INFO Channel 01/0 : 29[1] -> 30[2] via P2P/CUMEM
1: nid007117:210407:210970 [2] NCCL INFO Channel 05/0 : 6[2] -> 7[3] via P2P/CUMEM
6: nid007123:108289:108900 [0] NCCL INFO Channel 00/0 : 24[0] -> 25[1] via P2P/CUMEM
7: nid007124:126397:126907 [2] NCCL INFO Channel 01/0 : 30[2] -> 31[3] via P2P/CUMEM
3: nid007120:197036:197547 [0] NCCL INFO Channel 07/0 : 12[0] -> 15[3] via P2P/CUMEM
7: nid007124:126396:126906 [1] NCCL INFO Channel 05/0 : 29[1] -> 30[2] via P2P/CUMEM
6: nid007123:108290:108898 [1] NCCL INFO Channel 00/0 : 25[1] -> 26[2] via P2P/CUMEM
7: nid007124:126397:126907 [2] NCCL INFO Channel 05/0 : 30[2] -> 31[3] via P2P/CUMEM
6: nid007123:108291:108899 [2] NCCL INFO Channel 00/0 : 26[2] -> 27[3] via P2P/CUMEM
6: nid007123:108289:108900 [0] NCCL INFO Channel 04/0 : 24[0] -> 25[1] via P2P/CUMEM
7: nid007124:126395:126909 [0] NCCL INFO Channel 02/0 : 28[0] -> 31[3] via P2P/CUMEM
6: nid007123:108290:108898 [1] NCCL INFO Channel 04/0 : 25[1] -> 26[2] via P2P/CUMEM
7: nid007124:126395:126909 [0] NCCL INFO Channel 03/0 : 28[0] -> 31[3] via P2P/CUMEM
1: nid007117:210406:210973 [1] NCCL INFO Channel 01/0 : 5[1] -> 6[2] via P2P/CUMEM
6: nid007123:108291:108899 [2] NCCL INFO Channel 04/0 : 26[2] -> 27[3] via P2P/CUMEM
1: nid007117:210406:210973 [1] NCCL INFO Channel 05/0 : 5[1] -> 6[2] via P2P/CUMEM
7: nid007124:126395:126909 [0] NCCL INFO Channel 06/0 : 28[0] -> 31[3] via P2P/CUMEM
6: nid007123:108289:108900 [0] NCCL INFO Channel 02/0 : 24[0] -> 27[3] via P2P/CUMEM
7: nid007124:126395:126909 [0] NCCL INFO Channel 07/0 : 28[0] -> 31[3] via P2P/CUMEM
1: nid007117:210405:210971 [0] NCCL INFO Channel 02/0 : 4[0] -> 7[3] via P2P/CUMEM
6: nid007123:108289:108900 [0] NCCL INFO Channel 03/0 : 24[0] -> 27[3] via P2P/CUMEM
0: nid007116:202901:203412 [1] NCCL INFO Channel 03/0 : 29[1] -> 1[1] [receive] via NET/AWS Libfabric/1
0: nid007116:202901:203412 [1] NCCL INFO Channel 07/0 : 29[1] -> 1[1] [receive] via NET/AWS Libfabric/1
1: nid007117:210405:210971 [0] NCCL INFO Channel 03/0 : 4[0] -> 7[3] via P2P/CUMEM
6: nid007123:108289:108900 [0] NCCL INFO Channel 06/0 : 24[0] -> 27[3] via P2P/CUMEM
0: nid007116:202901:203412 [1] NCCL INFO Channel 02/0 : 1[1] -> 5[1] [send] via NET/AWS Libfabric/1
0: nid007116:202901:203412 [1] NCCL INFO Channel 06/0 : 1[1] -> 5[1] [send] via NET/AWS Libfabric/1
0: nid007116:202902:203415 [2] NCCL INFO Channel 02/0 : 30[2] -> 2[2] [receive] via NET/AWS Libfabric/2
0: nid007116:202902:203415 [2] NCCL INFO Channel 06/0 : 30[2] -> 2[2] [receive] via NET/AWS Libfabric/2
1: nid007117:210405:210971 [0] NCCL INFO Channel 06/0 : 4[0] -> 7[3] via P2P/CUMEM
0: nid007116:202902:203415 [2] NCCL INFO Channel 03/0 : 2[2] -> 6[2] [send] via NET/AWS Libfabric/2
4: nid007121:116203:116715 [2] NCCL INFO Channel 02/0 : 14[2] -> 18[2] [receive] via NET/AWS Libfabric/2
6: nid007123:108289:108900 [0] NCCL INFO Channel 07/0 : 24[0] -> 27[3] via P2P/CUMEM
0: nid007116:202902:203415 [2] NCCL INFO Channel 07/0 : 2[2] -> 6[2] [send] via NET/AWS Libfabric/2
4: nid007121:116203:116715 [2] NCCL INFO Channel 06/0 : 14[2] -> 18[2] [receive] via NET/AWS Libfabric/2
1: nid007117:210405:210971 [0] NCCL INFO Channel 07/0 : 4[0] -> 7[3] via P2P/CUMEM
4: nid007121:116203:116715 [2] NCCL INFO Channel 03/0 : 18[2] -> 22[2] [send] via NET/AWS Libfabric/2
4: nid007121:116202:116716 [1] NCCL INFO Channel 03/0 : 13[1] -> 17[1] [receive] via NET/AWS Libfabric/1
4: nid007121:116203:116715 [2] NCCL INFO Channel 07/0 : 18[2] -> 22[2] [send] via NET/AWS Libfabric/2
4: nid007121:116202:116716 [1] NCCL INFO Channel 07/0 : 13[1] -> 17[1] [receive] via NET/AWS Libfabric/1
2: nid007119:146137:146663 [1] NCCL INFO Channel 03/0 : 5[1] -> 9[1] [receive] via NET/AWS Libfabric/1
4: nid007121:116202:116716 [1] NCCL INFO Channel 02/0 : 17[1] -> 21[1] [send] via NET/AWS Libfabric/1
4: nid007121:116202:116716 [1] NCCL INFO Channel 06/0 : 17[1] -> 21[1] [send] via NET/AWS Libfabric/1
2: nid007119:146137:146663 [1] NCCL INFO Channel 07/0 : 5[1] -> 9[1] [receive] via NET/AWS Libfabric/1
2: nid007119:146137:146663 [1] NCCL INFO Channel 02/0 : 9[1] -> 13[1] [send] via NET/AWS Libfabric/1
2: nid007119:146137:146663 [1] NCCL INFO Channel 06/0 : 9[1] -> 13[1] [send] via NET/AWS Libfabric/1
2: nid007119:146138:146665 [2] NCCL INFO Channel 02/0 : 6[2] -> 10[2] [receive] via NET/AWS Libfabric/2
0: nid007116:202903:203414 [3] NCCL INFO Channel 01/0 : 31[3] -> 3[3] [receive] via NET/AWS Libfabric/3
2: nid007119:146138:146665 [2] NCCL INFO Channel 06/0 : 6[2] -> 10[2] [receive] via NET/AWS Libfabric/2
0: nid007116:202900:203413 [0] NCCL INFO Channel 00/0 : 28[0] -> 0[0] [receive] via NET/AWS Libfabric/0
0: nid007116:202903:203414 [3] NCCL INFO Channel 05/0 : 31[3] -> 3[3] [receive] via NET/AWS Libfabric/3
2: nid007119:146138:146665 [2] NCCL INFO Channel 03/0 : 10[2] -> 14[2] [send] via NET/AWS Libfabric/2
0: nid007116:202900:203413 [0] NCCL INFO Channel 04/0 : 28[0] -> 0[0] [receive] via NET/AWS Libfabric/0
0: nid007116:202903:203414 [3] NCCL INFO Channel 00/0 : 3[3] -> 7[3] [send] via NET/AWS Libfabric/3
4: nid007121:116204:116714 [3] NCCL INFO Channel 01/0 : 15[3] -> 19[3] [receive] via NET/AWS Libfabric/3
2: nid007119:146138:146665 [2] NCCL INFO Channel 07/0 : 10[2] -> 14[2] [send] via NET/AWS Libfabric/2
0: nid007116:202900:203413 [0] NCCL INFO Channel 01/0 : 0[0] -> 4[0] [send] via NET/AWS Libfabric/0
4: nid007121:116204:116714 [3] NCCL INFO Channel 05/0 : 15[3] -> 19[3] [receive] via NET/AWS Libfabric/3
0: nid007116:202903:203414 [3] NCCL INFO Channel 04/0 : 3[3] -> 7[3] [send] via NET/AWS Libfabric/3
0: nid007116:202900:203413 [0] NCCL INFO Channel 05/0 : 0[0] -> 4[0] [send] via NET/AWS Libfabric/0
4: nid007121:116204:116714 [3] NCCL INFO Channel 00/0 : 19[3] -> 23[3] [send] via NET/AWS Libfabric/3
4: nid007121:116204:116714 [3] NCCL INFO Channel 04/0 : 19[3] -> 23[3] [send] via NET/AWS Libfabric/3
5: nid007122:66176:66712 [1] NCCL INFO Channel 02/0 : 17[1] -> 21[1] [receive] via NET/AWS Libfabric/1
5: nid007122:66176:66712 [1] NCCL INFO Channel 06/0 : 17[1] -> 21[1] [receive] via NET/AWS Libfabric/1
5: nid007122:66176:66712 [1] NCCL INFO Channel 03/0 : 21[1] -> 25[1] [send] via NET/AWS Libfabric/1
2: nid007119:146136:146664 [0] NCCL INFO Channel 00/0 : 4[0] -> 8[0] [receive] via NET/AWS Libfabric/0
2: nid007119:146139:146662 [3] NCCL INFO Channel 01/0 : 7[3] -> 11[3] [receive] via NET/AWS Libfabric/3
5: nid007122:66176:66712 [1] NCCL INFO Channel 07/0 : 21[1] -> 25[1] [send] via NET/AWS Libfabric/1
2: nid007119:146139:146662 [3] NCCL INFO Channel 05/0 : 7[3] -> 11[3] [receive] via NET/AWS Libfabric/3
3: nid007120:197038:197550 [2] NCCL INFO Channel 03/0 : 10[2] -> 14[2] [receive] via NET/AWS Libfabric/2
2: nid007119:146136:146664 [0] NCCL INFO Channel 04/0 : 4[0] -> 8[0] [receive] via NET/AWS Libfabric/0
2: nid007119:146139:146662 [3] NCCL INFO Channel 00/0 : 11[3] -> 15[3] [send] via NET/AWS Libfabric/3
3: nid007120:197038:197550 [2] NCCL INFO Channel 07/0 : 10[2] -> 14[2] [receive] via NET/AWS Libfabric/2
2: nid007119:146139:146662 [3] NCCL INFO Channel 04/0 : 11[3] -> 15[3] [send] via NET/AWS Libfabric/3
3: nid007120:197038:197550 [2] NCCL INFO Channel 02/0 : 14[2] -> 18[2] [send] via NET/AWS Libfabric/2
5: nid007122:66177:66710 [2] NCCL INFO Channel 03/0 : 18[2] -> 22[2] [receive] via NET/AWS Libfabric/2
3: nid007120:197037:197549 [1] NCCL INFO Channel 02/0 : 9[1] -> 13[1] [receive] via NET/AWS Libfabric/1
3: nid007120:197038:197550 [2] NCCL INFO Channel 06/0 : 14[2] -> 18[2] [send] via NET/AWS Libfabric/2
5: nid007122:66177:66710 [2] NCCL INFO Channel 07/0 : 18[2] -> 22[2] [receive] via NET/AWS Libfabric/2
3: nid007120:197037:197549 [1] NCCL INFO Channel 06/0 : 9[1] -> 13[1] [receive] via NET/AWS Libfabric/1
5: nid007122:66177:66710 [2] NCCL INFO Channel 02/0 : 22[2] -> 26[2] [send] via NET/AWS Libfabric/2
3: nid007120:197037:197549 [1] NCCL INFO Channel 03/0 : 13[1] -> 17[1] [send] via NET/AWS Libfabric/1
2: nid007119:146136:146664 [0] NCCL INFO Channel 01/0 : 8[0] -> 12[0] [send] via NET/AWS Libfabric/0
5: nid007122:66177:66710 [2] NCCL INFO Channel 06/0 : 22[2] -> 26[2] [send] via NET/AWS Libfabric/2
3: nid007120:197037:197549 [1] NCCL INFO Channel 07/0 : 13[1] -> 17[1] [send] via NET/AWS Libfabric/1
2: nid007119:146136:146664 [0] NCCL INFO Channel 05/0 : 8[0] -> 12[0] [send] via NET/AWS Libfabric/0
3: nid007120:197038:197550 [2] NCCL INFO Channel 03/0 : 14[2] -> 12[0] via P2P/CUMEM
4: nid007121:116203:116715 [2] NCCL INFO Channel 02/0 : 18[2] -> 16[0] via P2P/CUMEM
3: nid007120:197038:197550 [2] NCCL INFO Channel 07/0 : 14[2] -> 12[0] via P2P/CUMEM
5: nid007122:66178:66711 [3] NCCL INFO Channel 00/0 : 19[3] -> 23[3] [receive] via NET/AWS Libfabric/3
5: nid007122:66178:66711 [3] NCCL INFO Channel 04/0 : 19[3] -> 23[3] [receive] via NET/AWS Libfabric/3
5: nid007122:66178:66711 [3] NCCL INFO Channel 01/0 : 23[3] -> 27[3] [send] via NET/AWS Libfabric/3
3: nid007120:197036:197547 [0] NCCL INFO Channel 01/0 : 8[0] -> 12[0] [receive] via NET/AWS Libfabric/0
5: nid007122:66175:66709 [0] NCCL INFO Channel 01/0 : 16[0] -> 20[0] [receive] via NET/AWS Libfabric/0
4: nid007121:116201:116717 [0] NCCL INFO Channel 00/0 : 12[0] -> 16[0] [receive] via NET/AWS Libfabric/0
5: nid007122:66178:66711 [3] NCCL INFO Channel 05/0 : 23[3] -> 27[3] [send] via NET/AWS Libfabric/3
3: nid007120:197036:197547 [0] NCCL INFO Channel 05/0 : 8[0] -> 12[0] [receive] via NET/AWS Libfabric/0
4: nid007121:116203:116715 [2] NCCL INFO Channel 06/0 : 18[2] -> 16[0] via P2P/CUMEM
5: nid007122:66175:66709 [0] NCCL INFO Channel 05/0 : 16[0] -> 20[0] [receive] via NET/AWS Libfabric/0
3: nid007120:197036:197547 [0] NCCL INFO Channel 00/0 : 12[0] -> 16[0] [send] via NET/AWS Libfabric/0
3: nid007120:197036:197547 [0] NCCL INFO Channel 04/0 : 12[0] -> 16[0] [send] via NET/AWS Libfabric/0
6: nid007123:108290:108898 [1] NCCL INFO Channel 03/0 : 21[1] -> 25[1] [receive] via NET/AWS Libfabric/1
4: nid007121:116201:116717 [0] NCCL INFO Channel 04/0 : 12[0] -> 16[0] [receive] via NET/AWS Libfabric/0
7: nid007124:126396:126906 [1] NCCL INFO Channel 02/0 : 25[1] -> 29[1] [receive] via NET/AWS Libfabric/1
6: nid007123:108290:108898 [1] NCCL INFO Channel 07/0 : 21[1] -> 25[1] [receive] via NET/AWS Libfabric/1
5: nid007122:66175:66709 [0] NCCL INFO Channel 00/0 : 20[0] -> 24[0] [send] via NET/AWS Libfabric/0
6: nid007123:108290:108898 [1] NCCL INFO Channel 02/0 : 25[1] -> 29[1] [send] via NET/AWS Libfabric/1
7: nid007124:126396:126906 [1] NCCL INFO Channel 06/0 : 25[1] -> 29[1] [receive] via NET/AWS Libfabric/1
5: nid007122:66175:66709 [0] NCCL INFO Channel 04/0 : 20[0] -> 24[0] [send] via NET/AWS Libfabric/0
4: nid007121:116201:116717 [0] NCCL INFO Channel 01/0 : 16[0] -> 20[0] [send] via NET/AWS Libfabric/0
6: nid007123:108290:108898 [1] NCCL INFO Channel 06/0 : 25[1] -> 29[1] [send] via NET/AWS Libfabric/1
7: nid007124:126396:126906 [1] NCCL INFO Channel 03/0 : 29[1] -> 1[1] [send] via NET/AWS Libfabric/1
7: nid007124:126396:126906 [1] NCCL INFO Channel 07/0 : 29[1] -> 1[1] [send] via NET/AWS Libfabric/1
4: nid007121:116201:116717 [0] NCCL INFO Channel 05/0 : 16[0] -> 20[0] [send] via NET/AWS Libfabric/0
7: nid007124:126397:126907 [2] NCCL INFO Channel 03/0 : 26[2] -> 30[2] [receive] via NET/AWS Libfabric/2
7: nid007124:126397:126907 [2] NCCL INFO Channel 07/0 : 26[2] -> 30[2] [receive] via NET/AWS Libfabric/2
7: nid007124:126397:126907 [2] NCCL INFO Channel 02/0 : 30[2] -> 2[2] [send] via NET/AWS Libfabric/2
3: nid007120:197039:197548 [3] NCCL INFO Channel 00/0 : 11[3] -> 15[3] [receive] via NET/AWS Libfabric/3
3: nid007120:197039:197548 [3] NCCL INFO Channel 04/0 : 11[3] -> 15[3] [receive] via NET/AWS Libfabric/3
7: nid007124:126397:126907 [2] NCCL INFO Channel 06/0 : 30[2] -> 2[2] [send] via NET/AWS Libfabric/2
3: nid007120:197039:197548 [3] NCCL INFO Channel 01/0 : 15[3] -> 19[3] [send] via NET/AWS Libfabric/3
3: nid007120:197039:197548 [3] NCCL INFO Channel 05/0 : 15[3] -> 19[3] [send] via NET/AWS Libfabric/3
1: nid007117:210407:210970 [2] NCCL INFO Channel 03/0 : 2[2] -> 6[2] [receive] via NET/AWS Libfabric/2
4: nid007121:116204:116714 [3] NCCL INFO Channel 02/0 : 19[3] -> 17[1] via P2P/CUMEM
3: nid007120:197039:197548 [3] NCCL INFO Channel 03/0 : 15[3] -> 13[1] via P2P/CUMEM
6: nid007123:108291:108899 [2] NCCL INFO Channel 02/0 : 22[2] -> 26[2] [receive] via NET/AWS Libfabric/2
1: nid007117:210407:210970 [2] NCCL INFO Channel 07/0 : 2[2] -> 6[2] [receive] via NET/AWS Libfabric/2
1: nid007117:210407:210970 [2] NCCL INFO Channel 02/0 : 6[2] -> 10[2] [send] via NET/AWS Libfabric/2
6: nid007123:108291:108899 [2] NCCL INFO Channel 06/0 : 22[2] -> 26[2] [receive] via NET/AWS Libfabric/2
1: nid007117:210407:210970 [2] NCCL INFO Channel 06/0 : 6[2] -> 10[2] [send] via NET/AWS Libfabric/2
6: nid007123:108291:108899 [2] NCCL INFO Channel 03/0 : 26[2] -> 30[2] [send] via NET/AWS Libfabric/2
6: nid007123:108291:108899 [2] NCCL INFO Channel 07/0 : 26[2] -> 30[2] [send] via NET/AWS Libfabric/2
0: nid007116:202902:203415 [2] NCCL INFO Channel 02/0 : 2[2] -> 0[0] via P2P/CUMEM
1: nid007117:210406:210973 [1] NCCL INFO Channel 02/0 : 1[1] -> 5[1] [receive] via NET/AWS Libfabric/1
2: nid007119:146138:146665 [2] NCCL INFO Channel 02/0 : 10[2] -> 8[0] via P2P/CUMEM
5: nid007122:66177:66710 [2] NCCL INFO Channel 03/0 : 22[2] -> 20[0] via P2P/CUMEM
1: nid007117:210406:210973 [1] NCCL INFO Channel 06/0 : 1[1] -> 5[1] [receive] via NET/AWS Libfabric/1
1: nid007117:210407:210970 [2] NCCL INFO Channel 03/0 : 6[2] -> 4[0] via P2P/CUMEM
3: nid007120:197039:197548 [3] NCCL INFO Channel 07/0 : 15[3] -> 13[1] via P2P/CUMEM
7: nid007124:126397:126907 [2] NCCL INFO Channel 03/0 : 30[2] -> 28[0] via P2P/CUMEM
6: nid007123:108291:108899 [2] NCCL INFO Channel 02/0 : 26[2] -> 24[0] via P2P/CUMEM
1: nid007117:210406:210973 [1] NCCL INFO Channel 03/0 : 5[1] -> 9[1] [send] via NET/AWS Libfabric/1
4: nid007121:116204:116714 [3] NCCL INFO Channel 06/0 : 19[3] -> 17[1] via P2P/CUMEM
1: nid007117:210406:210973 [1] NCCL INFO Channel 07/0 : 5[1] -> 9[1] [send] via NET/AWS Libfabric/1
0: nid007116:202902:203415 [2] NCCL INFO Channel 06/0 : 2[2] -> 0[0] via P2P/CUMEM
5: nid007122:66177:66710 [2] NCCL INFO Channel 07/0 : 22[2] -> 20[0] via P2P/CUMEM
1: nid007117:210407:210970 [2] NCCL INFO Channel 07/0 : 6[2] -> 4[0] via P2P/CUMEM
2: nid007119:146138:146665 [2] NCCL INFO Channel 06/0 : 10[2] -> 8[0] via P2P/CUMEM
1: nid007117:210408:210972 [3] NCCL INFO Channel 00/0 : 3[3] -> 7[3] [receive] via NET/AWS Libfabric/3
7: nid007124:126397:126907 [2] NCCL INFO Channel 07/0 : 30[2] -> 28[0] via P2P/CUMEM
6: nid007123:108291:108899 [2] NCCL INFO Channel 06/0 : 26[2] -> 24[0] via P2P/CUMEM
1: nid007117:210408:210972 [3] NCCL INFO Channel 04/0 : 3[3] -> 7[3] [receive] via NET/AWS Libfabric/3
7: nid007124:126398:126908 [3] NCCL INFO Channel 00/0 : 27[3] -> 31[3] [receive] via NET/AWS Libfabric/3
1: nid007117:210408:210972 [3] NCCL INFO Channel 01/0 : 7[3] -> 11[3] [send] via NET/AWS Libfabric/3
7: nid007124:126398:126908 [3] NCCL INFO Channel 04/0 : 27[3] -> 31[3] [receive] via NET/AWS Libfabric/3
1: nid007117:210408:210972 [3] NCCL INFO Channel 05/0 : 7[3] -> 11[3] [send] via NET/AWS Libfabric/3
7: nid007124:126398:126908 [3] NCCL INFO Channel 01/0 : 31[3] -> 3[3] [send] via NET/AWS Libfabric/3
3: nid007120:197039:197548 [3] NCCL INFO Channel 00/0 : 15[3] -> 14[2] via P2P/CUMEM
7: nid007124:126398:126908 [3] NCCL INFO Channel 05/0 : 31[3] -> 3[3] [send] via NET/AWS Libfabric/3
4: nid007121:116204:116714 [3] NCCL INFO Channel 01/0 : 19[3] -> 18[2] via P2P/CUMEM
2: nid007119:146139:146662 [3] NCCL INFO Channel 02/0 : 11[3] -> 9[1] via P2P/CUMEM
1: nid007117:210408:210972 [3] NCCL INFO Channel 03/0 : 7[3] -> 5[1] via P2P/CUMEM
6: nid007123:108292:108897 [3] NCCL INFO Channel 01/0 : 23[3] -> 27[3] [receive] via NET/AWS Libfabric/3
0: nid007116:202903:203414 [3] NCCL INFO Channel 02/0 : 3[3] -> 1[1] via P2P/CUMEM
6: nid007123:108292:108897 [3] NCCL INFO Channel 05/0 : 23[3] -> 27[3] [receive] via NET/AWS Libfabric/3
7: nid007124:126395:126909 [0] NCCL INFO Channel 01/0 : 24[0] -> 28[0] [receive] via NET/AWS Libfabric/0
2: nid007119:146139:146662 [3] NCCL INFO Channel 06/0 : 11[3] -> 9[1] via P2P/CUMEM
6: nid007123:108292:108897 [3] NCCL INFO Channel 00/0 : 27[3] -> 31[3] [send] via NET/AWS Libfabric/3
1: nid007117:210408:210972 [3] NCCL INFO Channel 07/0 : 7[3] -> 5[1] via P2P/CUMEM
7: nid007124:126395:126909 [0] NCCL INFO Channel 05/0 : 24[0] -> 28[0] [receive] via NET/AWS Libfabric/0
6: nid007123:108292:108897 [3] NCCL INFO Channel 04/0 : 27[3] -> 31[3] [send] via NET/AWS Libfabric/3
7: nid007124:126395:126909 [0] NCCL INFO Channel 00/0 : 28[0] -> 0[0] [send] via NET/AWS Libfabric/0
0: nid007116:202903:203414 [3] NCCL INFO Channel 06/0 : 3[3] -> 1[1] via P2P/CUMEM
3: nid007120:197039:197548 [3] NCCL INFO Channel 02/0 : 15[3] -> 14[2] via P2P/CUMEM
5: nid007122:66178:66711 [3] NCCL INFO Channel 03/0 : 23[3] -> 21[1] via P2P/CUMEM
2: nid007119:146139:146662 [3] NCCL INFO Channel 01/0 : 11[3] -> 10[2] via P2P/CUMEM
3: nid007120:197038:197550 [2] NCCL INFO Channel 00/0 : 14[2] -> 13[1] via P2P/CUMEM
6: nid007123:108289:108900 [0] NCCL INFO Channel 00/0 : 20[0] -> 24[0] [receive] via NET/AWS Libfabric/0
7: nid007124:126398:126908 [3] NCCL INFO Channel 03/0 : 31[3] -> 29[1] via P2P/CUMEM
6: nid007123:108292:108897 [3] NCCL INFO Channel 02/0 : 27[3] -> 25[1] via P2P/CUMEM
1: nid007117:210408:210972 [3] NCCL INFO Channel 00/0 : 7[3] -> 6[2] via P2P/CUMEM
7: nid007124:126395:126909 [0] NCCL INFO Channel 04/0 : 28[0] -> 0[0] [send] via NET/AWS Libfabric/0
0: nid007116:202903:203414 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM
4: nid007121:116203:116715 [2] NCCL INFO Channel 01/0 : 18[2] -> 17[1] via P2P/CUMEM
6: nid007123:108289:108900 [0] NCCL INFO Channel 04/0 : 20[0] -> 24[0] [receive] via NET/AWS Libfabric/0
5: nid007122:66178:66711 [3] NCCL INFO Channel 07/0 : 23[3] -> 21[1] via P2P/CUMEM
4: nid007121:116202:116716 [1] NCCL INFO Channel 01/0 : 17[1] -> 16[0] via P2P/CUMEM
6: nid007123:108289:108900 [0] NCCL INFO Channel 01/0 : 24[0] -> 28[0] [send] via NET/AWS Libfabric/0
4: nid007121:116204:116714 [3] NCCL INFO Channel 03/0 : 19[3] -> 18[2] via P2P/CUMEM
6: nid007123:108292:108897 [3] NCCL INFO Channel 06/0 : 27[3] -> 25[1] via P2P/CUMEM
2: nid007119:146139:146662 [3] NCCL INFO Channel 03/0 : 11[3] -> 10[2] via P2P/CUMEM
3: nid007120:197037:197549 [1] NCCL INFO Channel 00/0 : 13[1] -> 12[0] via P2P/CUMEM
6: nid007123:108289:108900 [0] NCCL INFO Channel 05/0 : 24[0] -> 28[0] [send] via NET/AWS Libfabric/0
7: nid007124:126398:126908 [3] NCCL INFO Channel 07/0 : 31[3] -> 29[1] via P2P/CUMEM
1: nid007117:210405:210971 [0] NCCL INFO Channel 01/0 : 0[0] -> 4[0] [receive] via NET/AWS Libfabric/0
1: nid007117:210408:210972 [3] NCCL INFO Channel 02/0 : 7[3] -> 6[2] via P2P/CUMEM
3: nid007120:197039:197548 [3] NCCL INFO Channel 04/0 : 15[3] -> 14[2] via P2P/CUMEM
3: nid007120:197038:197550 [2] NCCL INFO Channel 04/0 : 14[2] -> 13[1] via P2P/CUMEM
1: nid007117:210405:210971 [0] NCCL INFO Channel 05/0 : 0[0] -> 4[0] [receive] via NET/AWS Libfabric/0
4: nid007121:116203:116715 [2] NCCL INFO Channel 05/0 : 18[2] -> 17[1] via P2P/CUMEM
1: nid007117:210405:210971 [0] NCCL INFO Channel 00/0 : 4[0] -> 8[0] [send] via NET/AWS Libfabric/0
5: nid007122:66178:66711 [3] NCCL INFO Channel 00/0 : 23[3] -> 22[2] via P2P/CUMEM
6: nid007123:108292:108897 [3] NCCL INFO Channel 01/0 : 27[3] -> 26[2] via P2P/CUMEM
0: nid007116:202903:203414 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM
1: nid007117:210405:210971 [0] NCCL INFO Channel 04/0 : 4[0] -> 8[0] [send] via NET/AWS Libfabric/0
2: nid007119:146137:146663 [1] NCCL INFO Channel 01/0 : 9[1] -> 8[0] via P2P/CUMEM
2: nid007119:146139:146662 [3] NCCL INFO Channel 05/0 : 11[3] -> 10[2] via P2P/CUMEM
4: nid007121:116202:116716 [1] NCCL INFO Channel 03/0 : 17[1] -> 16[0] via P2P/CUMEM
7: nid007124:126398:126908 [3] NCCL INFO Channel 00/0 : 31[3] -> 30[2] via P2P/CUMEM
1: nid007117:210406:210973 [1] NCCL INFO Channel 00/0 : 5[1] -> 4[0] via P2P/CUMEM
0: nid007116:202901:203412 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM
3: nid007120:197037:197549 [1] NCCL INFO Channel 02/0 : 13[1] -> 12[0] via P2P/CUMEM
4: nid007121:116204:116714 [3] NCCL INFO Channel 05/0 : 19[3] -> 18[2] via P2P/CUMEM
1: nid007117:210408:210972 [3] NCCL INFO Channel 04/0 : 7[3] -> 6[2] via P2P/CUMEM
3: nid007120:197039:197548 [3] NCCL INFO Channel 06/0 : 15[3] -> 14[2] via P2P/CUMEM
2: nid007119:146137:146663 [1] NCCL INFO Channel 03/0 : 9[1] -> 8[0] via P2P/CUMEM
0: nid007116:202903:203414 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/CUMEM
5: nid007122:66178:66711 [3] NCCL INFO Channel 02/0 : 23[3] -> 22[2] via P2P/CUMEM
4: nid007121:116202:116716 [1] NCCL INFO Channel 05/0 : 17[1] -> 16[0] via P2P/CUMEM
2: nid007119:146139:146662 [3] NCCL INFO Channel 07/0 : 11[3] -> 10[2] via P2P/CUMEM
1: nid007117:210406:210973 [1] NCCL INFO Channel 02/0 : 5[1] -> 4[0] via P2P/CUMEM
4: nid007121:116204:116714 [3] NCCL INFO Channel 07/0 : 19[3] -> 18[2] via P2P/CUMEM
6: nid007123:108292:108897 [3] NCCL INFO Channel 03/0 : 27[3] -> 26[2] via P2P/CUMEM
3: nid007120:197037:197549 [1] NCCL INFO Channel 04/0 : 13[1] -> 12[0] via P2P/CUMEM
0: nid007116:202901:203412 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM
7: nid007124:126398:126908 [3] NCCL INFO Channel 02/0 : 31[3] -> 30[2] via P2P/CUMEM
1: nid007117:210408:210972 [3] NCCL INFO Channel 06/0 : 7[3] -> 6[2] via P2P/CUMEM
3: nid007120:197037:197549 [1] NCCL INFO Channel 06/0 : 13[1] -> 12[0] via P2P/CUMEM
2: nid007119:146137:146663 [1] NCCL INFO Channel 05/0 : 9[1] -> 8[0] via P2P/CUMEM
4: nid007121:116202:116716 [1] NCCL INFO Channel 07/0 : 17[1] -> 16[0] via P2P/CUMEM
5: nid007122:66176:66712 [1] NCCL INFO Channel 00/0 : 21[1] -> 20[0] via P2P/CUMEM
0: nid007116:202903:203414 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/CUMEM
1: nid007117:210406:210973 [1] NCCL INFO Channel 04/0 : 5[1] -> 4[0] via P2P/CUMEM
6: nid007123:108290:108898 [1] NCCL INFO Channel 01/0 : 25[1] -> 24[0] via P2P/CUMEM
6: nid007123:108292:108897 [3] NCCL INFO Channel 05/0 : 27[3] -> 26[2] via P2P/CUMEM
0: nid007116:202901:203412 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM
7: nid007124:126396:126906 [1] NCCL INFO Channel 00/0 : 29[1] -> 28[0] via P2P/CUMEM
5: nid007122:66178:66711 [3] NCCL INFO Channel 04/0 : 23[3] -> 22[2] via P2P/CUMEM
7: nid007124:126398:126908 [3] NCCL INFO Channel 04/0 : 31[3] -> 30[2] via P2P/CUMEM
2: nid007119:146137:146663 [1] NCCL INFO Channel 07/0 : 9[1] -> 8[0] via P2P/CUMEM
1: nid007117:210406:210973 [1] NCCL INFO Channel 06/0 : 5[1] -> 4[0] via P2P/CUMEM
6: nid007123:108290:108898 [1] NCCL INFO Channel 03/0 : 25[1] -> 24[0] via P2P/CUMEM
5: nid007122:66176:66712 [1] NCCL INFO Channel 02/0 : 21[1] -> 20[0] via P2P/CUMEM
0: nid007116:202901:203412 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/CUMEM
6: nid007123:108292:108897 [3] NCCL INFO Channel 07/0 : 27[3] -> 26[2] via P2P/CUMEM
7: nid007124:126396:126906 [1] NCCL INFO Channel 02/0 : 29[1] -> 28[0] via P2P/CUMEM
7: nid007124:126398:126908 [3] NCCL INFO Channel 06/0 : 31[3] -> 30[2] via P2P/CUMEM
5: nid007122:66176:66712 [1] NCCL INFO Channel 04/0 : 21[1] -> 20[0] via P2P/CUMEM
6: nid007123:108290:108898 [1] NCCL INFO Channel 05/0 : 25[1] -> 24[0] via P2P/CUMEM
7: nid007124:126396:126906 [1] NCCL INFO Channel 04/0 : 29[1] -> 28[0] via P2P/CUMEM
5: nid007122:66178:66711 [3] NCCL INFO Channel 06/0 : 23[3] -> 22[2] via P2P/CUMEM
6: nid007123:108290:108898 [1] NCCL INFO Channel 07/0 : 25[1] -> 24[0] via P2P/CUMEM
5: nid007122:66176:66712 [1] NCCL INFO Channel 06/0 : 21[1] -> 20[0] via P2P/CUMEM
7: nid007124:126396:126906 [1] NCCL INFO Channel 06/0 : 29[1] -> 28[0] via P2P/CUMEM
0: nid007116:202902:203415 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM
1: nid007117:210407:210970 [2] NCCL INFO Channel 00/0 : 6[2] -> 5[1] via P2P/CUMEM
2: nid007119:146138:146665 [2] NCCL INFO Channel 01/0 : 10[2] -> 9[1] via P2P/CUMEM
0: nid007116:202902:203415 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/CUMEM
1: nid007117:210407:210970 [2] NCCL INFO Channel 04/0 : 6[2] -> 5[1] via P2P/CUMEM
2: nid007119:146138:146665 [2] NCCL INFO Channel 05/0 : 10[2] -> 9[1] via P2P/CUMEM
6: nid007123:108291:108899 [2] NCCL INFO Channel 01/0 : 26[2] -> 25[1] via P2P/CUMEM
5: nid007122:66177:66710 [2] NCCL INFO Channel 00/0 : 22[2] -> 21[1] via P2P/CUMEM
7: nid007124:126397:126907 [2] NCCL INFO Channel 00/0 : 30[2] -> 29[1] via P2P/CUMEM
6: nid007123:108291:108899 [2] NCCL INFO Channel 05/0 : 26[2] -> 25[1] via P2P/CUMEM
5: nid007122:66177:66710 [2] NCCL INFO Channel 04/0 : 22[2] -> 21[1] via P2P/CUMEM
7: nid007124:126397:126907 [2] NCCL INFO Channel 04/0 : 30[2] -> 29[1] via P2P/CUMEM
0: nid007116:202900:203413 [0] NCCL INFO Connected all rings
0: nid007116:202903:203414 [3] NCCL INFO Connected all rings
0: nid007116:202902:203415 [2] NCCL INFO Connected all rings
0: nid007116:202901:203412 [1] NCCL INFO Connected all rings
7: nid007124:126398:126908 [3] NCCL INFO Connected all rings
7: nid007124:126395:126909 [0] NCCL INFO Connected all rings
5: nid007122:66178:66711 [3] NCCL INFO Connected all rings
5: nid007122:66175:66709 [0] NCCL INFO Connected all rings
7: nid007124:126396:126906 [1] NCCL INFO Connected all rings
6: nid007123:108292:108897 [3] NCCL INFO Connected all rings
1: nid007117:210405:210971 [0] NCCL INFO Connected all rings
1: nid007117:210408:210972 [3] NCCL INFO Connected all rings
5: nid007122:66177:66710 [2] NCCL INFO Connected all rings
5: nid007122:66176:66712 [1] NCCL INFO Connected all rings
7: nid007124:126397:126907 [2] NCCL INFO Connected all rings
6: nid007123:108289:108900 [0] NCCL INFO Connected all rings
1: nid007117:210407:210970 [2] NCCL INFO Connected all rings
6: nid007123:108291:108899 [2] NCCL INFO Connected all rings
1: nid007117:210406:210973 [1] NCCL INFO Connected all rings
6: nid007123:108290:108898 [1] NCCL INFO Connected all rings
4: nid007121:116202:116716 [1] NCCL INFO Connected all rings
4: nid007121:116201:116717 [0] NCCL INFO Connected all rings
4: nid007121:116204:116714 [3] NCCL INFO Connected all rings
3: nid007120:197036:197547 [0] NCCL INFO Connected all rings
4: nid007121:116203:116715 [2] NCCL INFO Connected all rings
3: nid007120:197037:197549 [1] NCCL INFO Connected all rings
2: nid007119:146136:146664 [0] NCCL INFO Connected all rings
3: nid007120:197039:197548 [3] NCCL INFO Connected all rings
2: nid007119:146137:146663 [1] NCCL INFO Connected all rings
3: nid007120:197038:197550 [2] NCCL INFO Connected all rings
2: nid007119:146139:146662 [3] NCCL INFO Connected all rings
2: nid007119:146138:146665 [2] NCCL INFO Connected all rings
0: [2025-07-11 11:47:46,938] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 729, num_elems = 8.29B
3: Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.99s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.97s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.97s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.99s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.24s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.24s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.23s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.23s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  60%|████
2: Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.98s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.98s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.99s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.99s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.24s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.24s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.24s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.24s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  60%|████
6: Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.98s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.98s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.98s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.99s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.24s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.24s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.24s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.24s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  60%|████
0: Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.99s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.97s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.99s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.95s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.24s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.23s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.24s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.25s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  60%|████
1: Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.98s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.98s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.98s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.98s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.24s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.24s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.24s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.24s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  60%|████
5: Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.98s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.98s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.97s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.97s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.24s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.24s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.23s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.23s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  60%|████
4: Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.99s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.97s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.98s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.92s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.24s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.24s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.23s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.21s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  60%|████
7: Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.98s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.97s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.96s/it]Loading checkpoint shards:  20%|██        | 1/5 [00:09<00:39,  9.97s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.24s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.24s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.23s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:21,  7.23s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  60%|████
4: █    | 3/5 [00:18<00:10,  5.46s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.69s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.43s/it]
4: [INFO|modeling_utils.py:4930] 2025-07-11 11:48:09,104 >> All model checkpoint weights were used when initializing Qwen2_5_VLForConditionalGeneration.
4: 
4: [INFO|modeling_utils.py:4938] 2025-07-11 11:48:09,104 >> All the weights of Qwen2_5_VLForConditionalGeneration were initialized from the model checkpoint at Qwen/Qwen2.5-VL-7B-Instruct.
4: If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2_5_VLForConditionalGeneration for predictions without further training.
3: █    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
3: 
6: █    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
4: Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
4: Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
3: Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
3: [INFO|modeling_utils.py:4930] 2025-07-11 11:48:09,113 >> All model checkpoint weights were used when initializing Qwen2_5_VLForConditionalGeneration.
3: 
1: █    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
6: Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
3: [INFO|modeling_utils.py:4938] 2025-07-11 11:48:09,113 >> All the weights of Qwen2_5_VLForConditionalGeneration were initialized from the model checkpoint at Qwen/Qwen2.5-VL-7B-Instruct.
3: If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2_5_VLForConditionalGeneration for predictions without further training.
0: █    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:18<00:11,  5.51s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
6: Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
1: Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
2: █    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
7: █    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
1: Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
2: Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
0: Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
5: █    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:18<00:10,  5.47s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.70s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
2: 
3: Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
5: Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
5: Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
7: Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
1: [INFO|modeling_utils.py:4930] 2025-07-11 11:48:09,113 >> All model checkpoint weights were used when initializing Qwen2_5_VLForConditionalGeneration.
1: 
1: [INFO|modeling_utils.py:4938] 2025-07-11 11:48:09,113 >> All the weights of Qwen2_5_VLForConditionalGeneration were initialized from the model checkpoint at Qwen/Qwen2.5-VL-7B-Instruct.
1: If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2_5_VLForConditionalGeneration for predictions without further training.
7: Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
7: [INFO|modeling_utils.py:4930] 2025-07-11 11:48:09,113 >> All model checkpoint weights were used when initializing Qwen2_5_VLForConditionalGeneration.
7: 
2: [INFO|modeling_utils.py:4930] 2025-07-11 11:48:09,113 >> All model checkpoint weights were used when initializing Qwen2_5_VLForConditionalGeneration.
2: 
7: [INFO|modeling_utils.py:4938] 2025-07-11 11:48:09,113 >> All the weights of Qwen2_5_VLForConditionalGeneration were initialized from the model checkpoint at Qwen/Qwen2.5-VL-7B-Instruct.
7: If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2_5_VLForConditionalGeneration for predictions without further training.
2: [INFO|modeling_utils.py:4938] 2025-07-11 11:48:09,114 >> All the weights of Qwen2_5_VLForConditionalGeneration were initialized from the model checkpoint at Qwen/Qwen2.5-VL-7B-Instruct.
2: If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2_5_VLForConditionalGeneration for predictions without further training.
5: [INFO|modeling_utils.py:4930] 2025-07-11 11:48:09,113 >> All model checkpoint weights were used when initializing Qwen2_5_VLForConditionalGeneration.
5: 
5: [INFO|modeling_utils.py:4938] 2025-07-11 11:48:09,113 >> All the weights of Qwen2_5_VLForConditionalGeneration were initialized from the model checkpoint at Qwen/Qwen2.5-VL-7B-Instruct.
5: If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2_5_VLForConditionalGeneration for predictions without further training.
7: Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
6: Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
1: Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
2: Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
4: Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
6: [INFO|modeling_utils.py:4930] 2025-07-11 11:48:09,114 >> All model checkpoint weights were used when initializing Qwen2_5_VLForConditionalGeneration.
6: 
6: [INFO|modeling_utils.py:4938] 2025-07-11 11:48:09,114 >> All the weights of Qwen2_5_VLForConditionalGeneration were initialized from the model checkpoint at Qwen/Qwen2.5-VL-7B-Instruct.
6: If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2_5_VLForConditionalGeneration for predictions without further training.
5: Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
0: Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.44s/it]
4: [INFO|configuration_utils.py:1097] 2025-07-11 11:48:09,239 >> loading configuration file generation_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/generation_config.json
4: [INFO|configuration_utils.py:1142] 2025-07-11 11:48:09,239 >> Generate config GenerationConfig {
4:   "bos_token_id": 151643,
4:   "do_sample": true,
4:   "eos_token_id": [
4:     151645,
4:     151643
4:   ],
4:   "pad_token_id": 151643,
4:   "repetition_penalty": 1.05,
4:   "temperature": 1e-06
4: }
4: 
1: [INFO|configuration_utils.py:1097] 2025-07-11 11:48:09,239 >> loading configuration file generation_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/generation_config.json
6: [INFO|configuration_utils.py:1097] 2025-07-11 11:48:09,239 >> loading configuration file generation_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/generation_config.json
1: [INFO|configuration_utils.py:1142] 2025-07-11 11:48:09,239 >> Generate config GenerationConfig {
1:   "bos_token_id": 151643,
1:   "do_sample": true,
1:   "eos_token_id": [
1:     151645,
1:     151643
1:   ],
1:   "pad_token_id": 151643,
1:   "repetition_penalty": 1.05,
1:   "temperature": 1e-06
1: }
1: 
5: [INFO|configuration_utils.py:1097] 2025-07-11 11:48:09,239 >> loading configuration file generation_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/generation_config.json
7: [INFO|configuration_utils.py:1097] 2025-07-11 11:48:09,239 >> loading configuration file generation_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/generation_config.json
7: [INFO|configuration_utils.py:1142] 2025-07-11 11:48:09,240 >> Generate config GenerationConfig {
7:   "bos_token_id": 151643,
7:   "do_sample": true,
7:   "eos_token_id": [
7:     151645,
7:     151643
7:   ],
7:   "pad_token_id": 151643,
7:   "repetition_penalty": 1.05,
7:   "temperature": 1e-06
7: }
7: 
6: [INFO|configuration_utils.py:1142] 2025-07-11 11:48:09,240 >> Generate config GenerationConfig {
6:   "bos_token_id": 151643,
6:   "do_sample": true,
6:   "eos_token_id": [
6:     151645,
6:     151643
6:   ],
6:   "pad_token_id": 151643,
6:   "repetition_penalty": 1.05,
6:   "temperature": 1e-06
6: }
6: 
5: [INFO|configuration_utils.py:1142] 2025-07-11 11:48:09,240 >> Generate config GenerationConfig {
5:   "bos_token_id": 151643,
5:   "do_sample": true,
5:   "eos_token_id": [
5:     151645,
5:     151643
5:   ],
5:   "pad_token_id": 151643,
5:   "repetition_penalty": 1.05,
5:   "temperature": 1e-06
5: }
5: 
4: [INFO|2025-07-11 11:48:09] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
4: [INFO|2025-07-11 11:48:09] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
4: [INFO|2025-07-11 11:48:09] llamafactory.model.adapter:143 >> DeepSpeed ZeRO3 detected, remaining trainable params in float32.
4: [INFO|2025-07-11 11:48:09] llamafactory.model.adapter:143 >> Fine-tuning method: Full
1: [INFO|2025-07-11 11:48:09] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
1: [INFO|2025-07-11 11:48:09] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
1: [INFO|2025-07-11 11:48:09] llamafactory.model.adapter:143 >> DeepSpeed ZeRO3 detected, remaining trainable params in float32.
1: [INFO|2025-07-11 11:48:09] llamafactory.model.adapter:143 >> Fine-tuning method: Full
7: [INFO|2025-07-11 11:48:09] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
2: [INFO|configuration_utils.py:1097] 2025-07-11 11:48:09,241 >> loading configuration file generation_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/generation_config.json
7: [INFO|2025-07-11 11:48:09] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
7: [INFO|2025-07-11 11:48:09] llamafactory.model.adapter:143 >> DeepSpeed ZeRO3 detected, remaining trainable params in float32.
5: [INFO|2025-07-11 11:48:09] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
7: [INFO|2025-07-11 11:48:09] llamafactory.model.adapter:143 >> Fine-tuning method: Full
5: [INFO|2025-07-11 11:48:09] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
5: [INFO|2025-07-11 11:48:09] llamafactory.model.adapter:143 >> DeepSpeed ZeRO3 detected, remaining trainable params in float32.
6: [INFO|2025-07-11 11:48:09] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
5: [INFO|2025-07-11 11:48:09] llamafactory.model.adapter:143 >> Fine-tuning method: Full
6: [INFO|2025-07-11 11:48:09] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
6: [INFO|2025-07-11 11:48:09] llamafactory.model.adapter:143 >> DeepSpeed ZeRO3 detected, remaining trainable params in float32.
2: [INFO|configuration_utils.py:1142] 2025-07-11 11:48:09,242 >> Generate config GenerationConfig {
2:   "bos_token_id": 151643,
2:   "do_sample": true,
2:   "eos_token_id": [
2:     151645,
2:     151643
2:   ],
2:   "pad_token_id": 151643,
2:   "repetition_penalty": 1.05,
2:   "temperature": 1e-06
2: }
2: 
6: [INFO|2025-07-11 11:48:09] llamafactory.model.adapter:143 >> Fine-tuning method: Full
2: [INFO|2025-07-11 11:48:09] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
2: [INFO|2025-07-11 11:48:09] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
2: [INFO|2025-07-11 11:48:09] llamafactory.model.adapter:143 >> DeepSpeed ZeRO3 detected, remaining trainable params in float32.
2: [INFO|2025-07-11 11:48:09] llamafactory.model.adapter:143 >> Fine-tuning method: Full
4: [INFO|2025-07-11 11:48:09] llamafactory.model.loader:143 >> trainable params: 8,292,166,656 || all params: 8,292,166,656 || trainable%: 100.0000
1: [INFO|2025-07-11 11:48:09] llamafactory.model.loader:143 >> trainable params: 8,292,166,656 || all params: 8,292,166,656 || trainable%: 100.0000
7: [INFO|2025-07-11 11:48:09] llamafactory.model.loader:143 >> trainable params: 8,292,166,656 || all params: 8,292,166,656 || trainable%: 100.0000
5: [INFO|2025-07-11 11:48:09] llamafactory.model.loader:143 >> trainable params: 8,292,166,656 || all params: 8,292,166,656 || trainable%: 100.0000
6: [INFO|2025-07-11 11:48:09] llamafactory.model.loader:143 >> trainable params: 8,292,166,656 || all params: 8,292,166,656 || trainable%: 100.0000
2: [INFO|2025-07-11 11:48:09] llamafactory.model.loader:143 >> trainable params: 8,292,166,656 || all params: 8,292,166,656 || trainable%: 100.0000
3: [INFO|configuration_utils.py:1097] 2025-07-11 11:48:09,249 >> loading configuration file generation_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/generation_config.json
3: [INFO|configuration_utils.py:1142] 2025-07-11 11:48:09,249 >> Generate config GenerationConfig {
3:   "bos_token_id": 151643,
3:   "do_sample": true,
3:   "eos_token_id": [
3:     151645,
3:     151643
3:   ],
3:   "pad_token_id": 151643,
3:   "repetition_penalty": 1.05,
3:   "temperature": 1e-06
3: }
3: 
3: [INFO|2025-07-11 11:48:09] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
3: [INFO|2025-07-11 11:48:09] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
3: [INFO|2025-07-11 11:48:09] llamafactory.model.adapter:143 >> DeepSpeed ZeRO3 detected, remaining trainable params in float32.
3: [INFO|2025-07-11 11:48:09] llamafactory.model.adapter:143 >> Fine-tuning method: Full
3: [INFO|2025-07-11 11:48:09] llamafactory.model.loader:143 >> trainable params: 8,292,166,656 || all params: 8,292,166,656 || trainable%: 100.0000
7: [INFO|trainer.py:748] 2025-07-11 11:48:09,259 >> Using auto half precision backend
1: [INFO|trainer.py:748] 2025-07-11 11:48:09,259 >> Using auto half precision backend
5: [INFO|trainer.py:748] 2025-07-11 11:48:09,260 >> Using auto half precision backend
4: [INFO|trainer.py:748] 2025-07-11 11:48:09,260 >> Using auto half precision backend
2: [INFO|trainer.py:748] 2025-07-11 11:48:09,262 >> Using auto half precision backend
6: [INFO|trainer.py:748] 2025-07-11 11:48:09,261 >> Using auto half precision backend
3: [INFO|trainer.py:748] 2025-07-11 11:48:09,270 >> Using auto half precision backend
0: Loading checkpoint shards:  80%|████████  | 4/5 [00:22<00:04,  4.75s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  3.24s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:22<00:00,  4.58s/it]
0: [INFO|modeling_utils.py:4930] 2025-07-11 11:48:09,855 >> All model checkpoint weights were used when initializing Qwen2_5_VLForConditionalGeneration.
0: 
0: [INFO|modeling_utils.py:4938] 2025-07-11 11:48:09,856 >> All the weights of Qwen2_5_VLForConditionalGeneration were initialized from the model checkpoint at Qwen/Qwen2.5-VL-7B-Instruct.
0: If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2_5_VLForConditionalGeneration for predictions without further training.
7: [2025-07-11 11:48:09,930] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
3: [2025-07-11 11:48:09,930] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
4: [2025-07-11 11:48:09,931] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
2: [2025-07-11 11:48:09,935] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
1: [2025-07-11 11:48:09,937] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
5: [2025-07-11 11:48:09,939] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
6: [2025-07-11 11:48:09,941] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
0: [INFO|configuration_utils.py:1097] 2025-07-11 11:48:09,983 >> loading configuration file generation_config.json from cache at /capstor/scratch/cscs/ndeperr/.cache/hub/models--Qwen--Qwen2.5-VL-7B-Instruct/snapshots/cc594898137f460bfe9f0759e9844b3ce807cfb5/generation_config.json
0: [INFO|configuration_utils.py:1142] 2025-07-11 11:48:09,983 >> Generate config GenerationConfig {
0:   "bos_token_id": 151643,
0:   "do_sample": true,
0:   "eos_token_id": [
0:     151645,
0:     151643
0:   ],
0:   "pad_token_id": 151643,
0:   "repetition_penalty": 1.05,
0:   "temperature": 1e-06
0: }
0: 
0: [INFO|2025-07-11 11:48:09] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
0: [INFO|2025-07-11 11:48:09] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
0: [INFO|2025-07-11 11:48:09] llamafactory.model.adapter:143 >> DeepSpeed ZeRO3 detected, remaining trainable params in float32.
0: [INFO|2025-07-11 11:48:09] llamafactory.model.adapter:143 >> Fine-tuning method: Full
0: [INFO|2025-07-11 11:48:09] llamafactory.model.loader:143 >> trainable params: 8,292,166,656 || all params: 8,292,166,656 || trainable%: 100.0000
0: [INFO|trainer.py:748] 2025-07-11 11:48:10,004 >> Using auto half precision backend
0: nid007116:202902:203491 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM
0: nid007116:202902:203491 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM
0: nid007116:202902:203491 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/CUMEM
0: nid007116:202902:203491 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/CUMEM
0: nid007116:202902:203491 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM
0: nid007116:202902:203491 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/CUMEM
0: nid007116:202901:203492 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM
0: nid007116:202901:203492 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM
3: nid007120:197037:197633 [1] NCCL INFO Channel 00/0 : 13[1] -> 14[2] via P2P/CUMEM
4: nid007121:116203:116798 [2] NCCL INFO Channel 01/0 : 18[2] -> 19[3] via P2P/CUMEM
4: nid007121:116203:116798 [2] NCCL INFO Channel 02/0 : 18[2] -> 19[3] via P2P/CUMEM
0: nid007116:202903:203493 [3] NCCL INFO Channel 02/0 : 3[3] -> 0[0] via P2P/CUMEM
3: nid007120:197037:197633 [1] NCCL INFO Channel 04/0 : 13[1] -> 14[2] via P2P/CUMEM
4: nid007121:116203:116798 [2] NCCL INFO Channel 03/0 : 18[2] -> 19[3] via P2P/CUMEM
0: nid007116:202902:203491 [2] NCCL INFO Channel 06/0 : 2[2] -> 6[2] [send] via NET/AWS Libfabric/2
7: nid007124:126395:126988 [0] NCCL INFO Channel 00/0 : 28[0] -> 29[1] via P2P/CUMEM
0: nid007116:202903:203493 [3] NCCL INFO Channel 03/0 : 3[3] -> 0[0] via P2P/CUMEM
4: nid007121:116203:116798 [2] NCCL INFO Channel 05/0 : 18[2] -> 19[3] via P2P/CUMEM
4: nid007121:116203:116798 [2] NCCL INFO Channel 06/0 : 18[2] -> 19[3] via P2P/CUMEM
0: nid007116:202903:203493 [3] NCCL INFO Channel 06/0 : 3[3] -> 0[0] via P2P/CUMEM
4: nid007121:116203:116798 [2] NCCL INFO Channel 07/0 : 18[2] -> 19[3] via P2P/CUMEM
0: nid007116:202903:203493 [3] NCCL INFO Channel 07/0 : 3[3] -> 0[0] via P2P/CUMEM
7: nid007124:126395:126988 [0] NCCL INFO Channel 02/0 : 28[0] -> 29[1] via P2P/CUMEM
7: nid007124:126395:126988 [0] NCCL INFO Channel 03/0 : 28[0] -> 29[1] via P2P/CUMEM
3: nid007120:197036:197635 [0] NCCL INFO Channel 00/0 : 12[0] -> 13[1] via P2P/CUMEM
7: nid007124:126395:126988 [0] NCCL INFO Channel 04/0 : 28[0] -> 29[1] via P2P/CUMEM
3: nid007120:197036:197635 [0] NCCL INFO Channel 02/0 : 12[0] -> 13[1] via P2P/CUMEM
7: nid007124:126395:126988 [0] NCCL INFO Channel 06/0 : 28[0] -> 29[1] via P2P/CUMEM
3: nid007120:197036:197635 [0] NCCL INFO Channel 03/0 : 12[0] -> 13[1] via P2P/CUMEM
7: nid007124:126395:126988 [0] NCCL INFO Channel 07/0 : 28[0] -> 29[1] via P2P/CUMEM
3: nid007120:197036:197635 [0] NCCL INFO Channel 04/0 : 12[0] -> 13[1] via P2P/CUMEM
3: nid007120:197036:197635 [0] NCCL INFO Channel 06/0 : 12[0] -> 13[1] via P2P/CUMEM
4: nid007121:116201:116800 [0] NCCL INFO Channel 01/0 : 16[0] -> 17[1] via P2P/CUMEM
3: nid007120:197036:197635 [0] NCCL INFO Channel 07/0 : 12[0] -> 13[1] via P2P/CUMEM
3: nid007120:197038:197636 [2] NCCL INFO Channel 00/0 : 14[2] -> 15[3] via P2P/CUMEM
3: nid007120:197036:197635 [0] NCCL INFO Channel 00/0 : 8[0] -> 12[0] [receive] via NET/AWS Libfabric/0
3: nid007120:197038:197636 [2] NCCL INFO Channel 02/0 : 14[2] -> 15[3] via P2P/CUMEM
4: nid007121:116201:116800 [0] NCCL INFO Channel 02/0 : 16[0] -> 17[1] via P2P/CUMEM
3: nid007120:197038:197636 [2] NCCL INFO Channel 03/0 : 14[2] -> 15[3] via P2P/CUMEM
4: nid007121:116201:116800 [0] NCCL INFO Channel 03/0 : 16[0] -> 17[1] via P2P/CUMEM
3: nid007120:197038:197636 [2] NCCL INFO Channel 04/0 : 14[2] -> 15[3] via P2P/CUMEM
3: nid007120:197038:197636 [2] NCCL INFO Channel 06/0 : 14[2] -> 15[3] via P2P/CUMEM
4: nid007121:116201:116800 [0] NCCL INFO Channel 05/0 : 16[0] -> 17[1] via P2P/CUMEM
3: nid007120:197038:197636 [2] NCCL INFO Channel 07/0 : 14[2] -> 15[3] via P2P/CUMEM
4: nid007121:116201:116800 [0] NCCL INFO Channel 06/0 : 16[0] -> 17[1] via P2P/CUMEM
4: nid007121:116201:116800 [0] NCCL INFO Channel 07/0 : 16[0] -> 17[1] via P2P/CUMEM
3: nid007120:197039:197634 [3] NCCL INFO Channel 02/0 : 15[3] -> 12[0] via P2P/CUMEM
4: nid007121:116204:116799 [3] NCCL INFO Channel 02/0 : 19[3] -> 16[0] via P2P/CUMEM
3: nid007120:197038:197636 [2] NCCL INFO Channel 02/0 : 10[2] -> 14[2] [receive] via NET/AWS Libfabric/2
4: nid007121:116204:116799 [3] NCCL INFO Channel 03/0 : 19[3] -> 16[0] via P2P/CUMEM
3: nid007120:197039:197634 [3] NCCL INFO Channel 03/0 : 15[3] -> 12[0] via P2P/CUMEM
4: nid007121:116204:116799 [3] NCCL INFO Channel 06/0 : 19[3] -> 16[0] via P2P/CUMEM
3: nid007120:197037:197633 [1] NCCL INFO Channel 01/0 : 13[1] -> 12[0] via P2P/CUMEM
4: nid007121:116204:116799 [3] NCCL INFO Channel 07/0 : 19[3] -> 16[0] via P2P/CUMEM
3: nid007120:197039:197634 [3] NCCL INFO Channel 06/0 : 15[3] -> 12[0] via P2P/CUMEM
3: nid007120:197037:197633 [1] NCCL INFO Channel 03/0 : 13[1] -> 12[0] via P2P/CUMEM
3: nid007120:197039:197634 [3] NCCL INFO Channel 07/0 : 15[3] -> 12[0] via P2P/CUMEM
2: nid007119:146136:146747 [0] NCCL INFO Channel 01/0 : 8[0] -> 9[1] via P2P/CUMEM
3: nid007120:197037:197633 [1] NCCL INFO Channel 05/0 : 13[1] -> 12[0] via P2P/CUMEM
3: nid007120:197037:197633 [1] NCCL INFO Channel 07/0 : 13[1] -> 12[0] via P2P/CUMEM
7: nid007124:126397:126989 [2] NCCL INFO Channel 00/0 : 30[2] -> 31[3] via P2P/CUMEM
2: nid007119:146137:146746 [1] NCCL INFO Channel 01/0 : 9[1] -> 10[2] via P2P/CUMEM
2: nid007119:146136:146747 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[1] via P2P/CUMEM
7: nid007124:126397:126989 [2] NCCL INFO Channel 02/0 : 30[2] -> 31[3] via P2P/CUMEM
2: nid007119:146137:146746 [1] NCCL INFO Channel 05/0 : 9[1] -> 10[2] via P2P/CUMEM
2: nid007119:146136:146747 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[1] via P2P/CUMEM
7: nid007124:126397:126989 [2] NCCL INFO Channel 03/0 : 30[2] -> 31[3] via P2P/CUMEM
2: nid007119:146136:146747 [0] NCCL INFO Channel 05/0 : 8[0] -> 9[1] via P2P/CUMEM
4: nid007121:116202:116801 [1] NCCL INFO Channel 01/0 : 17[1] -> 18[2] via P2P/CUMEM
2: nid007119:146136:146747 [0] NCCL INFO Channel 06/0 : 8[0] -> 9[1] via P2P/CUMEM
7: nid007124:126397:126989 [2] NCCL INFO Channel 04/0 : 30[2] -> 31[3] via P2P/CUMEM
4: nid007121:116202:116801 [1] NCCL INFO Channel 05/0 : 17[1] -> 18[2] via P2P/CUMEM
2: nid007119:146136:146747 [0] NCCL INFO Channel 07/0 : 8[0] -> 9[1] via P2P/CUMEM
7: nid007124:126397:126989 [2] NCCL INFO Channel 06/0 : 30[2] -> 31[3] via P2P/CUMEM
4: nid007121:116201:116800 [0] NCCL INFO Channel 04/0 : 16[0] -> 20[0] [send] via NET/AWS Libfabric/0
2: nid007119:146136:146747 [0] NCCL INFO Channel 01/0 : 4[0] -> 8[0] [receive] via NET/AWS Libfabric/0
4: nid007121:116203:116798 [2] NCCL INFO Channel 06/0 : 18[2] -> 22[2] [send] via NET/AWS Libfabric/2
7: nid007124:126397:126989 [2] NCCL INFO Channel 07/0 : 30[2] -> 31[3] via P2P/CUMEM
2: nid007119:146136:146747 [0] NCCL INFO Channel 05/0 : 4[0] -> 8[0] [receive] via NET/AWS Libfabric/0
2: nid007119:146136:146747 [0] NCCL INFO Channel 00/0 : 8[0] -> 12[0] [send] via NET/AWS Libfabric/0
4: nid007121:116202:116801 [1] NCCL INFO Channel 00/0 : 17[1] -> 16[0] via P2P/CUMEM
3: nid007120:197036:197635 [0] NCCL INFO Channel 04/0 : 4[0] -> 12[0] [receive] via NET/AWS Libfabric/0
4: nid007121:116202:116801 [1] NCCL INFO Channel 02/0 : 17[1] -> 16[0] via P2P/CUMEM
3: nid007120:197036:197635 [0] NCCL INFO Channel 05/0 : 4[0] -> 12[0] [receive] via NET/AWS Libfabric/0
3: nid007120:197036:197635 [0] NCCL INFO Channel 04/0 : 12[0] -> 20[0] [send] via NET/AWS Libfabric/0
4: nid007121:116202:116801 [1] NCCL INFO Channel 04/0 : 17[1] -> 16[0] via P2P/CUMEM
3: nid007120:197036:197635 [0] NCCL INFO Channel 05/0 : 12[0] -> 20[0] [send] via NET/AWS Libfabric/0
7: nid007124:126398:126987 [3] NCCL INFO Channel 02/0 : 31[3] -> 28[0] via P2P/CUMEM
7: nid007124:126396:126990 [1] NCCL INFO Channel 00/0 : 29[1] -> 30[2] via P2P/CUMEM
4: nid007121:116202:116801 [1] NCCL INFO Channel 06/0 : 17[1] -> 16[0] via P2P/CUMEM
7: nid007124:126398:126987 [3] NCCL INFO Channel 03/0 : 31[3] -> 28[0] via P2P/CUMEM
7: nid007124:126396:126990 [1] NCCL INFO Channel 04/0 : 29[1] -> 30[2] via P2P/CUMEM
1: nid007117:210405:211083 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[1] via P2P/CUMEM
7: nid007124:126398:126987 [3] NCCL INFO Channel 06/0 : 31[3] -> 28[0] via P2P/CUMEM
2: nid007119:146138:146748 [2] NCCL INFO Channel 01/0 : 10[2] -> 11[3] via P2P/CUMEM
1: nid007117:210407:211082 [2] NCCL INFO Channel 00/0 : 6[2] -> 7[3] via P2P/CUMEM
1: nid007117:210407:211082 [2] NCCL INFO Channel 02/0 : 6[2] -> 7[3] via P2P/CUMEM
7: nid007124:126398:126987 [3] NCCL INFO Channel 07/0 : 31[3] -> 28[0] via P2P/CUMEM
2: nid007119:146138:146748 [2] NCCL INFO Channel 02/0 : 10[2] -> 11[3] via P2P/CUMEM
2: nid007119:146138:146748 [2] NCCL INFO Channel 03/0 : 10[2] -> 11[3] via P2P/CUMEM
7: nid007124:126395:126988 [0] NCCL INFO Channel 00/0 : 24[0] -> 28[0] [receive] via NET/AWS Libfabric/0
7: nid007124:126397:126989 [2] NCCL INFO Channel 02/0 : 26[2] -> 30[2] [receive] via NET/AWS Libfabric/2
2: nid007119:146138:146748 [2] NCCL INFO Channel 05/0 : 10[2] -> 11[3] via P2P/CUMEM
2: nid007119:146138:146748 [2] NCCL INFO Channel 06/0 : 10[2] -> 11[3] via P2P/CUMEM
7: nid007124:126396:126990 [1] NCCL INFO Channel 01/0 : 29[1] -> 28[0] via P2P/CUMEM
2: nid007119:146138:146748 [2] NCCL INFO Channel 07/0 : 10[2] -> 11[3] via P2P/CUMEM
7: nid007124:126396:126990 [1] NCCL INFO Channel 03/0 : 29[1] -> 28[0] via P2P/CUMEM
5: nid007122:66175:66802 [0] NCCL INFO Channel 00/0 : 20[0] -> 21[1] via P2P/CUMEM
7: nid007124:126396:126990 [1] NCCL INFO Channel 05/0 : 29[1] -> 28[0] via P2P/CUMEM
2: nid007119:146139:146745 [3] NCCL INFO Channel 02/0 : 11[3] -> 8[0] via P2P/CUMEM
7: nid007124:126396:126990 [1] NCCL INFO Channel 07/0 : 29[1] -> 28[0] via P2P/CUMEM
2: nid007119:146138:146748 [2] NCCL INFO Channel 03/0 : 6[2] -> 10[2] [receive] via NET/AWS Libfabric/2
5: nid007122:66175:66802 [0] NCCL INFO Channel 02/0 : 20[0] -> 21[1] via P2P/CUMEM
2: nid007119:146139:146745 [3] NCCL INFO Channel 03/0 : 11[3] -> 8[0] via P2P/CUMEM
2: nid007119:146138:146748 [2] NCCL INFO Channel 07/0 : 6[2] -> 10[2] [receive] via NET/AWS Libfabric/2
2: nid007119:146138:146748 [2] NCCL INFO Channel 02/0 : 10[2] -> 14[2] [send] via NET/AWS Libfabric/2
1: nid007117:210407:211082 [2] NCCL INFO Channel 03/0 : 6[2] -> 7[3] via P2P/CUMEM
1: nid007117:210407:211082 [2] NCCL INFO Channel 04/0 : 6[2] -> 7[3] via P2P/CUMEM
1: nid007117:210407:211082 [2] NCCL INFO Channel 06/0 : 6[2] -> 7[3] via P2P/CUMEM
5: nid007122:66175:66802 [0] NCCL INFO Channel 03/0 : 20[0] -> 21[1] via P2P/CUMEM
1: nid007117:210407:211082 [2] NCCL INFO Channel 07/0 : 6[2] -> 7[3] via P2P/CUMEM
2: nid007119:146137:146746 [1] NCCL INFO Channel 00/0 : 9[1] -> 8[0] via P2P/CUMEM
2: nid007119:146139:146745 [3] NCCL INFO Channel 06/0 : 11[3] -> 8[0] via P2P/CUMEM
5: nid007122:66175:66802 [0] NCCL INFO Channel 04/0 : 20[0] -> 21[1] via P2P/CUMEM
3: nid007120:197038:197636 [2] NCCL INFO Channel 06/0 : 6[2] -> 14[2] [receive] via NET/AWS Libfabric/2
5: nid007122:66175:66802 [0] NCCL INFO Channel 06/0 : 20[0] -> 21[1] via P2P/CUMEM
6: nid007123:108291:108979 [2] NCCL INFO Channel 01/0 : 26[2] -> 27[3] via P2P/CUMEM
3: nid007120:197038:197636 [2] NCCL INFO Channel 07/0 : 6[2] -> 14[2] [receive] via NET/AWS Libfabric/2
2: nid007119:146139:146745 [3] NCCL INFO Channel 07/0 : 11[3] -> 8[0] via P2P/CUMEM
3: nid007120:197038:197636 [2] NCCL INFO Channel 06/0 : 14[2] -> 22[2] [send] via NET/AWS Libfabric/2
2: nid007119:146137:146746 [1] NCCL INFO Channel 02/0 : 9[1] -> 8[0] via P2P/CUMEM
5: nid007122:66175:66802 [0] NCCL INFO Channel 07/0 : 20[0] -> 21[1] via P2P/CUMEM
3: nid007120:197038:197636 [2] NCCL INFO Channel 07/0 : 14[2] -> 22[2] [send] via NET/AWS Libfabric/2
1: nid007117:210405:211083 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[1] via P2P/CUMEM
1: nid007117:210408:211084 [3] NCCL INFO Channel 02/0 : 7[3] -> 4[0] via P2P/CUMEM
2: nid007119:146137:146746 [1] NCCL INFO Channel 04/0 : 9[1] -> 8[0] via P2P/CUMEM
6: nid007123:108291:108979 [2] NCCL INFO Channel 02/0 : 26[2] -> 27[3] via P2P/CUMEM
2: nid007119:146137:146746 [1] NCCL INFO Channel 06/0 : 9[1] -> 8[0] via P2P/CUMEM
1: nid007117:210408:211084 [3] NCCL INFO Channel 03/0 : 7[3] -> 4[0] via P2P/CUMEM
1: nid007117:210405:211083 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[1] via P2P/CUMEM
1: nid007117:210408:211084 [3] NCCL INFO Channel 06/0 : 7[3] -> 4[0] via P2P/CUMEM
6: nid007123:108291:108979 [2] NCCL INFO Channel 03/0 : 26[2] -> 27[3] via P2P/CUMEM
1: nid007117:210405:211083 [0] NCCL INFO Channel 04/0 : 4[0] -> 5[1] via P2P/CUMEM
1: nid007117:210408:211084 [3] NCCL INFO Channel 07/0 : 7[3] -> 4[0] via P2P/CUMEM
1: nid007117:210405:211083 [0] NCCL INFO Channel 06/0 : 4[0] -> 5[1] via P2P/CUMEM
6: nid007123:108291:108979 [2] NCCL INFO Channel 05/0 : 26[2] -> 27[3] via P2P/CUMEM
1: nid007117:210405:211083 [0] NCCL INFO Channel 07/0 : 4[0] -> 5[1] via P2P/CUMEM
5: nid007122:66176:66803 [1] NCCL INFO Channel 00/0 : 21[1] -> 22[2] via P2P/CUMEM
5: nid007122:66176:66803 [1] NCCL INFO Channel 04/0 : 21[1] -> 22[2] via P2P/CUMEM
6: nid007123:108291:108979 [2] NCCL INFO Channel 06/0 : 26[2] -> 27[3] via P2P/CUMEM
6: nid007123:108291:108979 [2] NCCL INFO Channel 07/0 : 26[2] -> 27[3] via P2P/CUMEM
5: nid007122:66175:66802 [0] NCCL INFO Channel 04/0 : 16[0] -> 20[0] [receive] via NET/AWS Libfabric/0
5: nid007122:66175:66802 [0] NCCL INFO Channel 01/0 : 20[0] -> 24[0] [send] via NET/AWS Libfabric/0
5: nid007122:66175:66802 [0] NCCL INFO Channel 05/0 : 20[0] -> 24[0] [send] via NET/AWS Libfabric/0
6: nid007123:108290:108980 [1] NCCL INFO Channel 01/0 : 25[1] -> 26[2] via P2P/CUMEM
6: nid007123:108290:108980 [1] NCCL INFO Channel 05/0 : 25[1] -> 26[2] via P2P/CUMEM
4: nid007121:116201:116800 [0] NCCL INFO Channel 00/0 : 8[0] -> 16[0] [receive] via NET/AWS Libfabric/0
6: nid007123:108292:108978 [3] NCCL INFO Channel 02/0 : 27[3] -> 24[0] via P2P/CUMEM
4: nid007121:116201:116800 [0] NCCL INFO Channel 01/0 : 8[0] -> 16[0] [receive] via NET/AWS Libfabric/0
6: nid007123:108289:108981 [0] NCCL INFO Channel 01/0 : 24[0] -> 25[1] via P2P/CUMEM
4: nid007121:116201:116800 [0] NCCL INFO Channel 00/0 : 16[0] -> 24[0] [send] via NET/AWS Libfabric/0
4: nid007121:116201:116800 [0] NCCL INFO Channel 01/0 : 16[0] -> 24[0] [send] via NET/AWS Libfabric/0
6: nid007123:108292:108978 [3] NCCL INFO Channel 03/0 : 27[3] -> 24[0] via P2P/CUMEM
6: nid007123:108292:108978 [3] NCCL INFO Channel 06/0 : 27[3] -> 24[0] via P2P/CUMEM
6: nid007123:108291:108979 [2] NCCL INFO Channel 03/0 : 22[2] -> 26[2] [receive] via NET/AWS Libfabric/2
6: nid007123:108291:108979 [2] NCCL INFO Channel 07/0 : 22[2] -> 26[2] [receive] via NET/AWS Libfabric/2
6: nid007123:108292:108978 [3] NCCL INFO Channel 07/0 : 27[3] -> 24[0] via P2P/CUMEM
6: nid007123:108291:108979 [2] NCCL INFO Channel 02/0 : 26[2] -> 30[2] [send] via NET/AWS Libfabric/2
6: nid007123:108289:108981 [0] NCCL INFO Channel 02/0 : 24[0] -> 25[1] via P2P/CUMEM
6: nid007123:108289:108981 [0] NCCL INFO Channel 03/0 : 24[0] -> 25[1] via P2P/CUMEM
7: nid007124:126397:126989 [2] NCCL INFO Channel 06/0 : 14[2] -> 30[2] [receive] via NET/AWS Libfabric/2
6: nid007123:108289:108981 [0] NCCL INFO Channel 05/0 : 24[0] -> 25[1] via P2P/CUMEM
7: nid007124:126397:126989 [2] NCCL INFO Channel 07/0 : 14[2] -> 30[2] [receive] via NET/AWS Libfabric/2
7: nid007124:126397:126989 [2] NCCL INFO Channel 06/0 : 30[2] -> 14[2] [send] via NET/AWS Libfabric/2
6: nid007123:108289:108981 [0] NCCL INFO Channel 06/0 : 24[0] -> 25[1] via P2P/CUMEM
7: nid007124:126397:126989 [2] NCCL INFO Channel 07/0 : 30[2] -> 14[2] [send] via NET/AWS Libfabric/2
6: nid007123:108289:108981 [0] NCCL INFO Channel 07/0 : 24[0] -> 25[1] via P2P/CUMEM
6: nid007123:108289:108981 [0] NCCL INFO Channel 01/0 : 20[0] -> 24[0] [receive] via NET/AWS Libfabric/0
6: nid007123:108290:108980 [1] NCCL INFO Channel 00/0 : 25[1] -> 24[0] via P2P/CUMEM
6: nid007123:108289:108981 [0] NCCL INFO Channel 05/0 : 20[0] -> 24[0] [receive] via NET/AWS Libfabric/0
6: nid007123:108289:108981 [0] NCCL INFO Channel 00/0 : 24[0] -> 28[0] [send] via NET/AWS Libfabric/0
6: nid007123:108290:108980 [1] NCCL INFO Channel 02/0 : 25[1] -> 24[0] via P2P/CUMEM
1: nid007117:210406:211085 [1] NCCL INFO Channel 00/0 : 5[1] -> 6[2] via P2P/CUMEM
5: nid007122:66175:66802 [0] NCCL INFO Channel 04/0 : 12[0] -> 20[0] [receive] via NET/AWS Libfabric/0
6: nid007123:108290:108980 [1] NCCL INFO Channel 04/0 : 25[1] -> 24[0] via P2P/CUMEM
5: nid007122:66175:66802 [0] NCCL INFO Channel 05/0 : 12[0] -> 20[0] [receive] via NET/AWS Libfabric/0
1: nid007117:210406:211085 [1] NCCL INFO Channel 04/0 : 5[1] -> 6[2] via P2P/CUMEM
6: nid007123:108289:108981 [0] NCCL INFO Channel 00/0 : 16[0] -> 24[0] [receive] via NET/AWS Libfabric/0
6: nid007123:108290:108980 [1] NCCL INFO Channel 06/0 : 25[1] -> 24[0] via P2P/CUMEM
7: nid007124:126395:126988 [0] NCCL INFO Channel 04/0 : 12[0] -> 28[0] [receive] via NET/AWS Libfabric/0
6: nid007123:108289:108981 [0] NCCL INFO Channel 01/0 : 16[0] -> 24[0] [receive] via NET/AWS Libfabric/0
5: nid007122:66175:66802 [0] NCCL INFO Channel 04/0 : 20[0] -> 12[0] [send] via NET/AWS Libfabric/0
7: nid007124:126395:126988 [0] NCCL INFO Channel 05/0 : 12[0] -> 28[0] [receive] via NET/AWS Libfabric/0
5: nid007122:66175:66802 [0] NCCL INFO Channel 05/0 : 20[0] -> 12[0] [send] via NET/AWS Libfabric/0
7: nid007124:126395:126988 [0] NCCL INFO Channel 04/0 : 28[0] -> 12[0] [send] via NET/AWS Libfabric/0
6: nid007123:108289:108981 [0] NCCL INFO Channel 00/0 : 24[0] -> 16[0] [send] via NET/AWS Libfabric/0
7: nid007124:126395:126988 [0] NCCL INFO Channel 05/0 : 28[0] -> 12[0] [send] via NET/AWS Libfabric/0
6: nid007123:108289:108981 [0] NCCL INFO Channel 01/0 : 24[0] -> 16[0] [send] via NET/AWS Libfabric/0
1: nid007117:210405:211083 [0] NCCL INFO Channel 04/0 : 0[0] -> 4[0] [receive] via NET/AWS Libfabric/0
1: nid007117:210407:211082 [2] NCCL INFO Channel 06/0 : 2[2] -> 6[2] [receive] via NET/AWS Libfabric/2
1: nid007117:210405:211083 [0] NCCL INFO Channel 01/0 : 4[0] -> 8[0] [send] via NET/AWS Libfabric/0
1: nid007117:210407:211082 [2] NCCL INFO Channel 03/0 : 6[2] -> 10[2] [send] via NET/AWS Libfabric/2
1: nid007117:210407:211082 [2] NCCL INFO Channel 07/0 : 6[2] -> 10[2] [send] via NET/AWS Libfabric/2
1: nid007117:210406:211085 [1] NCCL INFO Channel 01/0 : 5[1] -> 4[0] via P2P/CUMEM
1: nid007117:210405:211083 [0] NCCL INFO Channel 05/0 : 4[0] -> 8[0] [send] via NET/AWS Libfabric/0
1: nid007117:210406:211085 [1] NCCL INFO Channel 03/0 : 5[1] -> 4[0] via P2P/CUMEM
0: nid007116:202902:203491 [2] NCCL INFO Channel 02/0 : 18[2] -> 2[2] [receive] via NET/AWS Libfabric/2
1: nid007117:210406:211085 [1] NCCL INFO Channel 05/0 : 5[1] -> 4[0] via P2P/CUMEM
1: nid007117:210407:211082 [2] NCCL INFO Channel 06/0 : 6[2] -> 14[2] [send] via NET/AWS Libfabric/2
0: nid007116:202902:203491 [2] NCCL INFO Channel 03/0 : 18[2] -> 2[2] [receive] via NET/AWS Libfabric/2
1: nid007117:210407:211082 [2] NCCL INFO Channel 07/0 : 6[2] -> 14[2] [send] via NET/AWS Libfabric/2
2: nid007119:146138:146748 [2] NCCL INFO Channel 02/0 : 10[2] -> 18[2] [send] via NET/AWS Libfabric/2
1: nid007117:210406:211085 [1] NCCL INFO Channel 07/0 : 5[1] -> 4[0] via P2P/CUMEM
0: nid007116:202902:203491 [2] NCCL INFO Channel 02/0 : 2[2] -> 18[2] [send] via NET/AWS Libfabric/2
0: nid007116:202902:203491 [2] NCCL INFO Channel 03/0 : 2[2] -> 18[2] [send] via NET/AWS Libfabric/2
2: nid007119:146138:146748 [2] NCCL INFO Channel 03/0 : 10[2] -> 18[2] [send] via NET/AWS Libfabric/2
2: nid007119:146136:146747 [0] NCCL INFO Channel 00/0 : 8[0] -> 16[0] [send] via NET/AWS Libfabric/0
1: nid007117:210407:211082 [2] NCCL INFO Channel 06/0 : 14[2] -> 6[2] [receive] via NET/AWS Libfabric/2
2: nid007119:146136:146747 [0] NCCL INFO Channel 01/0 : 8[0] -> 16[0] [send] via NET/AWS Libfabric/0
1: nid007117:210407:211082 [2] NCCL INFO Channel 07/0 : 14[2] -> 6[2] [receive] via NET/AWS Libfabric/2
2: nid007119:146136:146747 [0] NCCL INFO Channel 00/0 : 16[0] -> 8[0] [receive] via NET/AWS Libfabric/0
4: nid007121:116201:116800 [0] NCCL INFO Channel 00/0 : 0[0] -> 16[0] [receive] via NET/AWS Libfabric/0
2: nid007119:146136:146747 [0] NCCL INFO Channel 01/0 : 16[0] -> 8[0] [receive] via NET/AWS Libfabric/0
4: nid007121:116201:116800 [0] NCCL INFO Channel 01/0 : 0[0] -> 16[0] [receive] via NET/AWS Libfabric/0
4: nid007121:116201:116800 [0] NCCL INFO Channel 00/0 : 16[0] -> 0[0] [send] via NET/AWS Libfabric/0
4: nid007121:116201:116800 [0] NCCL INFO Channel 01/0 : 16[0] -> 0[0] [send] via NET/AWS Libfabric/0
5: nid007122:66177:66804 [2] NCCL INFO Channel 00/0 : 22[2] -> 23[3] via P2P/CUMEM
5: nid007122:66177:66804 [2] NCCL INFO Channel 02/0 : 22[2] -> 23[3] via P2P/CUMEM
5: nid007122:66177:66804 [2] NCCL INFO Channel 03/0 : 22[2] -> 23[3] via P2P/CUMEM
5: nid007122:66177:66804 [2] NCCL INFO Channel 04/0 : 22[2] -> 23[3] via P2P/CUMEM
5: nid007122:66177:66804 [2] NCCL INFO Channel 06/0 : 22[2] -> 23[3] via P2P/CUMEM
5: nid007122:66177:66804 [2] NCCL INFO Channel 07/0 : 22[2] -> 23[3] via P2P/CUMEM
5: nid007122:66178:66801 [3] NCCL INFO Channel 02/0 : 23[3] -> 20[0] via P2P/CUMEM
5: nid007122:66177:66804 [2] NCCL INFO Channel 06/0 : 18[2] -> 22[2] [receive] via NET/AWS Libfabric/2
5: nid007122:66178:66801 [3] NCCL INFO Channel 03/0 : 23[3] -> 20[0] via P2P/CUMEM
5: nid007122:66177:66804 [2] NCCL INFO Channel 03/0 : 22[2] -> 26[2] [send] via NET/AWS Libfabric/2
5: nid007122:66177:66804 [2] NCCL INFO Channel 07/0 : 22[2] -> 26[2] [send] via NET/AWS Libfabric/2
5: nid007122:66176:66803 [1] NCCL INFO Channel 01/0 : 21[1] -> 20[0] via P2P/CUMEM
5: nid007122:66178:66801 [3] NCCL INFO Channel 06/0 : 23[3] -> 20[0] via P2P/CUMEM
4: nid007121:116203:116798 [2] NCCL INFO Channel 02/0 : 10[2] -> 18[2] [receive] via NET/AWS Libfabric/2
4: nid007121:116203:116798 [2] NCCL INFO Channel 03/0 : 10[2] -> 18[2] [receive] via NET/AWS Libfabric/2
5: nid007122:66177:66804 [2] NCCL INFO Channel 06/0 : 14[2] -> 22[2] [receive] via NET/AWS Libfabric/2
6: nid007123:108291:108979 [2] NCCL INFO Channel 02/0 : 18[2] -> 26[2] [receive] via NET/AWS Libfabric/2
5: nid007122:66177:66804 [2] NCCL INFO Channel 07/0 : 14[2] -> 22[2] [receive] via NET/AWS Libfabric/2
4: nid007121:116203:116798 [2] NCCL INFO Channel 02/0 : 18[2] -> 26[2] [send] via NET/AWS Libfabric/2
6: nid007123:108291:108979 [2] NCCL INFO Channel 03/0 : 18[2] -> 26[2] [receive] via NET/AWS Libfabric/2
4: nid007121:116203:116798 [2] NCCL INFO Channel 03/0 : 18[2] -> 26[2] [send] via NET/AWS Libfabric/2
5: nid007122:66178:66801 [3] NCCL INFO Channel 07/0 : 23[3] -> 20[0] via P2P/CUMEM
5: nid007122:66176:66803 [1] NCCL INFO Channel 03/0 : 21[1] -> 20[0] via P2P/CUMEM
5: nid007122:66177:66804 [2] NCCL INFO Channel 06/0 : 22[2] -> 14[2] [send] via NET/AWS Libfabric/2
5: nid007122:66177:66804 [2] NCCL INFO Channel 07/0 : 22[2] -> 14[2] [send] via NET/AWS Libfabric/2
3: nid007120:197038:197636 [2] NCCL INFO Channel 06/0 : 30[2] -> 14[2] [receive] via NET/AWS Libfabric/2
3: nid007120:197038:197636 [2] NCCL INFO Channel 07/0 : 30[2] -> 14[2] [receive] via NET/AWS Libfabric/2
2: nid007119:146138:146748 [2] NCCL INFO Channel 02/0 : 18[2] -> 10[2] [receive] via NET/AWS Libfabric/2
4: nid007121:116203:116798 [2] NCCL INFO Channel 02/0 : 2[2] -> 18[2] [receive] via NET/AWS Libfabric/2
6: nid007123:108291:108979 [2] NCCL INFO Channel 02/0 : 26[2] -> 18[2] [send] via NET/AWS Libfabric/2
3: nid007120:197038:197636 [2] NCCL INFO Channel 06/0 : 14[2] -> 30[2] [send] via NET/AWS Libfabric/2
2: nid007119:146138:146748 [2] NCCL INFO Channel 03/0 : 18[2] -> 10[2] [receive] via NET/AWS Libfabric/2
4: nid007121:116203:116798 [2] NCCL INFO Channel 03/0 : 2[2] -> 18[2] [receive] via NET/AWS Libfabric/2
3: nid007120:197038:197636 [2] NCCL INFO Channel 07/0 : 14[2] -> 30[2] [send] via NET/AWS Libfabric/2
6: nid007123:108291:108979 [2] NCCL INFO Channel 03/0 : 26[2] -> 18[2] [send] via NET/AWS Libfabric/2
5: nid007122:66176:66803 [1] NCCL INFO Channel 05/0 : 21[1] -> 20[0] via P2P/CUMEM
4: nid007121:116203:116798 [2] NCCL INFO Channel 02/0 : 18[2] -> 2[2] [send] via NET/AWS Libfabric/2
4: nid007121:116203:116798 [2] NCCL INFO Channel 03/0 : 18[2] -> 2[2] [send] via NET/AWS Libfabric/2
5: nid007122:66176:66803 [1] NCCL INFO Channel 07/0 : 21[1] -> 20[0] via P2P/CUMEM
3: nid007120:197038:197636 [2] NCCL INFO Channel 06/0 : 22[2] -> 14[2] [receive] via NET/AWS Libfabric/2
3: nid007120:197038:197636 [2] NCCL INFO Channel 07/0 : 22[2] -> 14[2] [receive] via NET/AWS Libfabric/2
7: nid007124:126397:126989 [2] NCCL INFO Channel 02/0 : 30[2] -> 26[2] [send] via NET/AWS Libfabric/2
7: nid007124:126397:126989 [2] NCCL INFO Channel 03/0 : 30[2] -> 26[2] [send] via NET/AWS Libfabric/2
4: nid007121:116203:116798 [2] NCCL INFO Channel 02/0 : 26[2] -> 18[2] [receive] via NET/AWS Libfabric/2
3: nid007120:197038:197636 [2] NCCL INFO Channel 06/0 : 14[2] -> 6[2] [send] via NET/AWS Libfabric/2
0: nid007116:202902:203491 [2] NCCL INFO Channel 06/0 : 6[2] -> 2[2] [receive] via NET/AWS Libfabric/2
4: nid007121:116203:116798 [2] NCCL INFO Channel 03/0 : 26[2] -> 18[2] [receive] via NET/AWS Libfabric/2
3: nid007120:197038:197636 [2] NCCL INFO Channel 07/0 : 14[2] -> 6[2] [send] via NET/AWS Libfabric/2
4: nid007121:116203:116798 [2] NCCL INFO Channel 02/0 : 18[2] -> 10[2] [send] via NET/AWS Libfabric/2
0: nid007116:202902:203491 [2] NCCL INFO Channel 07/0 : 6[2] -> 2[2] [receive] via NET/AWS Libfabric/2
4: nid007121:116203:116798 [2] NCCL INFO Channel 03/0 : 18[2] -> 10[2] [send] via NET/AWS Libfabric/2
5: nid007122:66177:66804 [2] NCCL INFO Channel 02/0 : 26[2] -> 22[2] [receive] via NET/AWS Libfabric/2
5: nid007122:66177:66804 [2] NCCL INFO Channel 03/0 : 26[2] -> 22[2] [receive] via NET/AWS Libfabric/2
3: nid007120:197038:197636 [2] NCCL INFO Channel 02/0 : 14[2] -> 10[2] [send] via NET/AWS Libfabric/2
5: nid007122:66177:66804 [2] NCCL INFO Channel 06/0 : 26[2] -> 22[2] [receive] via NET/AWS Libfabric/2
6: nid007123:108291:108979 [2] NCCL INFO Channel 02/0 : 30[2] -> 26[2] [receive] via NET/AWS Libfabric/2
3: nid007120:197038:197636 [2] NCCL INFO Channel 03/0 : 14[2] -> 10[2] [send] via NET/AWS Libfabric/2
1: nid007117:210407:211082 [2] NCCL INFO Channel 02/0 : 10[2] -> 6[2] [receive] via NET/AWS Libfabric/2
5: nid007122:66177:66804 [2] NCCL INFO Channel 07/0 : 26[2] -> 22[2] [receive] via NET/AWS Libfabric/2
6: nid007123:108291:108979 [2] NCCL INFO Channel 03/0 : 30[2] -> 26[2] [receive] via NET/AWS Libfabric/2
1: nid007117:210407:211082 [2] NCCL INFO Channel 03/0 : 10[2] -> 6[2] [receive] via NET/AWS Libfabric/2
4: nid007121:116203:116798 [2] NCCL INFO Channel 06/0 : 22[2] -> 18[2] [receive] via NET/AWS Libfabric/2
5: nid007122:66177:66804 [2] NCCL INFO Channel 06/0 : 22[2] -> 18[2] [send] via NET/AWS Libfabric/2
6: nid007123:108291:108979 [2] NCCL INFO Channel 02/0 : 26[2] -> 22[2] [send] via NET/AWS Libfabric/2
2: nid007119:146138:146748 [2] NCCL INFO Channel 02/0 : 14[2] -> 10[2] [receive] via NET/AWS Libfabric/2
4: nid007121:116203:116798 [2] NCCL INFO Channel 07/0 : 22[2] -> 18[2] [receive] via NET/AWS Libfabric/2
1: nid007117:210407:211082 [2] NCCL INFO Channel 06/0 : 10[2] -> 6[2] [receive] via NET/AWS Libfabric/2
5: nid007122:66177:66804 [2] NCCL INFO Channel 07/0 : 22[2] -> 18[2] [send] via NET/AWS Libfabric/2
2: nid007119:146138:146748 [2] NCCL INFO Channel 03/0 : 14[2] -> 10[2] [receive] via NET/AWS Libfabric/2
6: nid007123:108291:108979 [2] NCCL INFO Channel 03/0 : 26[2] -> 22[2] [send] via NET/AWS Libfabric/2
1: nid007117:210407:211082 [2] NCCL INFO Channel 07/0 : 10[2] -> 6[2] [receive] via NET/AWS Libfabric/2
6: nid007123:108291:108979 [2] NCCL INFO Channel 06/0 : 26[2] -> 22[2] [send] via NET/AWS Libfabric/2
2: nid007119:146138:146748 [2] NCCL INFO Channel 02/0 : 10[2] -> 6[2] [send] via NET/AWS Libfabric/2
1: nid007117:210407:211082 [2] NCCL INFO Channel 06/0 : 6[2] -> 2[2] [send] via NET/AWS Libfabric/2
2: nid007119:146138:146748 [2] NCCL INFO Channel 03/0 : 10[2] -> 6[2] [send] via NET/AWS Libfabric/2
6: nid007123:108291:108979 [2] NCCL INFO Channel 07/0 : 26[2] -> 22[2] [send] via NET/AWS Libfabric/2
1: nid007117:210407:211082 [2] NCCL INFO Channel 07/0 : 6[2] -> 2[2] [send] via NET/AWS Libfabric/2
2: nid007119:146138:146748 [2] NCCL INFO Channel 06/0 : 10[2] -> 6[2] [send] via NET/AWS Libfabric/2
2: nid007119:146138:146748 [2] NCCL INFO Channel 07/0 : 10[2] -> 6[2] [send] via NET/AWS Libfabric/2
4: nid007121:116203:116798 [2] NCCL INFO Channel 00/0 : 18[2] -> 17[1] via P2P/CUMEM
7: nid007124:126397:126989 [2] NCCL INFO Channel 01/0 : 30[2] -> 29[1] via P2P/CUMEM
4: nid007121:116203:116798 [2] NCCL INFO Channel 04/0 : 18[2] -> 17[1] via P2P/CUMEM
5: nid007122:66177:66804 [2] NCCL INFO Channel 01/0 : 22[2] -> 21[1] via P2P/CUMEM
6: nid007123:108291:108979 [2] NCCL INFO Channel 00/0 : 26[2] -> 25[1] via P2P/CUMEM
3: nid007120:197038:197636 [2] NCCL INFO Channel 01/0 : 14[2] -> 13[1] via P2P/CUMEM
7: nid007124:126397:126989 [2] NCCL INFO Channel 05/0 : 30[2] -> 29[1] via P2P/CUMEM
0: nid007116:202902:203491 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM
5: nid007122:66177:66804 [2] NCCL INFO Channel 05/0 : 22[2] -> 21[1] via P2P/CUMEM
6: nid007123:108291:108979 [2] NCCL INFO Channel 04/0 : 26[2] -> 25[1] via P2P/CUMEM
2: nid007119:146138:146748 [2] NCCL INFO Channel 00/0 : 10[2] -> 9[1] via P2P/CUMEM
1: nid007117:210407:211082 [2] NCCL INFO Channel 01/0 : 6[2] -> 5[1] via P2P/CUMEM
3: nid007120:197038:197636 [2] NCCL INFO Channel 05/0 : 14[2] -> 13[1] via P2P/CUMEM
0: nid007116:202902:203491 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/CUMEM
2: nid007119:146138:146748 [2] NCCL INFO Channel 04/0 : 10[2] -> 9[1] via P2P/CUMEM
1: nid007117:210407:211082 [2] NCCL INFO Channel 05/0 : 6[2] -> 5[1] via P2P/CUMEM
0: [2025-07-11 11:48:10,249] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.9, git-hash=unknown, git-branch=unknown
0: [2025-07-11 11:48:10,249] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 32
0: [2025-07-11 11:48:10,262] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
0: [2025-07-11 11:48:10,263] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer
0: [2025-07-11 11:48:10,263] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
0: [2025-07-11 11:48:10,298] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
0: [2025-07-11 11:48:10,298] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
0: [2025-07-11 11:48:10,298] [INFO] [logging.py:107:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
0: [2025-07-11 11:48:10,298] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
0: [2025-07-11 11:48:10,490] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
0: [2025-07-11 11:48:10,491] [INFO] [utils.py:782:see_memory_usage] MA 0.48 GB         Max_MA 3.5 GB         CA 0.5 GB         Max_CA 4 GB 
0: [2025-07-11 11:48:10,491] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 107.83 GB, percent = 12.6%
0: [2025-07-11 11:48:10,494] [INFO] [stage3.py:170:__init__] Reduce bucket size 12845056
0: [2025-07-11 11:48:10,494] [INFO] [stage3.py:171:__init__] Prefetch bucket size 11560550
0: [2025-07-11 11:48:10,681] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
0: [2025-07-11 11:48:10,682] [INFO] [utils.py:782:see_memory_usage] MA 0.48 GB         Max_MA 0.48 GB         CA 0.5 GB         Max_CA 0 GB 
0: [2025-07-11 11:48:10,682] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 107.83 GB, percent = 12.6%
0: Parameter Offload: Total persistent parameters: 848896 in 368 params
0: [2025-07-11 11:48:10,942] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
0: [2025-07-11 11:48:10,943] [INFO] [utils.py:782:see_memory_usage] MA 0.48 GB         Max_MA 0.48 GB         CA 0.5 GB         Max_CA 0 GB 
0: [2025-07-11 11:48:10,943] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 107.84 GB, percent = 12.6%
0: [2025-07-11 11:48:11,144] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
0: [2025-07-11 11:48:11,144] [INFO] [utils.py:782:see_memory_usage] MA 0.48 GB         Max_MA 0.48 GB         CA 0.5 GB         Max_CA 0 GB 
0: [2025-07-11 11:48:11,145] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 107.84 GB, percent = 12.6%
0: nid007116:202900:203496 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM
0: nid007116:202900:203496 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM
0: nid007116:202900:203496 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM
0: nid007116:202900:203496 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/CUMEM
0: nid007116:202900:203496 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM
0: nid007116:202900:203496 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM
0: nid007116:202900:203496 [0] NCCL INFO Channel 04/0 : 0[0] -> 4[0] [send] via NET/AWS Libfabric/0
0: nid007116:202900:203496 [0] NCCL INFO Channel 00/0 : 16[0] -> 0[0] [receive] via NET/AWS Libfabric/0
0: nid007116:202900:203496 [0] NCCL INFO Channel 01/0 : 16[0] -> 0[0] [receive] via NET/AWS Libfabric/0
0: nid007116:202901:203492 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM
1: nid007117:210405:211083 [0] NCCL INFO Channel 04/0 : 4[0] -> 12[0] [send] via NET/AWS Libfabric/0
0: nid007116:202900:203496 [0] NCCL INFO Channel 00/0 : 0[0] -> 16[0] [send] via NET/AWS Libfabric/0
1: nid007117:210405:211083 [0] NCCL INFO Channel 05/0 : 4[0] -> 12[0] [send] via NET/AWS Libfabric/0
0: nid007116:202900:203496 [0] NCCL INFO Channel 01/0 : 0[0] -> 16[0] [send] via NET/AWS Libfabric/0
0: nid007116:202901:203492 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM
1: nid007117:210405:211083 [0] NCCL INFO Channel 04/0 : 12[0] -> 4[0] [receive] via NET/AWS Libfabric/0
0: nid007116:202901:203492 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/CUMEM
0: nid007116:202900:203496 [0] NCCL INFO Channel 04/0 : 4[0] -> 0[0] [receive] via NET/AWS Libfabric/0
3: nid007120:197036:197635 [0] NCCL INFO Channel 04/0 : 28[0] -> 12[0] [receive] via NET/AWS Libfabric/0
1: nid007117:210405:211083 [0] NCCL INFO Channel 05/0 : 12[0] -> 4[0] [receive] via NET/AWS Libfabric/0
0: nid007116:202900:203496 [0] NCCL INFO Channel 05/0 : 4[0] -> 0[0] [receive] via NET/AWS Libfabric/0
0: nid007116:202901:203492 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/CUMEM
4: nid007121:116201:116800 [0] NCCL INFO Channel 00/0 : 24[0] -> 16[0] [receive] via NET/AWS Libfabric/0
3: nid007120:197036:197635 [0] NCCL INFO Channel 05/0 : 28[0] -> 12[0] [receive] via NET/AWS Libfabric/0
4: nid007121:116201:116800 [0] NCCL INFO Channel 01/0 : 24[0] -> 16[0] [receive] via NET/AWS Libfabric/0
3: nid007120:197036:197635 [0] NCCL INFO Channel 04/0 : 12[0] -> 28[0] [send] via NET/AWS Libfabric/0
4: nid007121:116201:116800 [0] NCCL INFO Channel 00/0 : 16[0] -> 8[0] [send] via NET/AWS Libfabric/0
3: nid007120:197036:197635 [0] NCCL INFO Channel 05/0 : 12[0] -> 28[0] [send] via NET/AWS Libfabric/0
4: nid007121:116201:116800 [0] NCCL INFO Channel 01/0 : 16[0] -> 8[0] [send] via NET/AWS Libfabric/0
3: nid007120:197036:197635 [0] NCCL INFO Channel 04/0 : 20[0] -> 12[0] [receive] via NET/AWS Libfabric/0
7: nid007124:126395:126988 [0] NCCL INFO Channel 00/0 : 28[0] -> 24[0] [send] via NET/AWS Libfabric/0
3: nid007120:197036:197635 [0] NCCL INFO Channel 05/0 : 20[0] -> 12[0] [receive] via NET/AWS Libfabric/0
6: nid007123:108289:108981 [0] NCCL INFO Channel 00/0 : 28[0] -> 24[0] [receive] via NET/AWS Libfabric/0
4: nid007121:116201:116800 [0] NCCL INFO Channel 04/0 : 20[0] -> 16[0] [receive] via NET/AWS Libfabric/0
3: nid007120:197036:197635 [0] NCCL INFO Channel 04/0 : 12[0] -> 4[0] [send] via NET/AWS Libfabric/0
7: nid007124:126395:126988 [0] NCCL INFO Channel 01/0 : 28[0] -> 24[0] [send] via NET/AWS Libfabric/0
6: nid007123:108289:108981 [0] NCCL INFO Channel 01/0 : 28[0] -> 24[0] [receive] via NET/AWS Libfabric/0
4: nid007121:116201:116800 [0] NCCL INFO Channel 05/0 : 20[0] -> 16[0] [receive] via NET/AWS Libfabric/0
3: nid007120:197036:197635 [0] NCCL INFO Channel 05/0 : 12[0] -> 4[0] [send] via NET/AWS Libfabric/0
2: nid007119:146136:146747 [0] NCCL INFO Channel 00/0 : 12[0] -> 8[0] [receive] via NET/AWS Libfabric/0
6: nid007123:108289:108981 [0] NCCL INFO Channel 00/0 : 24[0] -> 20[0] [send] via NET/AWS Libfabric/0
2: nid007119:146136:146747 [0] NCCL INFO Channel 01/0 : 12[0] -> 8[0] [receive] via NET/AWS Libfabric/0
6: nid007123:108289:108981 [0] NCCL INFO Channel 01/0 : 24[0] -> 20[0] [send] via NET/AWS Libfabric/0
2: nid007119:146136:146747 [0] NCCL INFO Channel 00/0 : 8[0] -> 4[0] [send] via NET/AWS Libfabric/0
6: nid007123:108289:108981 [0] NCCL INFO Channel 04/0 : 24[0] -> 20[0] [send] via NET/AWS Libfabric/0
2: nid007119:146136:146747 [0] NCCL INFO Channel 01/0 : 8[0] -> 4[0] [send] via NET/AWS Libfabric/0
6: nid007123:108289:108981 [0] NCCL INFO Channel 05/0 : 24[0] -> 20[0] [send] via NET/AWS Libfabric/0
3: nid007120:197036:197635 [0] NCCL INFO Channel 00/0 : 12[0] -> 8[0] [send] via NET/AWS Libfabric/0
2: nid007119:146136:146747 [0] NCCL INFO Channel 04/0 : 8[0] -> 4[0] [send] via NET/AWS Libfabric/0
1: nid007117:210405:211083 [0] NCCL INFO Channel 00/0 : 8[0] -> 4[0] [receive] via NET/AWS Libfabric/0
3: nid007120:197036:197635 [0] NCCL INFO Channel 01/0 : 12[0] -> 8[0] [send] via NET/AWS Libfabric/0
5: nid007122:66175:66802 [0] NCCL INFO Channel 00/0 : 24[0] -> 20[0] [receive] via NET/AWS Libfabric/0
5: nid007122:66175:66802 [0] NCCL INFO Channel 01/0 : 24[0] -> 20[0] [receive] via NET/AWS Libfabric/0
2: nid007119:146136:146747 [0] NCCL INFO Channel 05/0 : 8[0] -> 4[0] [send] via NET/AWS Libfabric/0
1: nid007117:210405:211083 [0] NCCL INFO Channel 01/0 : 8[0] -> 4[0] [receive] via NET/AWS Libfabric/0
5: nid007122:66175:66802 [0] NCCL INFO Channel 04/0 : 24[0] -> 20[0] [receive] via NET/AWS Libfabric/0
1: nid007117:210405:211083 [0] NCCL INFO Channel 04/0 : 8[0] -> 4[0] [receive] via NET/AWS Libfabric/0
5: nid007122:66175:66802 [0] NCCL INFO Channel 05/0 : 24[0] -> 20[0] [receive] via NET/AWS Libfabric/0
1: nid007117:210405:211083 [0] NCCL INFO Channel 05/0 : 8[0] -> 4[0] [receive] via NET/AWS Libfabric/0
5: nid007122:66175:66802 [0] NCCL INFO Channel 04/0 : 20[0] -> 16[0] [send] via NET/AWS Libfabric/0
1: nid007117:210405:211083 [0] NCCL INFO Channel 04/0 : 4[0] -> 0[0] [send] via NET/AWS Libfabric/0
5: nid007122:66175:66802 [0] NCCL INFO Channel 05/0 : 20[0] -> 16[0] [send] via NET/AWS Libfabric/0
1: nid007117:210405:211083 [0] NCCL INFO Channel 05/0 : 4[0] -> 0[0] [send] via NET/AWS Libfabric/0
7: nid007124:126398:126987 [3] NCCL INFO Channel 01/0 : 31[3] -> 30[2] via P2P/CUMEM
3: nid007120:197039:197634 [3] NCCL INFO Channel 01/0 : 15[3] -> 14[2] via P2P/CUMEM
6: nid007123:108292:108978 [3] NCCL INFO Channel 00/0 : 27[3] -> 26[2] via P2P/CUMEM
2: nid007119:146139:146745 [3] NCCL INFO Channel 00/0 : 11[3] -> 10[2] via P2P/CUMEM
0: nid007116:202903:203493 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM
4: nid007121:116204:116799 [3] NCCL INFO Channel 00/0 : 19[3] -> 18[2] via P2P/CUMEM
7: nid007124:126398:126987 [3] NCCL INFO Channel 03/0 : 31[3] -> 30[2] via P2P/CUMEM
3: nid007120:197039:197634 [3] NCCL INFO Channel 03/0 : 15[3] -> 14[2] via P2P/CUMEM
5: nid007122:66178:66801 [3] NCCL INFO Channel 01/0 : 23[3] -> 22[2] via P2P/CUMEM
1: nid007117:210408:211084 [3] NCCL INFO Channel 01/0 : 7[3] -> 6[2] via P2P/CUMEM
6: nid007123:108292:108978 [3] NCCL INFO Channel 02/0 : 27[3] -> 26[2] via P2P/CUMEM
2: nid007119:146139:146745 [3] NCCL INFO Channel 02/0 : 11[3] -> 10[2] via P2P/CUMEM
7: nid007124:126398:126987 [3] NCCL INFO Channel 05/0 : 31[3] -> 30[2] via P2P/CUMEM
3: nid007120:197039:197634 [3] NCCL INFO Channel 05/0 : 15[3] -> 14[2] via P2P/CUMEM
0: nid007116:202903:203493 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM
4: nid007121:116204:116799 [3] NCCL INFO Channel 02/0 : 19[3] -> 18[2] via P2P/CUMEM
6: nid007123:108292:108978 [3] NCCL INFO Channel 04/0 : 27[3] -> 26[2] via P2P/CUMEM
7: nid007124:126398:126987 [3] NCCL INFO Channel 07/0 : 31[3] -> 30[2] via P2P/CUMEM
2: nid007119:146139:146745 [3] NCCL INFO Channel 04/0 : 11[3] -> 10[2] via P2P/CUMEM
5: nid007122:66178:66801 [3] NCCL INFO Channel 03/0 : 23[3] -> 22[2] via P2P/CUMEM
3: nid007120:197039:197634 [3] NCCL INFO Channel 07/0 : 15[3] -> 14[2] via P2P/CUMEM
1: nid007117:210408:211084 [3] NCCL INFO Channel 03/0 : 7[3] -> 6[2] via P2P/CUMEM
6: nid007123:108292:108978 [3] NCCL INFO Channel 06/0 : 27[3] -> 26[2] via P2P/CUMEM
0: nid007116:202903:203493 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/CUMEM
4: nid007121:116204:116799 [3] NCCL INFO Channel 04/0 : 19[3] -> 18[2] via P2P/CUMEM
2: nid007119:146139:146745 [3] NCCL INFO Channel 06/0 : 11[3] -> 10[2] via P2P/CUMEM
5: nid007122:66178:66801 [3] NCCL INFO Channel 05/0 : 23[3] -> 22[2] via P2P/CUMEM
1: nid007117:210408:211084 [3] NCCL INFO Channel 05/0 : 7[3] -> 6[2] via P2P/CUMEM
0: nid007116:202903:203493 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/CUMEM
4: nid007121:116204:116799 [3] NCCL INFO Channel 06/0 : 19[3] -> 18[2] via P2P/CUMEM
5: nid007122:66178:66801 [3] NCCL INFO Channel 07/0 : 23[3] -> 22[2] via P2P/CUMEM
1: nid007117:210408:211084 [3] NCCL INFO Channel 07/0 : 7[3] -> 6[2] via P2P/CUMEM
1: nid007117:210405:211083 [0] NCCL INFO Connected all trees
2: nid007119:146136:146747 [0] NCCL INFO Connected all trees
4: nid007121:116201:116800 [0] NCCL INFO Connected all trees
5: nid007122:66175:66802 [0] NCCL INFO Connected all trees
6: nid007123:108289:108981 [0] NCCL INFO Connected all trees
3: nid007120:197036:197635 [0] NCCL INFO Connected all trees
0: nid007116:202900:203496 [0] NCCL INFO Connected all trees
4: nid007121:116202:116801 [1] NCCL INFO Connected all trees
5: nid007122:66176:66803 [1] NCCL INFO Connected all trees
4: nid007121:116204:116799 [3] NCCL INFO Connected all trees
5: nid007122:66178:66801 [3] NCCL INFO Connected all trees
4: nid007121:116203:116798 [2] NCCL INFO Connected all trees
6: nid007123:108290:108980 [1] NCCL INFO Connected all trees
5: nid007122:66177:66804 [2] NCCL INFO Connected all trees
7: nid007124:126398:126987 [3] NCCL INFO Connected all trees
6: nid007123:108292:108978 [3] NCCL INFO Connected all trees
6: nid007123:108291:108979 [2] NCCL INFO Connected all trees
0: nid007116:202901:203492 [1] NCCL INFO Connected all trees
3: nid007120:197037:197633 [1] NCCL INFO Connected all trees
0: nid007116:202903:203493 [3] NCCL INFO Connected all trees
2: nid007119:146137:146746 [1] NCCL INFO Connected all trees
3: nid007120:197039:197634 [3] NCCL INFO Connected all trees
0: nid007116:202902:203491 [2] NCCL INFO Connected all trees
3: nid007120:197038:197636 [2] NCCL INFO Connected all trees
1: nid007117:210407:211082 [2] NCCL INFO Connected all trees
2: nid007119:146139:146745 [3] NCCL INFO Connected all trees
2: nid007119:146138:146748 [2] NCCL INFO Connected all trees
1: nid007117:210408:211084 [3] NCCL INFO Connected all trees
1: nid007117:210406:211085 [1] NCCL INFO Connected all trees
7: nid007124:126395:126988 [0] NCCL INFO Connected all trees
7: nid007124:126397:126989 [2] NCCL INFO Connected all trees
7: nid007124:126396:126990 [1] NCCL INFO Connected all trees
0: [2025-07-11 11:48:14,780] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 2
0: [2025-07-11 11:48:14,781] [INFO] [utils.py:782:see_memory_usage] MA 0.48 GB         Max_MA 0.48 GB         CA 0.52 GB         Max_CA 1 GB 
0: [2025-07-11 11:48:14,781] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 112.68 GB, percent = 13.2%
0: [2025-07-11 11:48:14,984] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
0: [2025-07-11 11:48:14,985] [INFO] [utils.py:782:see_memory_usage] MA 0.48 GB         Max_MA 0.48 GB         CA 0.52 GB         Max_CA 1 GB 
0: [2025-07-11 11:48:14,985] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 112.68 GB, percent = 13.2%
0: [2025-07-11 11:48:15,198] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
0: [2025-07-11 11:48:15,199] [INFO] [utils.py:782:see_memory_usage] MA 1.45 GB         Max_MA 1.93 GB         CA 1.96 GB         Max_CA 2 GB 
0: [2025-07-11 11:48:15,199] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 114.12 GB, percent = 13.4%
0: [2025-07-11 11:48:15,400] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
0: [2025-07-11 11:48:15,401] [INFO] [utils.py:782:see_memory_usage] MA 1.45 GB         Max_MA 1.45 GB         CA 1.96 GB         Max_CA 2 GB 
0: [2025-07-11 11:48:15,401] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 117.06 GB, percent = 13.7%
0: [2025-07-11 11:48:15,611] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
0: [2025-07-11 11:48:15,612] [INFO] [utils.py:782:see_memory_usage] MA 1.45 GB         Max_MA 2.41 GB         CA 2.94 GB         Max_CA 3 GB 
0: [2025-07-11 11:48:15,612] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 118.03 GB, percent = 13.8%
0: [2025-07-11 11:48:15,613] [INFO] [stage3.py:534:_setup_for_real_optimizer] optimizer state initialized
2: [INFO|trainer.py:2414] 2025-07-11 11:48:16,310 >> ***** Running training *****
2: [INFO|trainer.py:2415] 2025-07-11 11:48:16,310 >>   Num examples = 71,172
2: [INFO|trainer.py:2416] 2025-07-11 11:48:16,310 >>   Num Epochs = 1
2: [INFO|trainer.py:2417] 2025-07-11 11:48:16,310 >>   Instantaneous batch size per device = 1
2: [INFO|trainer.py:2420] 2025-07-11 11:48:16,310 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
2: [INFO|trainer.py:2421] 2025-07-11 11:48:16,310 >>   Gradient Accumulation steps = 2
2: [INFO|trainer.py:2422] 2025-07-11 11:48:16,310 >>   Total optimization steps = 1,112
3: [INFO|trainer.py:2414] 2025-07-11 11:48:16,310 >> ***** Running training *****
3: [INFO|trainer.py:2415] 2025-07-11 11:48:16,310 >>   Num examples = 71,172
3: [INFO|trainer.py:2416] 2025-07-11 11:48:16,310 >>   Num Epochs = 1
3: [INFO|trainer.py:2417] 2025-07-11 11:48:16,310 >>   Instantaneous batch size per device = 1
3: [INFO|trainer.py:2420] 2025-07-11 11:48:16,310 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
5: [INFO|trainer.py:2414] 2025-07-11 11:48:16,310 >> ***** Running training *****
5: [INFO|trainer.py:2415] 2025-07-11 11:48:16,310 >>   Num examples = 71,172
5: [INFO|trainer.py:2416] 2025-07-11 11:48:16,310 >>   Num Epochs = 1
5: [INFO|trainer.py:2417] 2025-07-11 11:48:16,310 >>   Instantaneous batch size per device = 1
3: [INFO|trainer.py:2421] 2025-07-11 11:48:16,310 >>   Gradient Accumulation steps = 2
5: [INFO|trainer.py:2420] 2025-07-11 11:48:16,310 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
3: [INFO|trainer.py:2422] 2025-07-11 11:48:16,310 >>   Total optimization steps = 1,112
5: [INFO|trainer.py:2421] 2025-07-11 11:48:16,310 >>   Gradient Accumulation steps = 2
5: [INFO|trainer.py:2422] 2025-07-11 11:48:16,310 >>   Total optimization steps = 1,112
6: [INFO|trainer.py:2414] 2025-07-11 11:48:16,310 >> ***** Running training *****
6: [INFO|trainer.py:2415] 2025-07-11 11:48:16,310 >>   Num examples = 71,172
6: [INFO|trainer.py:2416] 2025-07-11 11:48:16,310 >>   Num Epochs = 1
6: [INFO|trainer.py:2417] 2025-07-11 11:48:16,310 >>   Instantaneous batch size per device = 1
6: [INFO|trainer.py:2420] 2025-07-11 11:48:16,310 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
6: [INFO|trainer.py:2421] 2025-07-11 11:48:16,310 >>   Gradient Accumulation steps = 2
6: [INFO|trainer.py:2422] 2025-07-11 11:48:16,310 >>   Total optimization steps = 1,112
2: [INFO|trainer.py:2423] 2025-07-11 11:48:16,311 >>   Number of trainable parameters = 8,292,166,656
4: [INFO|trainer.py:2414] 2025-07-11 11:48:16,311 >> ***** Running training *****
4: [INFO|trainer.py:2415] 2025-07-11 11:48:16,311 >>   Num examples = 71,172
4: [INFO|trainer.py:2416] 2025-07-11 11:48:16,311 >>   Num Epochs = 1
4: [INFO|trainer.py:2417] 2025-07-11 11:48:16,311 >>   Instantaneous batch size per device = 1
4: [INFO|trainer.py:2420] 2025-07-11 11:48:16,311 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
4: [INFO|trainer.py:2421] 2025-07-11 11:48:16,311 >>   Gradient Accumulation steps = 2
4: [INFO|trainer.py:2422] 2025-07-11 11:48:16,311 >>   Total optimization steps = 1,112
3: [INFO|trainer.py:2423] 2025-07-11 11:48:16,311 >>   Number of trainable parameters = 8,292,166,656
5: [INFO|trainer.py:2423] 2025-07-11 11:48:16,311 >>   Number of trainable parameters = 8,292,166,656
6: [INFO|trainer.py:2423] 2025-07-11 11:48:16,312 >>   Number of trainable parameters = 8,292,166,656
4: [INFO|trainer.py:2423] 2025-07-11 11:48:16,312 >>   Number of trainable parameters = 8,292,166,656
1: [INFO|trainer.py:2414] 2025-07-11 11:48:16,321 >> ***** Running training *****
1: [INFO|trainer.py:2415] 2025-07-11 11:48:16,321 >>   Num examples = 71,172
1: [INFO|trainer.py:2416] 2025-07-11 11:48:16,321 >>   Num Epochs = 1
1: [INFO|trainer.py:2417] 2025-07-11 11:48:16,321 >>   Instantaneous batch size per device = 1
1: [INFO|trainer.py:2420] 2025-07-11 11:48:16,321 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
1: [INFO|trainer.py:2421] 2025-07-11 11:48:16,321 >>   Gradient Accumulation steps = 2
1: [INFO|trainer.py:2422] 2025-07-11 11:48:16,321 >>   Total optimization steps = 1,112
1: [INFO|trainer.py:2423] 2025-07-11 11:48:16,323 >>   Number of trainable parameters = 8,292,166,656
7: [INFO|trainer.py:2414] 2025-07-11 11:48:16,334 >> ***** Running training *****
7: [INFO|trainer.py:2415] 2025-07-11 11:48:16,334 >>   Num examples = 71,172
7: [INFO|trainer.py:2416] 2025-07-11 11:48:16,334 >>   Num Epochs = 1
7: [INFO|trainer.py:2417] 2025-07-11 11:48:16,334 >>   Instantaneous batch size per device = 1
7: [INFO|trainer.py:2420] 2025-07-11 11:48:16,334 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
7: [INFO|trainer.py:2421] 2025-07-11 11:48:16,334 >>   Gradient Accumulation steps = 2
7: [INFO|trainer.py:2422] 2025-07-11 11:48:16,334 >>   Total optimization steps = 1,112
7: [INFO|trainer.py:2423] 2025-07-11 11:48:16,335 >>   Number of trainable parameters = 8,292,166,656
0: [2025-07-11 11:48:16,533] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
0: [2025-07-11 11:48:16,534] [INFO] [utils.py:782:see_memory_usage] MA 1.95 GB         Max_MA 3.98 GB         CA 4.48 GB         Max_CA 4 GB 
0: [2025-07-11 11:48:16,534] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 125.32 GB, percent = 14.7%
0: [2025-07-11 11:48:16,534] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
0: [2025-07-11 11:48:16,534] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
0: [2025-07-11 11:48:16,534] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
0: [2025-07-11 11:48:16,535] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]
0: [2025-07-11 11:48:16,536] [INFO] [config.py:1003:print] DeepSpeedEngine configuration:
0: [2025-07-11 11:48:16,536] [INFO] [config.py:1007:print]   activation_checkpointing_config  {
0:     "partition_activations": false, 
0:     "contiguous_memory_optimization": false, 
0:     "cpu_checkpointing": false, 
0:     "number_checkpoints": null, 
0:     "synchronize_checkpoint_boundary": false, 
0:     "profile": false
0: }
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   amp_enabled .................. False
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   amp_params ................... False
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   autotuning_config ............ {
0:     "enabled": false, 
0:     "start_step": null, 
0:     "end_step": null, 
0:     "metric_path": null, 
0:     "arg_mappings": null, 
0:     "metric": "throughput", 
0:     "model_info": null, 
0:     "results_dir": "autotuning_results", 
0:     "exps_dir": "autotuning_exps", 
0:     "overwrite": true, 
0:     "fast": true, 
0:     "start_profile_step": 3, 
0:     "end_profile_step": 5, 
0:     "tuner_type": "gridsearch", 
0:     "tuner_early_stopping": 5, 
0:     "tuner_num_trials": 50, 
0:     "model_info_path": null, 
0:     "mp_size": 1, 
0:     "max_train_batch_size": null, 
0:     "min_train_batch_size": 1, 
0:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
0:     "min_train_micro_batch_size_per_gpu": 1, 
0:     "num_tuning_micro_batch_sizes": 3
0: }
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   bfloat16_enabled ............. True
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   bfloat16_immediate_grad_update  True
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   checkpoint_parallel_write_pipeline  False
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   checkpoint_tag_validation_enabled  True
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   checkpoint_tag_validation_fail  False
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x400372ad8fd0>
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   communication_data_type ...... None
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_pa
0: rameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   curriculum_enabled_legacy .... False
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   curriculum_params_legacy ..... False
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   data_efficiency_enabled ...... False
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   dataloader_drop_last ......... False
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   disable_allgather ............ False
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   dump_state ................... False
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   dynamic_loss_scale_args ...... None
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   eigenvalue_enabled ........... False
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   eigenvalue_gas_boundary_resolution  1
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   eigenvalue_layer_name ........ bert.encoder.layer
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   eigenvalue_layer_num ......... 0
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   eigenvalue_max_iter .......... 100
0: [2025-07-11 11:48:16,537] [INFO] [config.py:1007:print]   eigenvalue_stability ......... 1e-06
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   eigenvalue_tol ............... 0.01
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   eigenvalue_verbose ........... False
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   elasticity_enabled ........... False
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   flops_profiler_config ........ {
0:     "enabled": false, 
0:     "recompute_fwd_factor": 0.0, 
0:     "profile_step": 1, 
0:     "module_depth": -1, 
0:     "top_modules": 1, 
0:     "detailed": true, 
0:     "output_file": null
0: }
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   fp16_auto_cast ............... None
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   fp16_enabled ................. False
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   fp16_master_weights_and_gradients  False
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   global_rank .................. 0
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   grad_accum_dtype ............. None
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   gradient_accumulation_steps .. 2
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   gradient_clipping ............ 1.0
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   gradient_predivide_factor .... 1.0
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   graph_harvesting ............. False
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   initial_dynamic_scale ........ 1
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   load_universal_checkpoint .... False
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   loss_scale ................... 1.0
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   memory_breakdown ............. False
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   mics_hierarchial_params_gather  False
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   mics_shard_size .............. -1
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   nebula_config ................ {
0:     "enabled": false, 
0:     "persistent_storage_path": null, 
0:     "persistent_time_interval": 100, 
0:     "num_of_version_in_retention": 2, 
0:     "enable_nebula_load": true, 
0:     "load_path": null
0: }
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   optimizer_legacy_fusion ...... False
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   optimizer_name ............... None
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   optimizer_params ............. None
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   pld_enabled .................. False
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   pld_params ................... False
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   prescale_gradients ........... False
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   scheduler_name ............... None
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   scheduler_params ............. None
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   seq_parallel_communication_data_type  torch.float32
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   sparse_attention ............. None
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   sparse_gradients_enabled ..... False
0: [2025-07-11 11:48:16,538] [INFO] [config.py:1007:print]   steps_per_print .............. inf
0: [2025-07-11 11:48:16,539] [INFO] [config.py:1007:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
0: [2025-07-11 11:48:16,539] [INFO] [config.py:1007:print]   timers_config ................ enabled=True synchronized=True
0: [2025-07-11 11:48:16,539] [INFO] [config.py:1007:print]   train_batch_size ............. 64
0: [2025-07-11 11:48:16,539] [INFO] [config.py:1007:print]   train_micro_batch_size_per_gpu  1
0: [2025-07-11 11:48:16,539] [INFO] [config.py:1007:print]   use_data_before_expert_parallel_  False
0: [2025-07-11 11:48:16,539] [INFO] [config.py:1007:print]   use_node_local_storage ....... False
0: [2025-07-11 11:48:16,539] [INFO] [config.py:1007:print]   wall_clock_breakdown ......... False
0: [2025-07-11 11:48:16,539] [INFO] [config.py:1007:print]   weight_quantization_config ... None
0: [2025-07-11 11:48:16,539] [INFO] [config.py:1007:print]   world_size ................... 32
0: [2025-07-11 11:48:16,539] [INFO] [config.py:1007:print]   zero_allow_untested_optimizer  True
0: [2025-07-11 11:48:16,539] [INFO] [config.py:1007:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=12845056 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=11560550 param_persistence_threshold=35840 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco
0: _param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
0: [2025-07-11 11:48:16,539] [INFO] [config.py:1007:print]   zero_enabled ................. True
0: [2025-07-11 11:48:16,539] [INFO] [config.py:1007:print]   zero_force_ds_cpu_optimizer .. True
0: [2025-07-11 11:48:16,539] [INFO] [config.py:1007:print]   zero_optimization_stage ...... 3
0: [2025-07-11 11:48:16,539] [INFO] [config.py:993:print_user_config]   json = {
0:     "train_batch_size": 64, 
0:     "train_micro_batch_size_per_gpu": 1, 
0:     "gradient_accumulation_steps": 2, 
0:     "gradient_clipping": 1.0, 
0:     "zero_allow_untested_optimizer": true, 
0:     "fp16": {
0:         "enabled": false, 
0:         "loss_scale": 0, 
0:         "loss_scale_window": 1000, 
0:         "initial_scale_power": 16, 
0:         "hysteresis": 2, 
0:         "min_loss_scale": 1
0:     }, 
0:     "bf16": {
0:         "enabled": true
0:     }, 
0:     "zero_optimization": {
0:         "stage": 3, 
0:         "overlap_comm": false, 
0:         "contiguous_gradients": true, 
0:         "sub_group_size": 1.000000e+09, 
0:         "reduce_bucket_size": 1.284506e+07, 
0:         "stage3_prefetch_bucket_size": 1.156055e+07, 
0:         "stage3_param_persistence_threshold": 3.584000e+04, 
0:         "stage3_max_live_parameters": 1.000000e+09, 
0:         "stage3_max_reuse_distance": 1.000000e+09, 
0:         "stage3_gather_16bit_weights_on_model_save": true
0:     }, 
0:     "steps_per_print": inf
0: }
0: [INFO|trainer.py:2414] 2025-07-11 11:48:16,541 >> ***** Running training *****
0: [INFO|trainer.py:2415] 2025-07-11 11:48:16,541 >>   Num examples = 71,172
0: [INFO|trainer.py:2416] 2025-07-11 11:48:16,541 >>   Num Epochs = 1
0: [INFO|trainer.py:2417] 2025-07-11 11:48:16,541 >>   Instantaneous batch size per device = 1
0: [INFO|trainer.py:2420] 2025-07-11 11:48:16,541 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
0: [INFO|trainer.py:2421] 2025-07-11 11:48:16,541 >>   Gradient Accumulation steps = 2
0: [INFO|trainer.py:2422] 2025-07-11 11:48:16,541 >>   Total optimization steps = 1,112
0: [INFO|trainer.py:2423] 2025-07-11 11:48:16,543 >>   Number of trainable parameters = 8,292,166,656
0: [INFO|integration_utils.py:831] 2025-07-11 11:48:16,544 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
0: wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
0: wandb: Currently logged in as: nicolas-deperrois (krauthammerlab) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
0: wandb: Tracking run with wandb version 0.20.1
0: wandb: Run data is saved locally in /capstor/scratch/cscs/ndeperr/code/LLaMA-Factory/wandb/run-20250711_114817-yfz4yjj8
0: wandb: Run `wandb offline` to turn off syncing.
0: wandb: Syncing run /capstor/scratch/cscs/ndeperr/checkpoints/qwen2vl_cs_long/
0: wandb: ⭐️ View project at https://wandb.ai/krauthammerlab/llamafactory
0: wandb: 🚀 View run at https://wandb.ai/krauthammerlab/llamafactory/runs/yfz4yjj8
4: nid007121:116203:125704 [2] NCCL INFO Using network AWS Libfabric
4: nid007121:116204:125705 [3] NCCL INFO Using network AWS Libfabric
0: nid007116:202902:212609 [2] NCCL INFO Using network AWS Libfabric
4: nid007121:116203:125704 [2] NCCL INFO DMA-BUF is available on GPU device 2
4: nid007121:116204:125705 [3] NCCL INFO DMA-BUF is available on GPU device 3
7: nid007124:126398:135761 [3] NCCL INFO Using network AWS Libfabric
7: nid007124:126395:135760 [0] NCCL INFO Using network AWS Libfabric
7: nid007124:126397:135759 [2] NCCL INFO Using network AWS Libfabric
7: nid007124:126396:135762 [1] NCCL INFO Using network AWS Libfabric
6: nid007123:108292:117855 [3] NCCL INFO Using network AWS Libfabric
6: nid007123:108291:117856 [2] NCCL INFO Using network AWS Libfabric
0: nid007116:202900:212607 [0] NCCL INFO Using network AWS Libfabric
0: nid007116:202901:212608 [1] NCCL INFO Using network AWS Libfabric
4: nid007121:116201:125706 [0] NCCL INFO Using network AWS Libfabric
4: nid007121:116202:125703 [1] NCCL INFO Using network AWS Libfabric
5: nid007122:66178:75708 [3] NCCL INFO Using network AWS Libfabric
7: nid007124:126395:135760 [0] NCCL INFO DMA-BUF is available on GPU device 0
7: nid007124:126398:135761 [3] NCCL INFO DMA-BUF is available on GPU device 3
3: nid007120:197037:206391 [1] NCCL INFO Using network AWS Libfabric
3: nid007120:197039:206392 [3] NCCL INFO Using network AWS Libfabric
6: nid007123:108292:117855 [3] NCCL INFO DMA-BUF is available on GPU device 3
6: nid007123:108291:117856 [2] NCCL INFO DMA-BUF is available on GPU device 2
0: nid007116:202903:212610 [3] NCCL INFO Using network AWS Libfabric
0: nid007116:202902:212609 [2] NCCL INFO DMA-BUF is available on GPU device 2
0: nid007116:202901:212608 [1] NCCL INFO DMA-BUF is available on GPU device 1
4: nid007121:116201:125706 [0] NCCL INFO DMA-BUF is available on GPU device 0
5: nid007122:66178:75708 [3] NCCL INFO DMA-BUF is available on GPU device 3
7: nid007124:126396:135762 [1] NCCL INFO DMA-BUF is available on GPU device 1
7: nid007124:126397:135759 [2] NCCL INFO DMA-BUF is available on GPU device 2
2: nid007119:146138:155507 [2] NCCL INFO Using network AWS Libfabric
2: nid007119:146139:155510 [3] NCCL INFO Using network AWS Libfabric
2: nid007119:146137:155509 [1] NCCL INFO Using network AWS Libfabric
2: nid007119:146136:155508 [0] NCCL INFO Using network AWS Libfabric
3: nid007120:197037:206391 [1] NCCL INFO DMA-BUF is available on GPU device 1
3: nid007120:197036:206394 [0] NCCL INFO Using network AWS Libfabric
3: nid007120:197038:206393 [2] NCCL INFO Using network AWS Libfabric
3: nid007120:197039:206392 [3] NCCL INFO DMA-BUF is available on GPU device 3
6: nid007123:108289:117858 [0] NCCL INFO Using network AWS Libfabric
6: nid007123:108290:117857 [1] NCCL INFO Using network AWS Libfabric
1: nid007117:210407:220331 [2] NCCL INFO Using network AWS Libfabric
1: nid007117:210405:220334 [0] NCCL INFO Using network AWS Libfabric
0: nid007116:202900:212607 [0] NCCL INFO DMA-BUF is available on GPU device 0
4: nid007121:116202:125703 [1] NCCL INFO DMA-BUF is available on GPU device 1
5: nid007122:66175:75710 [0] NCCL INFO Using network AWS Libfabric
5: nid007122:66176:75709 [1] NCCL INFO Using network AWS Libfabric
2: nid007119:146139:155510 [3] NCCL INFO DMA-BUF is available on GPU device 3
2: nid007119:146137:155509 [1] NCCL INFO DMA-BUF is available on GPU device 1
2: nid007119:146136:155508 [0] NCCL INFO DMA-BUF is available on GPU device 0
2: nid007119:146138:155507 [2] NCCL INFO DMA-BUF is available on GPU device 2
3: nid007120:197038:206393 [2] NCCL INFO DMA-BUF is available on GPU device 2
3: nid007120:197036:206394 [0] NCCL INFO DMA-BUF is available on GPU device 0
6: nid007123:108290:117857 [1] NCCL INFO DMA-BUF is available on GPU device 1
6: nid007123:108289:117858 [0] NCCL INFO DMA-BUF is available on GPU device 0
1: nid007117:210408:220333 [3] NCCL INFO Using network AWS Libfabric
1: nid007117:210406:220332 [1] NCCL INFO Using network AWS Libfabric
1: nid007117:210407:220331 [2] NCCL INFO DMA-BUF is available on GPU device 2
1: nid007117:210405:220334 [0] NCCL INFO DMA-BUF is available on GPU device 0
0: nid007116:202903:212610 [3] NCCL INFO DMA-BUF is available on GPU device 3
5: nid007122:66177:75711 [2] NCCL INFO Using network AWS Libfabric
1: nid007117:210408:220333 [3] NCCL INFO DMA-BUF is available on GPU device 3
5: nid007122:66176:75709 [1] NCCL INFO DMA-BUF is available on GPU device 1
5: nid007122:66175:75710 [0] NCCL INFO DMA-BUF is available on GPU device 0
5: nid007122:66177:75711 [2] NCCL INFO DMA-BUF is available on GPU device 2
1: nid007117:210406:220332 [1] NCCL INFO DMA-BUF is available on GPU device 1
4: nid007121:116204:125705 [3] NCCL INFO bootstrapSplit: comm 0x40055ca5cdf0 parent 0xaaab22c089c0 rank 19 nranks 32 color 698429859 key 19 prev 18 next 20 - DONE
4: nid007121:116204:125705 [3] NCCL INFO ncclCommSplit comm 0x40055ca5cdf0 rank 19 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab22c089c0 color 698429859 key 19 commId 0xe064a3d036cb71a - Init START
0: nid007116:202902:212609 [2] NCCL INFO bootstrapSplit: comm 0x400554cb2170 parent 0xaaaaf1faa2e0 rank 2 nranks 32 color 698429859 key 2 prev 1 next 3 - DONE
0: nid007116:202903:212610 [3] NCCL INFO bootstrapSplit: comm 0x400570ca94e0 parent 0xaaab18df7db0 rank 3 nranks 32 color 698429859 key 3 prev 2 next 4 - DONE
0: nid007116:202902:212609 [2] NCCL INFO ncclCommSplit comm 0x400554cb2170 rank 2 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaaf1faa2e0 color 698429859 key 2 commId 0xe064a3d036cb71a - Init START
0: nid007116:202903:212610 [3] NCCL INFO ncclCommSplit comm 0x400570ca94e0 rank 3 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab18df7db0 color 698429859 key 3 commId 0xe064a3d036cb71a - Init START
4: nid007121:116203:125704 [2] NCCL INFO bootstrapSplit: comm 0x400550a5d2b0 parent 0xaaab08232380 rank 18 nranks 32 color 698429859 key 18 prev 17 next 19 - DONE
4: nid007121:116203:125704 [2] NCCL INFO ncclCommSplit comm 0x400550a5d2b0 rank 18 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab08232380 color 698429859 key 18 commId 0xe064a3d036cb71a - Init START
0: nid007116:202901:212608 [1] NCCL INFO bootstrapSplit: comm 0x400564a5a3b0 parent 0xaaaaf4f9b440 rank 1 nranks 32 color 698429859 key 1 prev 0 next 2 - DONE
0: nid007116:202901:212608 [1] NCCL INFO ncclCommSplit comm 0x400564a5a3b0 rank 1 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaaf4f9b440 color 698429859 key 1 commId 0xe064a3d036cb71a - Init START
0: nid007116:202900:212607 [0] NCCL INFO bootstrapSplit: comm 0x40054ca59aa0 parent 0xaaab0f53e270 rank 0 nranks 32 color 698429859 key 0 prev 31 next 1 - DONE
4: nid007121:116201:125706 [0] NCCL INFO bootstrapSplit: comm 0x400534cad5d0 parent 0xaaaacfcf80d0 rank 16 nranks 32 color 698429859 key 16 prev 15 next 17 - DONE
4: nid007121:116202:125703 [1] NCCL INFO bootstrapSplit: comm 0x400534a86a10 parent 0xaaab2fe09390 rank 17 nranks 32 color 698429859 key 17 prev 16 next 18 - DONE
4: nid007121:116201:125706 [0] NCCL INFO ncclCommSplit comm 0x400534cad5d0 rank 16 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaacfcf80d0 color 698429859 key 16 commId 0xe064a3d036cb71a - Init START
4: nid007121:116202:125703 [1] NCCL INFO ncclCommSplit comm 0x400534a86a10 rank 17 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab2fe09390 color 698429859 key 17 commId 0xe064a3d036cb71a - Init START
5: nid007122:66175:75710 [0] NCCL INFO bootstrapSplit: comm 0x400568cb6280 parent 0xaaab0041ada0 rank 20 nranks 32 color 698429859 key 20 prev 19 next 21 - DONE
5: nid007122:66177:75711 [2] NCCL INFO bootstrapSplit: comm 0x40054ca8abd0 parent 0xaaaacb2d9e10 rank 22 nranks 32 color 698429859 key 22 prev 21 next 23 - DONE
5: nid007122:66178:75708 [3] NCCL INFO bootstrapSplit: comm 0x400554a7fd80 parent 0xaaab08462130 rank 23 nranks 32 color 698429859 key 23 prev 22 next 24 - DONE
5: nid007122:66176:75709 [1] NCCL INFO bootstrapSplit: comm 0x400554a5d250 parent 0xaaab1e05ac60 rank 21 nranks 32 color 698429859 key 21 prev 20 next 22 - DONE
5: nid007122:66175:75710 [0] NCCL INFO ncclCommSplit comm 0x400568cb6280 rank 20 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab0041ada0 color 698429859 key 20 commId 0xe064a3d036cb71a - Init START
6: nid007123:108292:117855 [3] NCCL INFO bootstrapSplit: comm 0x400538ca8050 parent 0xaaaaf5620a20 rank 27 nranks 32 color 698429859 key 27 prev 26 next 28 - DONE
6: nid007123:108292:117855 [3] NCCL INFO ncclCommSplit comm 0x400538ca8050 rank 27 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaaf5620a20 color 698429859 key 27 commId 0xe064a3d036cb71a - Init START
6: nid007123:108291:117856 [2] NCCL INFO bootstrapSplit: comm 0x400564cb0bc0 parent 0xaaab23840990 rank 26 nranks 32 color 698429859 key 26 prev 25 next 27 - DONE
0: nid007116:202900:212607 [0] NCCL INFO ncclCommSplit comm 0x40054ca59aa0 rank 0 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab0f53e270 color 698429859 key 0 commId 0xe064a3d036cb71a - Init START
5: nid007122:66177:75711 [2] NCCL INFO ncclCommSplit comm 0x40054ca8abd0 rank 22 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaacb2d9e10 color 698429859 key 22 commId 0xe064a3d036cb71a - Init START
5: nid007122:66178:75708 [3] NCCL INFO ncclCommSplit comm 0x400554a7fd80 rank 23 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab08462130 color 698429859 key 23 commId 0xe064a3d036cb71a - Init START
5: nid007122:66176:75709 [1] NCCL INFO ncclCommSplit comm 0x400554a5d250 rank 21 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab1e05ac60 color 698429859 key 21 commId 0xe064a3d036cb71a - Init START
6: nid007123:108289:117858 [0] NCCL INFO bootstrapSplit: comm 0x400550a5d390 parent 0xaaaaef3a5bf0 rank 24 nranks 32 color 698429859 key 24 prev 23 next 25 - DONE
6: nid007123:108291:117856 [2] NCCL INFO ncclCommSplit comm 0x400564cb0bc0 rank 26 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab23840990 color 698429859 key 26 commId 0xe064a3d036cb71a - Init START
6: nid007123:108290:117857 [1] NCCL INFO bootstrapSplit: comm 0x400554cb0c00 parent 0xaaab083c2070 rank 25 nranks 32 color 698429859 key 25 prev 24 next 26 - DONE
6: nid007123:108289:117858 [0] NCCL INFO ncclCommSplit comm 0x400550a5d390 rank 24 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaef3a5bf0 color 698429859 key 24 commId 0xe064a3d036cb71a - Init START
1: nid007117:210408:220333 [3] NCCL INFO bootstrapSplit: comm 0x400548ca90f0 parent 0xaaaaf8ea2d90 rank 7 nranks 32 color 698429859 key 7 prev 6 next 8 - DONE
1: nid007117:210408:220333 [3] NCCL INFO ncclCommSplit comm 0x400548ca90f0 rank 7 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaaf8ea2d90 color 698429859 key 7 commId 0xe064a3d036cb71a - Init START
2: nid007119:146138:155507 [2] NCCL INFO bootstrapSplit: comm 0x40053ca85450 parent 0xaaab072922f0 rank 10 nranks 32 color 698429859 key 10 prev 9 next 11 - DONE
2: nid007119:146139:155510 [3] NCCL INFO bootstrapSplit: comm 0x400550a5bfe0 parent 0xaaab16111160 rank 11 nranks 32 color 698429859 key 11 prev 10 next 12 - DONE
2: nid007119:146136:155508 [0] NCCL INFO bootstrapSplit: comm 0x400530a869c0 parent 0xaaab09168e10 rank 8 nranks 32 color 698429859 key 8 prev 7 next 9 - DONE
2: nid007119:146138:155507 [2] NCCL INFO ncclCommSplit comm 0x40053ca85450 rank 10 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab072922f0 color 698429859 key 10 commId 0xe064a3d036cb71a - Init START
2: nid007119:146137:155509 [1] NCCL INFO bootstrapSplit: comm 0x40055cca7df0 parent 0xaaab1b0b7890 rank 9 nranks 32 color 698429859 key 9 prev 8 next 10 - DONE
3: nid007120:197039:206392 [3] NCCL INFO bootstrapSplit: comm 0x400574a83ab0 parent 0xaaaaeadc27e0 rank 15 nranks 32 color 698429859 key 15 prev 14 next 16 - DONE
3: nid007120:197037:206391 [1] NCCL INFO bootstrapSplit: comm 0x400550a5bc80 parent 0xaaaafddafcb0 rank 13 nranks 32 color 698429859 key 13 prev 12 next 14 - DONE
3: nid007120:197038:206393 [2] NCCL INFO bootstrapSplit: comm 0x400534a5bf20 parent 0xaaab036888d0 rank 14 nranks 32 color 698429859 key 14 prev 13 next 15 - DONE
3: nid007120:197036:206394 [0] NCCL INFO bootstrapSplit: comm 0x400568a75b90 parent 0xaaaaffa2f9f0 rank 12 nranks 32 color 698429859 key 12 prev 11 next 13 - DONE
3: nid007120:197039:206392 [3] NCCL INFO ncclCommSplit comm 0x400574a83ab0 rank 15 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaaeadc27e0 color 698429859 key 15 commId 0xe064a3d036cb71a - Init START
6: nid007123:108290:117857 [1] NCCL INFO ncclCommSplit comm 0x400554cb0c00 rank 25 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab083c2070 color 698429859 key 25 commId 0xe064a3d036cb71a - Init START
1: nid007117:210407:220331 [2] NCCL INFO bootstrapSplit: comm 0x400544ca7a20 parent 0xaaab2c3c27a0 rank 6 nranks 32 color 698429859 key 6 prev 5 next 7 - DONE
1: nid007117:210406:220332 [1] NCCL INFO bootstrapSplit: comm 0x400548a5bdd0 parent 0xaaaaf2c70c20 rank 5 nranks 32 color 698429859 key 5 prev 4 next 6 - DONE
1: nid007117:210405:220334 [0] NCCL INFO bootstrapSplit: comm 0x40054ca55440 parent 0xaaab0fcf58a0 rank 4 nranks 32 color 698429859 key 4 prev 3 next 5 - DONE
1: nid007117:210407:220331 [2] NCCL INFO ncclCommSplit comm 0x400544ca7a20 rank 6 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab2c3c27a0 color 698429859 key 6 commId 0xe064a3d036cb71a - Init START
7: nid007124:126397:135759 [2] NCCL INFO bootstrapSplit: comm 0x40054ccb07a0 parent 0xaaab10cea560 rank 30 nranks 32 color 698429859 key 30 prev 29 next 31 - DONE
7: nid007124:126397:135759 [2] NCCL INFO ncclCommSplit comm 0x40054ccb07a0 rank 30 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab10cea560 color 698429859 key 30 commId 0xe064a3d036cb71a - Init START
7: nid007124:126395:135760 [0] NCCL INFO bootstrapSplit: comm 0x400560ca7c30 parent 0xaaaaef48c330 rank 28 nranks 32 color 698429859 key 28 prev 27 next 29 - DONE
7: nid007124:126396:135762 [1] NCCL INFO bootstrapSplit: comm 0x400544ca95d0 parent 0xaaab39820720 rank 29 nranks 32 color 698429859 key 29 prev 28 next 30 - DONE
7: nid007124:126398:135761 [3] NCCL INFO bootstrapSplit: comm 0x400574a5d2b0 parent 0xaaab12762370 rank 31 nranks 32 color 698429859 key 31 prev 30 next 0 - DONE
2: nid007119:146139:155510 [3] NCCL INFO ncclCommSplit comm 0x400550a5bfe0 rank 11 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab16111160 color 698429859 key 11 commId 0xe064a3d036cb71a - Init START
2: nid007119:146136:155508 [0] NCCL INFO ncclCommSplit comm 0x400530a869c0 rank 8 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab09168e10 color 698429859 key 8 commId 0xe064a3d036cb71a - Init START
3: nid007120:197037:206391 [1] NCCL INFO ncclCommSplit comm 0x400550a5bc80 rank 13 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaafddafcb0 color 698429859 key 13 commId 0xe064a3d036cb71a - Init START
3: nid007120:197038:206393 [2] NCCL INFO ncclCommSplit comm 0x400534a5bf20 rank 14 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab036888d0 color 698429859 key 14 commId 0xe064a3d036cb71a - Init START
3: nid007120:197036:206394 [0] NCCL INFO ncclCommSplit comm 0x400568a75b90 rank 12 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaffa2f9f0 color 698429859 key 12 commId 0xe064a3d036cb71a - Init START
1: nid007117:210406:220332 [1] NCCL INFO ncclCommSplit comm 0x400548a5bdd0 rank 5 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaaf2c70c20 color 698429859 key 5 commId 0xe064a3d036cb71a - Init START
1: nid007117:210405:220334 [0] NCCL INFO ncclCommSplit comm 0x40054ca55440 rank 4 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab0fcf58a0 color 698429859 key 4 commId 0xe064a3d036cb71a - Init START
7: nid007124:126395:135760 [0] NCCL INFO ncclCommSplit comm 0x400560ca7c30 rank 28 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaef48c330 color 698429859 key 28 commId 0xe064a3d036cb71a - Init START
2: nid007119:146137:155509 [1] NCCL INFO ncclCommSplit comm 0x40055cca7df0 rank 9 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab1b0b7890 color 698429859 key 9 commId 0xe064a3d036cb71a - Init START
7: nid007124:126396:135762 [1] NCCL INFO ncclCommSplit comm 0x400544ca95d0 rank 29 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab39820720 color 698429859 key 29 commId 0xe064a3d036cb71a - Init START
7: nid007124:126398:135761 [3] NCCL INFO ncclCommSplit comm 0x400574a5d2b0 rank 31 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab12762370 color 698429859 key 31 commId 0xe064a3d036cb71a - Init START
7: nid007124:126397:135759 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
7: nid007124:126395:135760 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
7: nid007124:126397:135759 [2] NCCL INFO NVLS multicast support is not available on dev 2
7: nid007124:126395:135760 [0] NCCL INFO NVLS multicast support is not available on dev 0
7: nid007124:126396:135762 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
7: nid007124:126398:135761 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
7: nid007124:126398:135761 [3] NCCL INFO NVLS multicast support is not available on dev 3
7: nid007124:126396:135762 [1] NCCL INFO NVLS multicast support is not available on dev 1
6: nid007123:108292:117855 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
6: nid007123:108292:117855 [3] NCCL INFO NVLS multicast support is not available on dev 3
6: nid007123:108291:117856 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
6: nid007123:108291:117856 [2] NCCL INFO NVLS multicast support is not available on dev 2
6: nid007123:108289:117858 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
6: nid007123:108290:117857 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
6: nid007123:108290:117857 [1] NCCL INFO NVLS multicast support is not available on dev 1
6: nid007123:108289:117858 [0] NCCL INFO NVLS multicast support is not available on dev 0
3: nid007120:197038:206393 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
3: nid007120:197038:206393 [2] NCCL INFO NVLS multicast support is not available on dev 2
3: nid007120:197036:206394 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
3: nid007120:197037:206391 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
3: nid007120:197039:206392 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
3: nid007120:197036:206394 [0] NCCL INFO NVLS multicast support is not available on dev 0
3: nid007120:197037:206391 [1] NCCL INFO NVLS multicast support is not available on dev 1
3: nid007120:197039:206392 [3] NCCL INFO NVLS multicast support is not available on dev 3
5: nid007122:66177:75711 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
5: nid007122:66177:75711 [2] NCCL INFO NVLS multicast support is not available on dev 2
5: nid007122:66175:75710 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
5: nid007122:66178:75708 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
5: nid007122:66175:75710 [0] NCCL INFO NVLS multicast support is not available on dev 0
5: nid007122:66178:75708 [3] NCCL INFO NVLS multicast support is not available on dev 3
5: nid007122:66176:75709 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
5: nid007122:66176:75709 [1] NCCL INFO NVLS multicast support is not available on dev 1
2: nid007119:146136:155508 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
2: nid007119:146136:155508 [0] NCCL INFO NVLS multicast support is not available on dev 0
2: nid007119:146139:155510 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
2: nid007119:146139:155510 [3] NCCL INFO NVLS multicast support is not available on dev 3
2: nid007119:146137:155509 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
2: nid007119:146137:155509 [1] NCCL INFO NVLS multicast support is not available on dev 1
2: nid007119:146138:155507 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
2: nid007119:146138:155507 [2] NCCL INFO NVLS multicast support is not available on dev 2
4: nid007121:116204:125705 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
4: nid007121:116204:125705 [3] NCCL INFO NVLS multicast support is not available on dev 3
4: nid007121:116203:125704 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
4: nid007121:116202:125703 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
4: nid007121:116202:125703 [1] NCCL INFO NVLS multicast support is not available on dev 1
4: nid007121:116203:125704 [2] NCCL INFO NVLS multicast support is not available on dev 2
4: nid007121:116201:125706 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
4: nid007121:116201:125706 [0] NCCL INFO NVLS multicast support is not available on dev 0
1: nid007117:210405:220334 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
1: nid007117:210407:220331 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
1: nid007117:210407:220331 [2] NCCL INFO NVLS multicast support is not available on dev 2
1: nid007117:210408:220333 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
1: nid007117:210405:220334 [0] NCCL INFO NVLS multicast support is not available on dev 0
1: nid007117:210408:220333 [3] NCCL INFO NVLS multicast support is not available on dev 3
1: nid007117:210406:220332 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
1: nid007117:210406:220332 [1] NCCL INFO NVLS multicast support is not available on dev 1
0: nid007116:202902:212609 [2] NCCL INFO Setting affinity for GPU 2 to ffffff,ffffffff,ffff0000,00000000,00000000,00000000,00000000
0: nid007116:202902:212609 [2] NCCL INFO NVLS multicast support is not available on dev 2
0: nid007116:202900:212607 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
0: nid007116:202901:212608 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ffffffff,ffffff00,00000000,00000000
0: nid007116:202901:212608 [1] NCCL INFO NVLS multicast support is not available on dev 1
0: nid007116:202900:212607 [0] NCCL INFO NVLS multicast support is not available on dev 0
0: nid007116:202903:212610 [3] NCCL INFO Setting affinity for GPU 3 to ffffffff,ffffffff,ff000000,00000000,00000000,00000000,00000000,00000000,00000000
0: nid007116:202903:212610 [3] NCCL INFO NVLS multicast support is not available on dev 3
0: nid007116:202903:212610 [3] NCCL INFO comm 0x400570ca94e0 rank 3 nRanks 32 nNodes 8 localRanks 4 localRank 3 MNNVL 0
0: nid007116:202903:212610 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 0/-1/-1->3->2 [3] 0/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] 0/-1/-1->3->2 [7] 0/-1/-1->3->2
0: nid007116:202903:212610 [3] NCCL INFO P2P Chunksize set to 131072
0: nid007116:202902:212609 [2] NCCL INFO comm 0x400554cb2170 rank 2 nRanks 32 nNodes 8 localRanks 4 localRank 2 MNNVL 0
0: nid007116:202900:212607 [0] NCCL INFO comm 0x40054ca59aa0 rank 0 nRanks 32 nNodes 8 localRanks 4 localRank 0 MNNVL 0
0: nid007116:202901:212608 [1] NCCL INFO comm 0x400564a5a3b0 rank 1 nRanks 32 nNodes 8 localRanks 4 localRank 1 MNNVL 0
0: nid007116:202900:212607 [0] NCCL INFO Channel 00/08 :    0   1   2   3   7   6   5   4   8   9  10  11  15  14  13  12  16  17  18  19
0: nid007116:202902:212609 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/18/-1->2->-1 [3] 3/18/-1->2->-1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->6 [7] 3/-1/-1->2->6
0: nid007116:202900:212607 [0] NCCL INFO Channel 01/08 :    0   4   5   6   7  11  10   9   8  12  13  14  15  19  18  17  16  20  21  22
0: nid007116:202901:212608 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] -1/-1/-1->1->0 [3] -1/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] -1/-1/-1->1->0 [7] -1/-1/-1->1->0
0: nid007116:202902:212609 [2] NCCL INFO P2P Chunksize set to 131072
0: nid007116:202900:212607 [0] NCCL INFO Channel 02/08 :    0   3   1   5   4   7   6  10   8  11   9  13  12  15  14  18  16  19  17  21
0: nid007116:202901:212608 [1] NCCL INFO P2P Chunksize set to 131072
0: nid007116:202900:212607 [0] NCCL INFO Channel 03/08 :    0   3   2   6   4   7   5   9   8  11  10  14  12  15  13  17  16  19  18  22
0: nid007116:202900:212607 [0] NCCL INFO Channel 04/08 :    0   1   2   3   7   6   5   4   8   9  10  11  15  14  13  12  16  17  18  19
0: nid007116:202900:212607 [0] NCCL INFO Channel 05/08 :    0   4   5   6   7  11  10   9   8  12  13  14  15  19  18  17  16  20  21  22
0: nid007116:202900:212607 [0] NCCL INFO Channel 06/08 :    0   3   1   5   4   7   6  10   8  11   9  13  12  15  14  18  16  19  17  21
0: nid007116:202900:212607 [0] NCCL INFO Channel 07/08 :    0   3   2   6   4   7   5   9   8  11  10  14  12  15  13  17  16  19  18  22
0: nid007116:202900:212607 [0] NCCL INFO Trees [0] 1/16/-1->0->-1 [1] 1/16/-1->0->-1 [2] 1/-1/-1->0->3 [3] 1/-1/-1->0->3 [4] 1/-1/-1->0->4 [5] 1/-1/-1->0->4 [6] 1/-1/-1->0->3 [7] 1/-1/-1->0->3
0: nid007116:202900:212607 [0] NCCL INFO P2P Chunksize set to 131072
7: nid007124:126398:135761 [3] NCCL INFO comm 0x400574a5d2b0 rank 31 nRanks 32 nNodes 8 localRanks 4 localRank 3 MNNVL 0
7: nid007124:126397:135759 [2] NCCL INFO comm 0x40054ccb07a0 rank 30 nRanks 32 nNodes 8 localRanks 4 localRank 2 MNNVL 0
7: nid007124:126396:135762 [1] NCCL INFO comm 0x400544ca95d0 rank 29 nRanks 32 nNodes 8 localRanks 4 localRank 1 MNNVL 0
4: nid007121:116204:125705 [3] NCCL INFO comm 0x40055ca5cdf0 rank 19 nRanks 32 nNodes 8 localRanks 4 localRank 3 MNNVL 0
7: nid007124:126395:135760 [0] NCCL INFO comm 0x400560ca7c30 rank 28 nRanks 32 nNodes 8 localRanks 4 localRank 0 MNNVL 0
6: nid007123:108292:117855 [3] NCCL INFO comm 0x400538ca8050 rank 27 nRanks 32 nNodes 8 localRanks 4 localRank 3 MNNVL 0
4: nid007121:116204:125705 [3] NCCL INFO Trees [0] -1/-1/-1->19->18 [1] -1/-1/-1->19->18 [2] 16/-1/-1->19->18 [3] 16/-1/-1->19->18 [4] -1/-1/-1->19->18 [5] -1/-1/-1->19->18 [6] 16/-1/-1->19->18 [7] 16/-1/-1->19->18
4: nid007121:116204:125705 [3] NCCL INFO P2P Chunksize set to 131072
7: nid007124:126397:135759 [2] NCCL INFO Trees [0] 31/-1/-1->30->29 [1] 31/-1/-1->30->29 [2] 31/-1/-1->30->26 [3] 31/-1/-1->30->26 [4] 31/-1/-1->30->29 [5] 31/-1/-1->30->29 [6] 31/14/-1->30->-1 [7] 31/14/-1->30->-1
7: nid007124:126398:135761 [3] NCCL INFO Trees [0] -1/-1/-1->31->30 [1] -1/-1/-1->31->30 [2] 28/-1/-1->31->30 [3] 28/-1/-1->31->30 [4] -1/-1/-1->31->30 [5] -1/-1/-1->31->30 [6] 28/-1/-1->31->30 [7] 28/-1/-1->31->30
6: nid007123:108292:117855 [3] NCCL INFO Trees [0] -1/-1/-1->27->26 [1] -1/-1/-1->27->26 [2] 24/-1/-1->27->26 [3] 24/-1/-1->27->26 [4] -1/-1/-1->27->26 [5] -1/-1/-1->27->26 [6] 24/-1/-1->27->26 [7] 24/-1/-1->27->26
6: nid007123:108292:117855 [3] NCCL INFO P2P Chunksize set to 131072
7: nid007124:126396:135762 [1] NCCL INFO Trees [0] 30/-1/-1->29->28 [1] 30/-1/-1->29->28 [2] -1/-1/-1->29->28 [3] -1/-1/-1->29->28 [4] 30/-1/-1->29->28 [5] 30/-1/-1->29->28 [6] -1/-1/-1->29->28 [7] -1/-1/-1->29->28
7: nid007124:126397:135759 [2] NCCL INFO P2P Chunksize set to 131072
7: nid007124:126398:135761 [3] NCCL INFO P2P Chunksize set to 131072
7: nid007124:126396:135762 [1] NCCL INFO P2P Chunksize set to 131072
3: nid007120:197039:206392 [3] NCCL INFO comm 0x400574a83ab0 rank 15 nRanks 32 nNodes 8 localRanks 4 localRank 3 MNNVL 0
3: nid007120:197038:206393 [2] NCCL INFO comm 0x400534a5bf20 rank 14 nRanks 32 nNodes 8 localRanks 4 localRank 2 MNNVL 0
2: nid007119:146139:155510 [3] NCCL INFO comm 0x400550a5bfe0 rank 11 nRanks 32 nNodes 8 localRanks 4 localRank 3 MNNVL 0
3: nid007120:197037:206391 [1] NCCL INFO comm 0x400550a5bc80 rank 13 nRanks 32 nNodes 8 localRanks 4 localRank 1 MNNVL 0
3: nid007120:197038:206393 [2] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->10 [3] 15/-1/-1->14->10 [4] 15/-1/-1->14->13 [5] 15/-1/-1->14->13 [6] 15/22/6->14->30 [7] 15/22/6->14->30
3: nid007120:197036:206394 [0] NCCL INFO comm 0x400568a75b90 rank 12 nRanks 32 nNodes 8 localRanks 4 localRank 0 MNNVL 0
1: nid007117:210406:220332 [1] NCCL INFO comm 0x400548a5bdd0 rank 5 nRanks 32 nNodes 8 localRanks 4 localRank 1 MNNVL 0
1: nid007117:210405:220334 [0] NCCL INFO comm 0x40054ca55440 rank 4 nRanks 32 nNodes 8 localRanks 4 localRank 0 MNNVL 0
1: nid007117:210408:220333 [3] NCCL INFO comm 0x400548ca90f0 rank 7 nRanks 32 nNodes 8 localRanks 4 localRank 3 MNNVL 0
1: nid007117:210407:220331 [2] NCCL INFO comm 0x400544ca7a20 rank 6 nRanks 32 nNodes 8 localRanks 4 localRank 2 MNNVL 0
4: nid007121:116203:125704 [2] NCCL INFO comm 0x400550a5d2b0 rank 18 nRanks 32 nNodes 8 localRanks 4 localRank 2 MNNVL 0
4: nid007121:116202:125703 [1] NCCL INFO comm 0x400534a86a10 rank 17 nRanks 32 nNodes 8 localRanks 4 localRank 1 MNNVL 0
4: nid007121:116201:125706 [0] NCCL INFO comm 0x400534cad5d0 rank 16 nRanks 32 nNodes 8 localRanks 4 localRank 0 MNNVL 0
2: nid007119:146138:155507 [2] NCCL INFO comm 0x40053ca85450 rank 10 nRanks 32 nNodes 8 localRanks 4 localRank 2 MNNVL 0
2: nid007119:146139:155510 [3] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] -1/-1/-1->11->10 [2] 8/-1/-1->11->10 [3] 8/-1/-1->11->10 [4] -1/-1/-1->11->10 [5] -1/-1/-1->11->10 [6] 8/-1/-1->11->10 [7] 8/-1/-1->11->10
3: nid007120:197039:206392 [3] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] -1/-1/-1->15->14 [2] 12/-1/-1->15->14 [3] 12/-1/-1->15->14 [4] -1/-1/-1->15->14 [5] -1/-1/-1->15->14 [6] 12/-1/-1->15->14 [7] 12/-1/-1->15->14
3: nid007120:197038:206393 [2] NCCL INFO P2P Chunksize set to 131072
3: nid007120:197037:206391 [1] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12 [2] -1/-1/-1->13->12 [3] -1/-1/-1->13->12 [4] 14/-1/-1->13->12 [5] 14/-1/-1->13->12 [6] -1/-1/-1->13->12 [7] -1/-1/-1->13->12
6: nid007123:108290:117857 [1] NCCL INFO comm 0x400554cb0c00 rank 25 nRanks 32 nNodes 8 localRanks 4 localRank 1 MNNVL 0
6: nid007123:108291:117856 [2] NCCL INFO comm 0x400564cb0bc0 rank 26 nRanks 32 nNodes 8 localRanks 4 localRank 2 MNNVL 0
1: nid007117:210406:220332 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] -1/-1/-1->5->4 [3] -1/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] -1/-1/-1->5->4 [7] -1/-1/-1->5->4
1: nid007117:210408:220333 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] 4/-1/-1->7->6 [3] 4/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] 4/-1/-1->7->6 [7] 4/-1/-1->7->6
1: nid007117:210406:220332 [1] NCCL INFO P2P Chunksize set to 131072
1: nid007117:210407:220331 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->10 [3] 7/-1/-1->6->10 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/10/2->6->14 [7] 7/10/2->6->14
1: nid007117:210408:220333 [3] NCCL INFO P2P Chunksize set to 131072
1: nid007117:210407:220331 [2] NCCL INFO P2P Chunksize set to 131072
4: nid007121:116202:125703 [1] NCCL INFO Trees [0] 18/-1/-1->17->16 [1] 18/-1/-1->17->16 [2] -1/-1/-1->17->16 [3] -1/-1/-1->17->16 [4] 18/-1/-1->17->16 [5] 18/-1/-1->17->16 [6] -1/-1/-1->17->16 [7] -1/-1/-1->17->16
4: nid007121:116202:125703 [1] NCCL INFO P2P Chunksize set to 131072
4: nid007121:116203:125704 [2] NCCL INFO Trees [0] 19/-1/-1->18->17 [1] 19/-1/-1->18->17 [2] 19/10/26->18->2 [3] 19/10/26->18->2 [4] 19/-1/-1->18->17 [5] 19/-1/-1->18->17 [6] 19/-1/-1->18->22 [7] 19/-1/-1->18->22
7: nid007124:126395:135760 [0] NCCL INFO Trees [0] 29/-1/-1->28->24 [1] 29/-1/-1->28->24 [2] 29/-1/-1->28->31 [3] 29/-1/-1->28->31 [4] 29/12/-1->28->-1 [5] 29/12/-1->28->-1 [6] 29/-1/-1->28->31 [7] 29/-1/-1->28->31
7: nid007124:126395:135760 [0] NCCL INFO P2P Chunksize set to 131072
2: nid007119:146139:155510 [3] NCCL INFO P2P Chunksize set to 131072
3: nid007120:197039:206392 [3] NCCL INFO P2P Chunksize set to 131072
3: nid007120:197037:206391 [1] NCCL INFO P2P Chunksize set to 131072
3: nid007120:197036:206394 [0] NCCL INFO Trees [0] 13/-1/-1->12->8 [1] 13/-1/-1->12->8 [2] 13/-1/-1->12->15 [3] 13/-1/-1->12->15 [4] 13/20/4->12->28 [5] 13/20/4->12->28 [6] 13/-1/-1->12->15 [7] 13/-1/-1->12->15
3: nid007120:197036:206394 [0] NCCL INFO P2P Chunksize set to 131072
6: nid007123:108290:117857 [1] NCCL INFO Trees [0] 26/-1/-1->25->24 [1] 26/-1/-1->25->24 [2] -1/-1/-1->25->24 [3] -1/-1/-1->25->24 [4] 26/-1/-1->25->24 [5] 26/-1/-1->25->24 [6] -1/-1/-1->25->24 [7] -1/-1/-1->25->24
1: nid007117:210405:220334 [0] NCCL INFO Trees [0] 5/-1/-1->4->8 [1] 5/-1/-1->4->8 [2] 5/-1/-1->4->7 [3] 5/-1/-1->4->7 [4] 5/8/0->4->12 [5] 5/8/0->4->12 [6] 5/-1/-1->4->7 [7] 5/-1/-1->4->7
1: nid007117:210405:220334 [0] NCCL INFO P2P Chunksize set to 131072
4: nid007121:116203:125704 [2] NCCL INFO P2P Chunksize set to 131072
5: nid007122:66178:75708 [3] NCCL INFO comm 0x400554a7fd80 rank 23 nRanks 32 nNodes 8 localRanks 4 localRank 3 MNNVL 0
2: nid007119:146138:155507 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9 [2] 11/6/14->10->18 [3] 11/6/14->10->18 [4] 11/-1/-1->10->9 [5] 11/-1/-1->10->9 [6] 11/-1/-1->10->6 [7] 11/-1/-1->10->6
6: nid007123:108290:117857 [1] NCCL INFO P2P Chunksize set to 131072
6: nid007123:108291:117856 [2] NCCL INFO Trees [0] 27/-1/-1->26->25 [1] 27/-1/-1->26->25 [2] 27/22/30->26->18 [3] 27/22/30->26->18 [4] 27/-1/-1->26->25 [5] 27/-1/-1->26->25 [6] 27/-1/-1->26->22 [7] 27/-1/-1->26->22
4: nid007121:116201:125706 [0] NCCL INFO Trees [0] 17/8/24->16->0 [1] 17/8/24->16->0 [2] 17/-1/-1->16->19 [3] 17/-1/-1->16->19 [4] 17/-1/-1->16->20 [5] 17/-1/-1->16->20 [6] 17/-1/-1->16->19 [7] 17/-1/-1->16->19
4: nid007121:116201:125706 [0] NCCL INFO P2P Chunksize set to 131072
5: nid007122:66177:75711 [2] NCCL INFO comm 0x40054ca8abd0 rank 22 nRanks 32 nNodes 8 localRanks 4 localRank 2 MNNVL 0
2: nid007119:146138:155507 [2] NCCL INFO P2P Chunksize set to 131072
2: nid007119:146137:155509 [1] NCCL INFO comm 0x40055cca7df0 rank 9 nRanks 32 nNodes 8 localRanks 4 localRank 1 MNNVL 0
6: nid007123:108289:117858 [0] NCCL INFO comm 0x400550a5d390 rank 24 nRanks 32 nNodes 8 localRanks 4 localRank 0 MNNVL 0
6: nid007123:108291:117856 [2] NCCL INFO P2P Chunksize set to 131072
5: nid007122:66178:75708 [3] NCCL INFO Trees [0] -1/-1/-1->23->22 [1] -1/-1/-1->23->22 [2] 20/-1/-1->23->22 [3] 20/-1/-1->23->22 [4] -1/-1/-1->23->22 [5] -1/-1/-1->23->22 [6] 20/-1/-1->23->22 [7] 20/-1/-1->23->22
5: nid007122:66177:75711 [2] NCCL INFO Trees [0] 23/-1/-1->22->21 [1] 23/-1/-1->22->21 [2] 23/-1/-1->22->26 [3] 23/-1/-1->22->26 [4] 23/-1/-1->22->21 [5] 23/-1/-1->22->21 [6] 23/26/18->22->14 [7] 23/26/18->22->14
5: nid007122:66178:75708 [3] NCCL INFO P2P Chunksize set to 131072
5: nid007122:66177:75711 [2] NCCL INFO P2P Chunksize set to 131072
2: nid007119:146137:155509 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] 10/-1/-1->9->8 [2] -1/-1/-1->9->8 [3] -1/-1/-1->9->8 [4] 10/-1/-1->9->8 [5] 10/-1/-1->9->8 [6] -1/-1/-1->9->8 [7] -1/-1/-1->9->8
2: nid007119:146137:155509 [1] NCCL INFO P2P Chunksize set to 131072
5: nid007122:66176:75709 [1] NCCL INFO comm 0x400554a5d250 rank 21 nRanks 32 nNodes 8 localRanks 4 localRank 1 MNNVL 0
5: nid007122:66175:75710 [0] NCCL INFO comm 0x400568cb6280 rank 20 nRanks 32 nNodes 8 localRanks 4 localRank 0 MNNVL 0
2: nid007119:146136:155508 [0] NCCL INFO comm 0x400530a869c0 rank 8 nRanks 32 nNodes 8 localRanks 4 localRank 0 MNNVL 0
6: nid007123:108289:117858 [0] NCCL INFO Trees [0] 25/20/28->24->16 [1] 25/20/28->24->16 [2] 25/-1/-1->24->27 [3] 25/-1/-1->24->27 [4] 25/-1/-1->24->20 [5] 25/-1/-1->24->20 [6] 25/-1/-1->24->27 [7] 25/-1/-1->24->27
6: nid007123:108289:117858 [0] NCCL INFO P2P Chunksize set to 131072
5: nid007122:66176:75709 [1] NCCL INFO Trees [0] 22/-1/-1->21->20 [1] 22/-1/-1->21->20 [2] -1/-1/-1->21->20 [3] -1/-1/-1->21->20 [4] 22/-1/-1->21->20 [5] 22/-1/-1->21->20 [6] -1/-1/-1->21->20 [7] -1/-1/-1->21->20
5: nid007122:66176:75709 [1] NCCL INFO P2P Chunksize set to 131072
5: nid007122:66175:75710 [0] NCCL INFO Trees [0] 21/-1/-1->20->24 [1] 21/-1/-1->20->24 [2] 21/-1/-1->20->23 [3] 21/-1/-1->20->23 [4] 21/24/16->20->12 [5] 21/24/16->20->12 [6] 21/-1/-1->20->23 [7] 21/-1/-1->20->23
5: nid007122:66175:75710 [0] NCCL INFO P2P Chunksize set to 131072
2: nid007119:146136:155508 [0] NCCL INFO Trees [0] 9/4/12->8->16 [1] 9/4/12->8->16 [2] 9/-1/-1->8->11 [3] 9/-1/-1->8->11 [4] 9/-1/-1->8->4 [5] 9/-1/-1->8->4 [6] 9/-1/-1->8->11 [7] 9/-1/-1->8->11
2: nid007119:146136:155508 [0] NCCL INFO P2P Chunksize set to 131072
2: nid007119:146139:155510 [3] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
2: nid007119:146139:155510 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
6: nid007123:108292:117855 [3] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
6: nid007123:108292:117855 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
0: nid007116:202901:212608 [1] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
0: nid007116:202901:212608 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
0: nid007116:202900:212607 [0] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
0: nid007116:202900:212607 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
5: nid007122:66177:75711 [2] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
5: nid007122:66177:75711 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
6: nid007123:108290:117857 [1] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
6: nid007123:108290:117857 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
4: nid007121:116202:125703 [1] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
4: nid007121:116202:125703 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
3: nid007120:197038:206393 [2] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
3: nid007120:197038:206393 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
1: nid007117:210406:220332 [1] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
1: nid007117:210406:220332 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
3: nid007120:197037:206391 [1] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
3: nid007120:197037:206391 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
3: nid007120:197036:206394 [0] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
3: nid007120:197036:206394 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
5: nid007122:66175:75710 [0] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
5: nid007122:66175:75710 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
0: nid007116:202900:212607 [0] NCCL INFO CC Off, Multi-GPU CC Off, workFifoBytes 1048576
0: nid007116:202903:212610 [3] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
0: nid007116:202903:212610 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
0: nid007116:202902:212609 [2] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
0: nid007116:202902:212609 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
4: nid007121:116204:125705 [3] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
4: nid007121:116204:125705 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
5: nid007122:66178:75708 [3] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
5: nid007122:66178:75708 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
1: nid007117:210407:220331 [2] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
1: nid007117:210407:220331 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
4: nid007121:116203:125704 [2] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
4: nid007121:116201:125706 [0] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
4: nid007121:116203:125704 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
4: nid007121:116201:125706 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
5: nid007122:66176:75709 [1] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
5: nid007122:66176:75709 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
1: nid007117:210408:220333 [3] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
1: nid007117:210408:220333 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
2: nid007119:146138:155507 [2] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
2: nid007119:146138:155507 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
3: nid007120:197039:206392 [3] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
3: nid007120:197039:206392 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
2: nid007119:146137:155509 [1] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
2: nid007119:146137:155509 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
6: nid007123:108291:117856 [2] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
6: nid007123:108291:117856 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
7: nid007124:126398:135761 [3] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
7: nid007124:126398:135761 [3] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
7: nid007124:126396:135762 [1] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
7: nid007124:126396:135762 [1] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
6: nid007123:108289:117858 [0] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
6: nid007123:108289:117858 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
7: nid007124:126397:135759 [2] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
7: nid007124:126397:135759 [2] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
2: nid007119:146136:155508 [0] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
2: nid007119:146136:155508 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
1: nid007117:210405:220334 [0] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
1: nid007117:210405:220334 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
7: nid007124:126395:135760 [0] NCCL INFO threadThresholds 8/8/64 | 256/8/64 | 512 | 512
7: nid007124:126395:135760 [0] NCCL INFO 8 coll channels, 8 collnet channels, 0 nvls channels, 8 p2p channels, 1 p2p channels per peer
0: nid007116:202900:212607 [0] NCCL INFO ncclCommSplit comm 0x40054ca59aa0 rank 0 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab0f53e270 color 698429859 key 0 commId 0xe064a3d036cb71a - Init COMPLETE
0: nid007116:202900:212607 [0] NCCL INFO Init timings: rank 0 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.00, topo 0.51, graphs 0.02, connections 0.01, rest 0.00)
0: nid007116:202902:212609 [2] NCCL INFO ncclCommSplit comm 0x400554cb2170 rank 2 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaaf1faa2e0 color 698429859 key 2 commId 0xe064a3d036cb71a - Init COMPLETE
0: nid007116:202901:212608 [1] NCCL INFO ncclCommSplit comm 0x400564a5a3b0 rank 1 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaaf4f9b440 color 698429859 key 1 commId 0xe064a3d036cb71a - Init COMPLETE
3: nid007120:197038:206393 [2] NCCL INFO ncclCommSplit comm 0x400534a5bf20 rank 14 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab036888d0 color 698429859 key 14 commId 0xe064a3d036cb71a - Init COMPLETE
3: nid007120:197038:206393 [2] NCCL INFO Init timings: rank 14 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.08, topo 0.44, graphs 0.02, connections 0.01, rest 0.00)
0: nid007116:202901:212608 [1] NCCL INFO Init timings: rank 1 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.00, topo 0.51, graphs 0.02, connections 0.01, rest 0.00)
0: nid007116:202902:212609 [2] NCCL INFO Init timings: rank 2 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.00, topo 0.51, graphs 0.02, connections 0.01, rest 0.00)
0: nid007116:202903:212610 [3] NCCL INFO ncclCommSplit comm 0x400570ca94e0 rank 3 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab18df7db0 color 698429859 key 3 commId 0xe064a3d036cb71a - Init COMPLETE
0: nid007116:202903:212610 [3] NCCL INFO Init timings: rank 3 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.00, topo 0.51, graphs 0.02, connections 0.01, rest 0.00)
5: nid007122:66175:75710 [0] NCCL INFO ncclCommSplit comm 0x400568cb6280 rank 20 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab0041ada0 color 698429859 key 20 commId 0xe064a3d036cb71a - Init COMPLETE
5: nid007122:66175:75710 [0] NCCL INFO Init timings: rank 20 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.05, topo 0.47, graphs 0.02, connections 0.01, rest 0.00)
3: nid007120:197036:206394 [0] NCCL INFO ncclCommSplit comm 0x400568a75b90 rank 12 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaffa2f9f0 color 698429859 key 12 commId 0xe064a3d036cb71a - Init COMPLETE
3: nid007120:197036:206394 [0] NCCL INFO Init timings: rank 12 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.08, topo 0.44, graphs 0.02, connections 0.01, rest 0.00)
3: nid007120:197037:206391 [1] NCCL INFO ncclCommSplit comm 0x400550a5bc80 rank 13 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaafddafcb0 color 698429859 key 13 commId 0xe064a3d036cb71a - Init COMPLETE
3: nid007120:197037:206391 [1] NCCL INFO Init timings: rank 13 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.08, topo 0.44, graphs 0.02, connections 0.01, rest 0.00)
5: nid007122:66177:75711 [2] NCCL INFO ncclCommSplit comm 0x40054ca8abd0 rank 22 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaaacb2d9e10 color 698429859 key 22 commId 0xe064a3d036cb71a - Init COMPLETE
5: nid007122:66178:75708 [3] NCCL INFO ncclCommSplit comm 0x400554a7fd80 rank 23 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab08462130 color 698429859 key 23 commId 0xe064a3d036cb71a - Init COMPLETE
5: nid007122:66176:75709 [1] NCCL INFO ncclCommSplit comm 0x400554a5d250 rank 21 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab1e05ac60 color 698429859 key 21 commId 0xe064a3d036cb71a - Init COMPLETE
5: nid007122:66178:75708 [3] NCCL INFO Init timings: rank 23 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.05, topo 0.47, graphs 0.02, connections 0.01, rest 0.00)
5: nid007122:66177:75711 [2] NCCL INFO Init timings: rank 22 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.05, topo 0.47, graphs 0.02, connections 0.01, rest 0.00)
5: nid007122:66176:75709 [1] NCCL INFO Init timings: rank 21 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.05, topo 0.47, graphs 0.02, connections 0.01, rest 0.00)
3: nid007120:197039:206392 [3] NCCL INFO ncclCommSplit comm 0x400574a83ab0 rank 15 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaaeadc27e0 color 698429859 key 15 commId 0xe064a3d036cb71a - Init COMPLETE
6: nid007123:108292:117855 [3] NCCL INFO ncclCommSplit comm 0x400538ca8050 rank 27 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaaf5620a20 color 698429859 key 27 commId 0xe064a3d036cb71a - Init COMPLETE
6: nid007123:108290:117857 [1] NCCL INFO ncclCommSplit comm 0x400554cb0c00 rank 25 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab083c2070 color 698429859 key 25 commId 0xe064a3d036cb71a - Init COMPLETE
6: nid007123:108292:117855 [3] NCCL INFO Init timings: rank 27 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.10, topo 0.41, graphs 0.02, connections 0.01, rest 0.00)
3: nid007120:197039:206392 [3] NCCL INFO Init timings: rank 15 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.08, topo 0.44, graphs 0.02, connections 0.01, rest 0.00)
6: nid007123:108290:117857 [1] NCCL INFO Init timings: rank 25 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.10, topo 0.42, graphs 0.02, connections 0.01, rest 0.00)
6: nid007123:108291:117856 [2] NCCL INFO ncclCommSplit comm 0x400564cb0bc0 rank 26 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab23840990 color 698429859 key 26 commId 0xe064a3d036cb71a - Init COMPLETE
6: nid007123:108291:117856 [2] NCCL INFO Init timings: rank 26 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.10, topo 0.42, graphs 0.02, connections 0.01, rest 0.00)
6: nid007123:108289:117858 [0] NCCL INFO ncclCommSplit comm 0x400550a5d390 rank 24 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaef3a5bf0 color 698429859 key 24 commId 0xe064a3d036cb71a - Init COMPLETE
0: nid007116:202900:212619 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
6: nid007123:108289:117858 [0] NCCL INFO Init timings: rank 24 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.10, topo 0.42, graphs 0.02, connections 0.01, rest 0.00)
4: nid007121:116204:125705 [3] NCCL INFO ncclCommSplit comm 0x40055ca5cdf0 rank 19 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab22c089c0 color 698429859 key 19 commId 0xe064a3d036cb71a - Init COMPLETE
4: nid007121:116202:125703 [1] NCCL INFO ncclCommSplit comm 0x400534a86a10 rank 17 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab2fe09390 color 698429859 key 17 commId 0xe064a3d036cb71a - Init COMPLETE
4: nid007121:116204:125705 [3] NCCL INFO Init timings: rank 19 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.02, topo 0.49, graphs 0.02, connections 0.01, rest 0.00)
4: nid007121:116202:125703 [1] NCCL INFO Init timings: rank 17 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.02, topo 0.49, graphs 0.02, connections 0.01, rest 0.00)
4: nid007121:116203:125704 [2] NCCL INFO ncclCommSplit comm 0x400550a5d2b0 rank 18 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab08232380 color 698429859 key 18 commId 0xe064a3d036cb71a - Init COMPLETE
4: nid007121:116203:125704 [2] NCCL INFO Init timings: rank 18 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.02, topo 0.49, graphs 0.02, connections 0.01, rest 0.00)
4: nid007121:116201:125706 [0] NCCL INFO ncclCommSplit comm 0x400534cad5d0 rank 16 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaacfcf80d0 color 698429859 key 16 commId 0xe064a3d036cb71a - Init COMPLETE
3: nid007120:197036:206404 [0] NCCL INFO Channel 01/0 : 12[0] -> 13[1] via P2P/CUMEM
4: nid007121:116201:125706 [0] NCCL INFO Init timings: rank 16 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.02, topo 0.49, graphs 0.02, connections 0.01, rest 0.00)
1: nid007117:210408:220333 [3] NCCL INFO ncclCommSplit comm 0x400548ca90f0 rank 7 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaaaf8ea2d90 color 698429859 key 7 commId 0xe064a3d036cb71a - Init COMPLETE
1: nid007117:210406:220332 [1] NCCL INFO ncclCommSplit comm 0x400548a5bdd0 rank 5 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaaaf2c70c20 color 698429859 key 5 commId 0xe064a3d036cb71a - Init COMPLETE
1: nid007117:210408:220333 [3] NCCL INFO Init timings: rank 7 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.01, topo 0.51, graphs 0.02, connections 0.01, rest 0.00)
5: nid007122:66175:75721 [0] NCCL INFO Channel 01/0 : 20[0] -> 21[1] via P2P/CUMEM
1: nid007117:210406:220332 [1] NCCL INFO Init timings: rank 5 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.01, topo 0.51, graphs 0.02, connections 0.01, rest 0.00)
1: nid007117:210407:220331 [2] NCCL INFO ncclCommSplit comm 0x400544ca7a20 rank 6 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab2c3c27a0 color 698429859 key 6 commId 0xe064a3d036cb71a - Init COMPLETE
1: nid007117:210405:220334 [0] NCCL INFO ncclCommSplit comm 0x40054ca55440 rank 4 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab0fcf58a0 color 698429859 key 4 commId 0xe064a3d036cb71a - Init COMPLETE
1: nid007117:210407:220331 [2] NCCL INFO Init timings: rank 6 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.01, topo 0.51, graphs 0.02, connections 0.01, rest 0.00)
2: nid007119:146139:155510 [3] NCCL INFO ncclCommSplit comm 0x400550a5bfe0 rank 11 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab16111160 color 698429859 key 11 commId 0xe064a3d036cb71a - Init COMPLETE
2: nid007119:146139:155510 [3] NCCL INFO Init timings: rank 11 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.04, topo 0.47, graphs 0.02, connections 0.01, rest 0.00)
1: nid007117:210405:220334 [0] NCCL INFO Init timings: rank 4 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.01, topo 0.51, graphs 0.02, connections 0.01, rest 0.00)
2: nid007119:146137:155509 [1] NCCL INFO ncclCommSplit comm 0x40055cca7df0 rank 9 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab1b0b7890 color 698429859 key 9 commId 0xe064a3d036cb71a - Init COMPLETE
2: nid007119:146137:155509 [1] NCCL INFO Init timings: rank 9 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.04, topo 0.47, graphs 0.02, connections 0.01, rest 0.00)
2: nid007119:146138:155507 [2] NCCL INFO ncclCommSplit comm 0x40053ca85450 rank 10 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab072922f0 color 698429859 key 10 commId 0xe064a3d036cb71a - Init COMPLETE
2: nid007119:146138:155507 [2] NCCL INFO Init timings: rank 10 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.04, topo 0.47, graphs 0.02, connections 0.01, rest 0.00)
2: nid007119:146136:155508 [0] NCCL INFO ncclCommSplit comm 0x400530a869c0 rank 8 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab09168e10 color 698429859 key 8 commId 0xe064a3d036cb71a - Init COMPLETE
2: nid007119:146136:155508 [0] NCCL INFO Init timings: rank 8 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.04, topo 0.47, graphs 0.02, connections 0.01, rest 0.00)
6: nid007123:108289:117870 [0] NCCL INFO Channel 00/0 : 24[0] -> 25[1] via P2P/CUMEM
4: nid007121:116201:125718 [0] NCCL INFO Channel 00/0 : 16[0] -> 17[1] via P2P/CUMEM
1: nid007117:210405:220346 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[1] via P2P/CUMEM
0: nid007116:202900:212619 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM
2: nid007119:146136:155522 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[1] via P2P/CUMEM
7: nid007124:126398:135761 [3] NCCL INFO ncclCommSplit comm 0x400574a5d2b0 rank 31 nranks 32 cudaDev 3 nvmlDev 3 busId 3901000 parent 0xaaab12762370 color 698429859 key 31 commId 0xe064a3d036cb71a - Init COMPLETE
7: nid007124:126396:135762 [1] NCCL INFO ncclCommSplit comm 0x400544ca95d0 rank 29 nranks 32 cudaDev 1 nvmlDev 1 busId 1901000 parent 0xaaab39820720 color 698429859 key 29 commId 0xe064a3d036cb71a - Init COMPLETE
7: nid007124:126398:135761 [3] NCCL INFO Init timings: rank 31 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.11, topo 0.41, graphs 0.02, connections 0.01, rest 0.00)
7: nid007124:126396:135762 [1] NCCL INFO Init timings: rank 29 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.11, topo 0.41, graphs 0.02, connections 0.01, rest 0.00)
3: nid007120:197036:206404 [0] NCCL INFO Channel 05/0 : 12[0] -> 13[1] via P2P/CUMEM
7: nid007124:126397:135759 [2] NCCL INFO ncclCommSplit comm 0x40054ccb07a0 rank 30 nranks 32 cudaDev 2 nvmlDev 2 busId 2901000 parent 0xaaab10cea560 color 698429859 key 30 commId 0xe064a3d036cb71a - Init COMPLETE
7: nid007124:126397:135759 [2] NCCL INFO Init timings: rank 30 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.11, topo 0.41, graphs 0.02, connections 0.01, rest 0.00)
7: nid007124:126395:135760 [0] NCCL INFO ncclCommSplit comm 0x400560ca7c30 rank 28 nranks 32 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaef48c330 color 698429859 key 28 commId 0xe064a3d036cb71a - Init COMPLETE
7: nid007124:126395:135760 [0] NCCL INFO Init timings: rank 28 nranks 32 total 0.56 (kernels 0.00, bootstrap 0.01, allgathers 0.11, topo 0.41, graphs 0.02, connections 0.02, rest 0.00)
5: nid007122:66175:75721 [0] NCCL INFO Channel 05/0 : 20[0] -> 21[1] via P2P/CUMEM
6: nid007123:108289:117870 [0] NCCL INFO Channel 04/0 : 24[0] -> 25[1] via P2P/CUMEM
7: nid007124:126395:135774 [0] NCCL INFO Channel 01/0 : 28[0] -> 29[1] via P2P/CUMEM
4: nid007121:116201:125718 [0] NCCL INFO Channel 04/0 : 16[0] -> 17[1] via P2P/CUMEM
0: nid007116:202901:212620 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM
5: nid007122:66177:75720 [2] NCCL INFO Channel 01/0 : 22[2] -> 23[3] via P2P/CUMEM
3: nid007120:197038:206403 [2] NCCL INFO Channel 01/0 : 14[2] -> 15[3] via P2P/CUMEM
0: nid007116:202902:212621 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM
2: nid007119:146136:155522 [0] NCCL INFO Channel 04/0 : 8[0] -> 9[1] via P2P/CUMEM
1: nid007117:210405:220346 [0] NCCL INFO Channel 05/0 : 4[0] -> 5[1] via P2P/CUMEM
4: nid007121:116202:125715 [1] NCCL INFO Channel 00/0 : 17[1] -> 18[2] via P2P/CUMEM
6: nid007123:108290:117867 [1] NCCL INFO Channel 00/0 : 25[1] -> 26[2] via P2P/CUMEM
3: nid007120:197037:206405 [1] NCCL INFO Channel 01/0 : 13[1] -> 14[2] via P2P/CUMEM
6: nid007123:108291:117869 [2] NCCL INFO Channel 00/0 : 26[2] -> 27[3] via P2P/CUMEM
5: nid007122:66176:75723 [1] NCCL INFO Channel 01/0 : 21[1] -> 22[2] via P2P/CUMEM
5: nid007122:66177:75720 [2] NCCL INFO Channel 05/0 : 22[2] -> 23[3] via P2P/CUMEM
0: nid007116:202901:212620 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM
4: nid007121:116203:125716 [2] NCCL INFO Channel 00/0 : 18[2] -> 19[3] via P2P/CUMEM
0: nid007116:202902:212621 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/CUMEM
3: nid007120:197038:206403 [2] NCCL INFO Channel 05/0 : 14[2] -> 15[3] via P2P/CUMEM
6: nid007123:108290:117867 [1] NCCL INFO Channel 04/0 : 25[1] -> 26[2] via P2P/CUMEM
1: nid007117:210406:220343 [1] NCCL INFO Channel 01/0 : 5[1] -> 6[2] via P2P/CUMEM
3: nid007120:197037:206405 [1] NCCL INFO Channel 05/0 : 13[1] -> 14[2] via P2P/CUMEM
6: nid007123:108291:117869 [2] NCCL INFO Channel 04/0 : 26[2] -> 27[3] via P2P/CUMEM
4: nid007121:116202:125715 [1] NCCL INFO Channel 04/0 : 17[1] -> 18[2] via P2P/CUMEM
2: nid007119:146137:155520 [1] NCCL INFO Channel 00/0 : 9[1] -> 10[2] via P2P/CUMEM
5: nid007122:66176:75723 [1] NCCL INFO Channel 05/0 : 21[1] -> 22[2] via P2P/CUMEM
4: nid007121:116203:125716 [2] NCCL INFO Channel 04/0 : 18[2] -> 19[3] via P2P/CUMEM
1: nid007117:210407:220345 [2] NCCL INFO Channel 01/0 : 6[2] -> 7[3] via P2P/CUMEM
2: nid007119:146138:155521 [2] NCCL INFO Channel 00/0 : 10[2] -> 11[3] via P2P/CUMEM
7: nid007124:126395:135774 [0] NCCL INFO Channel 05/0 : 28[0] -> 29[1] via P2P/CUMEM
1: nid007117:210406:220343 [1] NCCL INFO Channel 05/0 : 5[1] -> 6[2] via P2P/CUMEM
2: nid007119:146137:155520 [1] NCCL INFO Channel 04/0 : 9[1] -> 10[2] via P2P/CUMEM
6: nid007123:108289:117870 [0] NCCL INFO Channel 02/0 : 24[0] -> 27[3] via P2P/CUMEM
5: nid007122:66175:75721 [0] NCCL INFO Channel 02/0 : 20[0] -> 23[3] via P2P/CUMEM
2: nid007119:146138:155521 [2] NCCL INFO Channel 04/0 : 10[2] -> 11[3] via P2P/CUMEM
1: nid007117:210407:220345 [2] NCCL INFO Channel 05/0 : 6[2] -> 7[3] via P2P/CUMEM
0: nid007116:202900:212619 [0] NCCL INFO Channel 02/0 : 0[0] -> 3[3] via P2P/CUMEM
3: nid007120:197036:206404 [0] NCCL INFO Channel 02/0 : 12[0] -> 15[3] via P2P/CUMEM
4: nid007121:116201:125718 [0] NCCL INFO Channel 02/0 : 16[0] -> 19[3] via P2P/CUMEM
1: nid007117:210405:220346 [0] NCCL INFO Channel 02/0 : 4[0] -> 7[3] via P2P/CUMEM
0: nid007116:202900:212619 [0] NCCL INFO Channel 03/0 : 0[0] -> 3[3] via P2P/CUMEM
4: nid007121:116201:125718 [0] NCCL INFO Channel 03/0 : 16[0] -> 19[3] via P2P/CUMEM
5: nid007122:66175:75721 [0] NCCL INFO Channel 03/0 : 20[0] -> 23[3] via P2P/CUMEM
6: nid007123:108289:117870 [0] NCCL INFO Channel 03/0 : 24[0] -> 27[3] via P2P/CUMEM
2: nid007119:146136:155522 [0] NCCL INFO Channel 02/0 : 8[0] -> 11[3] via P2P/CUMEM
3: nid007120:197036:206404 [0] NCCL INFO Channel 03/0 : 12[0] -> 15[3] via P2P/CUMEM
7: nid007124:126396:135771 [1] NCCL INFO Channel 01/0 : 29[1] -> 30[2] via P2P/CUMEM
1: nid007117:210405:220346 [0] NCCL INFO Channel 03/0 : 4[0] -> 7[3] via P2P/CUMEM
7: nid007124:126397:135773 [2] NCCL INFO Channel 01/0 : 30[2] -> 31[3] via P2P/CUMEM
4: nid007121:116201:125718 [0] NCCL INFO Channel 06/0 : 16[0] -> 19[3] via P2P/CUMEM
0: nid007116:202900:212619 [0] NCCL INFO Channel 06/0 : 0[0] -> 3[3] via P2P/CUMEM
2: nid007119:146136:155522 [0] NCCL INFO Channel 03/0 : 8[0] -> 11[3] via P2P/CUMEM
5: nid007122:66175:75721 [0] NCCL INFO Channel 06/0 : 20[0] -> 23[3] via P2P/CUMEM
7: nid007124:126396:135771 [1] NCCL INFO Channel 05/0 : 29[1] -> 30[2] via P2P/CUMEM
6: nid007123:108289:117870 [0] NCCL INFO Channel 06/0 : 24[0] -> 27[3] via P2P/CUMEM
3: nid007120:197036:206404 [0] NCCL INFO Channel 06/0 : 12[0] -> 15[3] via P2P/CUMEM
7: nid007124:126397:135773 [2] NCCL INFO Channel 05/0 : 30[2] -> 31[3] via P2P/CUMEM
6: nid007123:108290:117867 [1] NCCL INFO Channel 03/0 : 21[1] -> 25[1] [receive] via NET/AWS Libfabric/1
0: nid007116:202900:212619 [0] NCCL INFO Channel 07/0 : 0[0] -> 3[3] via P2P/CUMEM
4: nid007121:116201:125718 [0] NCCL INFO Channel 07/0 : 16[0] -> 19[3] via P2P/CUMEM
5: nid007122:66177:75720 [2] NCCL INFO Channel 03/0 : 18[2] -> 22[2] [receive] via NET/AWS Libfabric/2
3: nid007120:197038:206403 [2] NCCL INFO Channel 03/0 : 10[2] -> 14[2] [receive] via NET/AWS Libfabric/2
2: nid007119:146136:155522 [0] NCCL INFO Channel 06/0 : 8[0] -> 11[3] via P2P/CUMEM
6: nid007123:108291:117869 [2] NCCL INFO Channel 02/0 : 22[2] -> 26[2] [receive] via NET/AWS Libfabric/2
1: nid007117:210405:220346 [0] NCCL INFO Channel 06/0 : 4[0] -> 7[3] via P2P/CUMEM
0: nid007116:202901:212620 [1] NCCL INFO Channel 03/0 : 29[1] -> 1[1] [receive] via NET/AWS Libfabric/1
3: nid007120:197037:206405 [1] NCCL INFO Channel 02/0 : 9[1] -> 13[1] [receive] via NET/AWS Libfabric/1
6: nid007123:108290:117867 [1] NCCL INFO Channel 07/0 : 21[1] -> 25[1] [receive] via NET/AWS Libfabric/1
4: nid007121:116202:125715 [1] NCCL INFO Channel 03/0 : 13[1] -> 17[1] [receive] via NET/AWS Libfabric/1
5: nid007122:66175:75721 [0] NCCL INFO Channel 07/0 : 20[0] -> 23[3] via P2P/CUMEM
3: nid007120:197038:206403 [2] NCCL INFO Channel 07/0 : 10[2] -> 14[2] [receive] via NET/AWS Libfabric/2
5: nid007122:66177:75720 [2] NCCL INFO Channel 07/0 : 18[2] -> 22[2] [receive] via NET/AWS Libfabric/2
6: nid007123:108291:117869 [2] NCCL INFO Channel 06/0 : 22[2] -> 26[2] [receive] via NET/AWS Libfabric/2
5: nid007122:66176:75723 [1] NCCL INFO Channel 02/0 : 17[1] -> 21[1] [receive] via NET/AWS Libfabric/1
6: nid007123:108290:117867 [1] NCCL INFO Channel 02/0 : 25[1] -> 29[1] [send] via NET/AWS Libfabric/1
3: nid007120:197037:206405 [1] NCCL INFO Channel 06/0 : 9[1] -> 13[1] [receive] via NET/AWS Libfabric/1
4: nid007121:116203:125716 [2] NCCL INFO Channel 02/0 : 14[2] -> 18[2] [receive] via NET/AWS Libfabric/2
0: nid007116:202901:212620 [1] NCCL INFO Channel 07/0 : 29[1] -> 1[1] [receive] via NET/AWS Libfabric/1
4: nid007121:116202:125715 [1] NCCL INFO Channel 07/0 : 13[1] -> 17[1] [receive] via NET/AWS Libfabric/1
5: nid007122:66177:75720 [2] NCCL INFO Channel 02/0 : 22[2] -> 26[2] [send] via NET/AWS Libfabric/2
3: nid007120:197038:206403 [2] NCCL INFO Channel 02/0 : 14[2] -> 18[2] [send] via NET/AWS Libfabric/2
6: nid007123:108291:117869 [2] NCCL INFO Channel 03/0 : 26[2] -> 30[2] [send] via NET/AWS Libfabric/2
0: nid007116:202902:212621 [2] NCCL INFO Channel 02/0 : 30[2] -> 2[2] [receive] via NET/AWS Libfabric/2
5: nid007122:66176:75723 [1] NCCL INFO Channel 06/0 : 17[1] -> 21[1] [receive] via NET/AWS Libfabric/1
3: nid007120:197037:206405 [1] NCCL INFO Channel 03/0 : 13[1] -> 17[1] [send] via NET/AWS Libfabric/1
6: nid007123:108290:117867 [1] NCCL INFO Channel 06/0 : 25[1] -> 29[1] [send] via NET/AWS Libfabric/1
0: nid007116:202901:212620 [1] NCCL INFO Channel 02/0 : 1[1] -> 5[1] [send] via NET/AWS Libfabric/1
4: nid007121:116203:125716 [2] NCCL INFO Channel 06/0 : 14[2] -> 18[2] [receive] via NET/AWS Libfabric/2
3: nid007120:197036:206404 [0] NCCL INFO Channel 07/0 : 12[0] -> 15[3] via P2P/CUMEM
3: nid007120:197038:206403 [2] NCCL INFO Channel 06/0 : 14[2] -> 18[2] [send] via NET/AWS Libfabric/2
4: nid007121:116202:125715 [1] NCCL INFO Channel 02/0 : 17[1] -> 21[1] [send] via NET/AWS Libfabric/1
5: nid007122:66177:75720 [2] NCCL INFO Channel 06/0 : 22[2] -> 26[2] [send] via NET/AWS Libfabric/2
6: nid007123:108291:117869 [2] NCCL INFO Channel 07/0 : 26[2] -> 30[2] [send] via NET/AWS Libfabric/2
0: nid007116:202902:212621 [2] NCCL INFO Channel 06/0 : 30[2] -> 2[2] [receive] via NET/AWS Libfabric/2
2: nid007119:146136:155522 [0] NCCL INFO Channel 07/0 : 8[0] -> 11[3] via P2P/CUMEM
1: nid007117:210406:220343 [1] NCCL INFO Channel 02/0 : 1[1] -> 5[1] [receive] via NET/AWS Libfabric/1
0: nid007116:202901:212620 [1] NCCL INFO Channel 06/0 : 1[1] -> 5[1] [send] via NET/AWS Libfabric/1
4: nid007121:116203:125716 [2] NCCL INFO Channel 03/0 : 18[2] -> 22[2] [send] via NET/AWS Libfabric/2
5: nid007122:66176:75723 [1] NCCL INFO Channel 03/0 : 21[1] -> 25[1] [send] via NET/AWS Libfabric/1
3: nid007120:197037:206405 [1] NCCL INFO Channel 07/0 : 13[1] -> 17[1] [send] via NET/AWS Libfabric/1
1: nid007117:210405:220346 [0] NCCL INFO Channel 07/0 : 4[0] -> 7[3] via P2P/CUMEM
0: nid007116:202902:212621 [2] NCCL INFO Channel 03/0 : 2[2] -> 6[2] [send] via NET/AWS Libfabric/2
4: nid007121:116202:125715 [1] NCCL INFO Channel 06/0 : 17[1] -> 21[1] [send] via NET/AWS Libfabric/1
7: nid007124:126395:135774 [0] NCCL INFO Channel 02/0 : 28[0] -> 31[3] via P2P/CUMEM
1: nid007117:210406:220343 [1] NCCL INFO Channel 06/0 : 1[1] -> 5[1] [receive] via NET/AWS Libfabric/1
2: nid007119:146137:155520 [1] NCCL INFO Channel 03/0 : 5[1] -> 9[1] [receive] via NET/AWS Libfabric/1
4: nid007121:116203:125716 [2] NCCL INFO Channel 07/0 : 18[2] -> 22[2] [send] via NET/AWS Libfabric/2
5: nid007122:66176:75723 [1] NCCL INFO Channel 07/0 : 21[1] -> 25[1] [send] via NET/AWS Libfabric/1
1: nid007117:210406:220343 [1] NCCL INFO Channel 03/0 : 5[1] -> 9[1] [send] via NET/AWS Libfabric/1
0: nid007116:202902:212621 [2] NCCL INFO Channel 07/0 : 2[2] -> 6[2] [send] via NET/AWS Libfabric/2
1: nid007117:210407:220345 [2] NCCL INFO Channel 03/0 : 2[2] -> 6[2] [receive] via NET/AWS Libfabric/2
1: nid007117:210406:220343 [1] NCCL INFO Channel 07/0 : 5[1] -> 9[1] [send] via NET/AWS Libfabric/1
2: nid007119:146137:155520 [1] NCCL INFO Channel 07/0 : 5[1] -> 9[1] [receive] via NET/AWS Libfabric/1
2: nid007119:146138:155521 [2] NCCL INFO Channel 02/0 : 6[2] -> 10[2] [receive] via NET/AWS Libfabric/2
6: nid007123:108289:117870 [0] NCCL INFO Channel 07/0 : 24[0] -> 27[3] via P2P/CUMEM
2: nid007119:146137:155520 [1] NCCL INFO Channel 02/0 : 9[1] -> 13[1] [send] via NET/AWS Libfabric/1
1: nid007117:210407:220345 [2] NCCL INFO Channel 07/0 : 2[2] -> 6[2] [receive] via NET/AWS Libfabric/2
2: nid007119:146138:155521 [2] NCCL INFO Channel 06/0 : 6[2] -> 10[2] [receive] via NET/AWS Libfabric/2
1: nid007117:210407:220345 [2] NCCL INFO Channel 02/0 : 6[2] -> 10[2] [send] via NET/AWS Libfabric/2
2: nid007119:146137:155520 [1] NCCL INFO Channel 06/0 : 9[1] -> 13[1] [send] via NET/AWS Libfabric/1
2: nid007119:146138:155521 [2] NCCL INFO Channel 03/0 : 10[2] -> 14[2] [send] via NET/AWS Libfabric/2
7: nid007124:126395:135774 [0] NCCL INFO Channel 03/0 : 28[0] -> 31[3] via P2P/CUMEM
1: nid007117:210407:220345 [2] NCCL INFO Channel 06/0 : 6[2] -> 10[2] [send] via NET/AWS Libfabric/2
4: nid007121:116203:125716 [2] NCCL INFO Channel 02/0 : 18[2] -> 16[0] via P2P/CUMEM
2: nid007119:146138:155521 [2] NCCL INFO Channel 07/0 : 10[2] -> 14[2] [send] via NET/AWS Libfabric/2
5: nid007122:66177:75720 [2] NCCL INFO Channel 03/0 : 22[2] -> 20[0] via P2P/CUMEM
7: nid007124:126395:135774 [0] NCCL INFO Channel 06/0 : 28[0] -> 31[3] via P2P/CUMEM
3: nid007120:197038:206403 [2] NCCL INFO Channel 03/0 : 14[2] -> 12[0] via P2P/CUMEM
7: nid007124:126397:135773 [2] NCCL INFO Channel 03/0 : 26[2] -> 30[2] [receive] via NET/AWS Libfabric/2
1: nid007117:210407:220345 [2] NCCL INFO Channel 03/0 : 6[2] -> 4[0] via P2P/CUMEM
7: nid007124:126396:135771 [1] NCCL INFO Channel 02/0 : 25[1] -> 29[1] [receive] via NET/AWS Libfabric/1
2: nid007119:146138:155521 [2] NCCL INFO Channel 02/0 : 10[2] -> 8[0] via P2P/CUMEM
4: nid007121:116203:125716 [2] NCCL INFO Channel 06/0 : 18[2] -> 16[0] via P2P/CUMEM
5: nid007122:66177:75720 [2] NCCL INFO Channel 07/0 : 22[2] -> 20[0] via P2P/CUMEM
7: nid007124:126397:135773 [2] NCCL INFO Channel 07/0 : 26[2] -> 30[2] [receive] via NET/AWS Libfabric/2
7: nid007124:126396:135771 [1] NCCL INFO Channel 06/0 : 25[1] -> 29[1] [receive] via NET/AWS Libfabric/1
0: nid007116:202900:212619 [0] NCCL INFO Channel 00/0 : 28[0] -> 0[0] [receive] via NET/AWS Libfabric/0
7: nid007124:126397:135773 [2] NCCL INFO Channel 02/0 : 30[2] -> 2[2] [send] via NET/AWS Libfabric/2
7: nid007124:126395:135774 [0] NCCL INFO Channel 07/0 : 28[0] -> 31[3] via P2P/CUMEM
3: nid007120:197036:206404 [0] NCCL INFO Channel 01/0 : 8[0] -> 12[0] [receive] via NET/AWS Libfabric/0
0: nid007116:202900:212619 [0] NCCL INFO Channel 04/0 : 28[0] -> 0[0] [receive] via NET/AWS Libfabric/0
0: nid007116:202903:212622 [3] NCCL INFO Channel 01/0 : 31[3] -> 3[3] [receive] via NET/AWS Libfabric/3
7: nid007124:126396:135771 [1] NCCL INFO Channel 03/0 : 29[1] -> 1[1] [send] via NET/AWS Libfabric/1
7: nid007124:126397:135773 [2] NCCL INFO Channel 06/0 : 30[2] -> 2[2] [send] via NET/AWS Libfabric/2
4: nid007121:116201:125718 [0] NCCL INFO Channel 00/0 : 12[0] -> 16[0] [receive] via NET/AWS Libfabric/0
3: nid007120:197039:206406 [3] NCCL INFO Channel 00/0 : 11[3] -> 15[3] [receive] via NET/AWS Libfabric/3
4: nid007121:116204:125717 [3] NCCL INFO Channel 01/0 : 15[3] -> 19[3] [receive] via NET/AWS Libfabric/3
3: nid007120:197036:206404 [0] NCCL INFO Channel 05/0 : 8[0] -> 12[0] [receive] via NET/AWS Libfabric/0
2: nid007119:146138:155521 [2] NCCL INFO Channel 06/0 : 10[2] -> 8[0] via P2P/CUMEM
7: nid007124:126396:135771 [1] NCCL INFO Channel 07/0 : 29[1] -> 1[1] [send] via NET/AWS Libfabric/1
3: nid007120:197039:206406 [3] NCCL INFO Channel 04/0 : 11[3] -> 15[3] [receive] via NET/AWS Libfabric/3
3: nid007120:197038:206403 [2] NCCL INFO Channel 07/0 : 14[2] -> 12[0] via P2P/CUMEM
0: nid007116:202900:212619 [0] NCCL INFO Channel 01/0 : 0[0] -> 4[0] [send] via NET/AWS Libfabric/0
0: nid007116:202903:212622 [3] NCCL INFO Channel 05/0 : 31[3] -> 3[3] [receive] via NET/AWS Libfabric/3
2: nid007119:146139:155519 [3] NCCL INFO Channel 01/0 : 7[3] -> 11[3] [receive] via NET/AWS Libfabric/3
1: nid007117:210407:220345 [2] NCCL INFO Channel 07/0 : 6[2] -> 4[0] via P2P/CUMEM
4: nid007121:116201:125718 [0] NCCL INFO Channel 04/0 : 12[0] -> 16[0] [receive] via NET/AWS Libfabric/0
4: nid007121:116204:125717 [3] NCCL INFO Channel 05/0 : 15[3] -> 19[3] [receive] via NET/AWS Libfabric/3
3: nid007120:197036:206404 [0] NCCL INFO Channel 00/0 : 12[0] -> 16[0] [send] via NET/AWS Libfabric/0
3: nid007120:197039:206406 [3] NCCL INFO Channel 01/0 : 15[3] -> 19[3] [send] via NET/AWS Libfabric/3
2: nid007119:146139:155519 [3] NCCL INFO Channel 05/0 : 7[3] -> 11[3] [receive] via NET/AWS Libfabric/3
2: nid007119:146136:155522 [0] NCCL INFO Channel 00/0 : 4[0] -> 8[0] [receive] via NET/AWS Libfabric/0
3: nid007120:197036:206404 [0] NCCL INFO Channel 04/0 : 12[0] -> 16[0] [send] via NET/AWS Libfabric/0
6: nid007123:108291:117869 [2] NCCL INFO Channel 02/0 : 26[2] -> 24[0] via P2P/CUMEM
6: nid007123:108292:117868 [3] NCCL INFO Channel 01/0 : 23[3] -> 27[3] [receive] via NET/AWS Libfabric/3
0: nid007116:202900:212619 [0] NCCL INFO Channel 05/0 : 0[0] -> 4[0] [send] via NET/AWS Libfabric/0
0: nid007116:202903:212622 [3] NCCL INFO Channel 00/0 : 3[3] -> 7[3] [send] via NET/AWS Libfabric/3
4: nid007121:116201:125718 [0] NCCL INFO Channel 01/0 : 16[0] -> 20[0] [send] via NET/AWS Libfabric/0
4: nid007121:116204:125717 [3] NCCL INFO Channel 00/0 : 19[3] -> 23[3] [send] via NET/AWS Libfabric/3
3: nid007120:197039:206406 [3] NCCL INFO Channel 05/0 : 15[3] -> 19[3] [send] via NET/AWS Libfabric/3
5: nid007122:66178:75722 [3] NCCL INFO Channel 00/0 : 19[3] -> 23[3] [receive] via NET/AWS Libfabric/3
0: nid007116:202902:212621 [2] NCCL INFO Channel 02/0 : 2[2] -> 0[0] via P2P/CUMEM
0: nid007116:202903:212622 [3] NCCL INFO Channel 04/0 : 3[3] -> 7[3] [send] via NET/AWS Libfabric/3
6: nid007123:108292:117868 [3] NCCL INFO Channel 05/0 : 23[3] -> 27[3] [receive] via NET/AWS Libfabric/3
2: nid007119:146139:155519 [3] NCCL INFO Channel 00/0 : 11[3] -> 15[3] [send] via NET/AWS Libfabric/3
2: nid007119:146136:155522 [0] NCCL INFO Channel 04/0 : 4[0] -> 8[0] [receive] via NET/AWS Libfabric/0
5: nid007122:66175:75721 [0] NCCL INFO Channel 01/0 : 16[0] -> 20[0] [receive] via NET/AWS Libfabric/0
7: nid007124:126397:135773 [2] NCCL INFO Channel 03/0 : 30[2] -> 28[0] via P2P/CUMEM
5: nid007122:66178:75722 [3] NCCL INFO Channel 04/0 : 19[3] -> 23[3] [receive] via NET/AWS Libfabric/3
6: nid007123:108292:117868 [3] NCCL INFO Channel 00/0 : 27[3] -> 31[3] [send] via NET/AWS Libfabric/3
4: nid007121:116201:125718 [0] NCCL INFO Channel 05/0 : 16[0] -> 20[0] [send] via NET/AWS Libfabric/0
4: nid007121:116204:125717 [3] NCCL INFO Channel 04/0 : 19[3] -> 23[3] [send] via NET/AWS Libfabric/3
2: nid007119:146139:155519 [3] NCCL INFO Channel 04/0 : 11[3] -> 15[3] [send] via NET/AWS Libfabric/3
2: nid007119:146136:155522 [0] NCCL INFO Channel 01/0 : 8[0] -> 12[0] [send] via NET/AWS Libfabric/0
6: nid007123:108292:117868 [3] NCCL INFO Channel 04/0 : 27[3] -> 31[3] [send] via NET/AWS Libfabric/3
5: nid007122:66175:75721 [0] NCCL INFO Channel 05/0 : 16[0] -> 20[0] [receive] via NET/AWS Libfabric/0
5: nid007122:66178:75722 [3] NCCL INFO Channel 01/0 : 23[3] -> 27[3] [send] via NET/AWS Libfabric/3
2: nid007119:146136:155522 [0] NCCL INFO Channel 05/0 : 8[0] -> 12[0] [send] via NET/AWS Libfabric/0
1: nid007117:210408:220344 [3] NCCL INFO Channel 00/0 : 3[3] -> 7[3] [receive] via NET/AWS Libfabric/3
5: nid007122:66178:75722 [3] NCCL INFO Channel 05/0 : 23[3] -> 27[3] [send] via NET/AWS Libfabric/3
5: nid007122:66175:75721 [0] NCCL INFO Channel 00/0 : 20[0] -> 24[0] [send] via NET/AWS Libfabric/0
6: nid007123:108291:117869 [2] NCCL INFO Channel 06/0 : 26[2] -> 24[0] via P2P/CUMEM
1: nid007117:210408:220344 [3] NCCL INFO Channel 04/0 : 3[3] -> 7[3] [receive] via NET/AWS Libfabric/3
6: nid007123:108289:117870 [0] NCCL INFO Channel 00/0 : 20[0] -> 24[0] [receive] via NET/AWS Libfabric/0
1: nid007117:210405:220346 [0] NCCL INFO Channel 01/0 : 0[0] -> 4[0] [receive] via NET/AWS Libfabric/0
4: nid007121:116204:125717 [3] NCCL INFO Channel 02/0 : 19[3] -> 17[1] via P2P/CUMEM
3: nid007120:197039:206406 [3] NCCL INFO Channel 03/0 : 15[3] -> 13[1] via P2P/CUMEM
1: nid007117:210408:220344 [3] NCCL INFO Channel 01/0 : 7[3] -> 11[3] [send] via NET/AWS Libfabric/3
0: nid007116:202902:212621 [2] NCCL INFO Channel 06/0 : 2[2] -> 0[0] via P2P/CUMEM
6: nid007123:108289:117870 [0] NCCL INFO Channel 04/0 : 20[0] -> 24[0] [receive] via NET/AWS Libfabric/0
1: nid007117:210405:220346 [0] NCCL INFO Channel 05/0 : 0[0] -> 4[0] [receive] via NET/AWS Libfabric/0
5: nid007122:66178:75722 [3] NCCL INFO Channel 03/0 : 23[3] -> 21[1] via P2P/CUMEM
7: nid007124:126397:135773 [2] NCCL INFO Channel 07/0 : 30[2] -> 28[0] via P2P/CUMEM
1: nid007117:210408:220344 [3] NCCL INFO Channel 05/0 : 7[3] -> 11[3] [send] via NET/AWS Libfabric/3
6: nid007123:108289:117870 [0] NCCL INFO Channel 01/0 : 24[0] -> 28[0] [send] via NET/AWS Libfabric/0
5: nid007122:66175:75721 [0] NCCL INFO Channel 04/0 : 20[0] -> 24[0] [send] via NET/AWS Libfabric/0
1: nid007117:210405:220346 [0] NCCL INFO Channel 00/0 : 4[0] -> 8[0] [send] via NET/AWS Libfabric/0
6: nid007123:108289:117870 [0] NCCL INFO Channel 05/0 : 24[0] -> 28[0] [send] via NET/AWS Libfabric/0
4: nid007121:116204:125717 [3] NCCL INFO Channel 06/0 : 19[3] -> 17[1] via P2P/CUMEM
1: nid007117:210405:220346 [0] NCCL INFO Channel 04/0 : 4[0] -> 8[0] [send] via NET/AWS Libfabric/0
5: nid007122:66178:75722 [3] NCCL INFO Channel 07/0 : 23[3] -> 21[1] via P2P/CUMEM
3: nid007120:197039:206406 [3] NCCL INFO Channel 07/0 : 15[3] -> 13[1] via P2P/CUMEM
2: nid007119:146139:155519 [3] NCCL INFO Channel 02/0 : 11[3] -> 9[1] via P2P/CUMEM
1: nid007117:210408:220344 [3] NCCL INFO Channel 03/0 : 7[3] -> 5[1] via P2P/CUMEM
4: nid007121:116204:125717 [3] NCCL INFO Channel 01/0 : 19[3] -> 18[2] via P2P/CUMEM
2: nid007119:146139:155519 [3] NCCL INFO Channel 06/0 : 11[3] -> 9[1] via P2P/CUMEM
5: nid007122:66178:75722 [3] NCCL INFO Channel 00/0 : 23[3] -> 22[2] via P2P/CUMEM
3: nid007120:197039:206406 [3] NCCL INFO Channel 00/0 : 15[3] -> 14[2] via P2P/CUMEM
1: nid007117:210408:220344 [3] NCCL INFO Channel 07/0 : 7[3] -> 5[1] via P2P/CUMEM
7: nid007124:126395:135774 [0] NCCL INFO Channel 01/0 : 24[0] -> 28[0] [receive] via NET/AWS Libfabric/0
7: nid007124:126398:135772 [3] NCCL INFO Channel 00/0 : 27[3] -> 31[3] [receive] via NET/AWS Libfabric/3
2: nid007119:146139:155519 [3] NCCL INFO Channel 01/0 : 11[3] -> 10[2] via P2P/CUMEM
4: nid007121:116204:125717 [3] NCCL INFO Channel 03/0 : 19[3] -> 18[2] via P2P/CUMEM
7: nid007124:126398:135772 [3] NCCL INFO Channel 04/0 : 27[3] -> 31[3] [receive] via NET/AWS Libfabric/3
7: nid007124:126395:135774 [0] NCCL INFO Channel 05/0 : 24[0] -> 28[0] [receive] via NET/AWS Libfabric/0
5: nid007122:66178:75722 [3] NCCL INFO Channel 02/0 : 23[3] -> 22[2] via P2P/CUMEM
7: nid007124:126398:135772 [3] NCCL INFO Channel 01/0 : 31[3] -> 3[3] [send] via NET/AWS Libfabric/3
7: nid007124:126395:135774 [0] NCCL INFO Channel 00/0 : 28[0] -> 0[0] [send] via NET/AWS Libfabric/0
1: nid007117:210408:220344 [3] NCCL INFO Channel 00/0 : 7[3] -> 6[2] via P2P/CUMEM
7: nid007124:126398:135772 [3] NCCL INFO Channel 05/0 : 31[3] -> 3[3] [send] via NET/AWS Libfabric/3
3: nid007120:197039:206406 [3] NCCL INFO Channel 02/0 : 15[3] -> 14[2] via P2P/CUMEM
6: nid007123:108292:117868 [3] NCCL INFO Channel 02/0 : 27[3] -> 25[1] via P2P/CUMEM
0: nid007116:202903:212622 [3] NCCL INFO Channel 02/0 : 3[3] -> 1[1] via P2P/CUMEM
7: nid007124:126398:135772 [3] NCCL INFO Channel 03/0 : 31[3] -> 29[1] via P2P/CUMEM
2: nid007119:146139:155519 [3] NCCL INFO Channel 03/0 : 11[3] -> 10[2] via P2P/CUMEM
4: nid007121:116202:125715 [1] NCCL INFO Channel 01/0 : 17[1] -> 16[0] via P2P/CUMEM
7: nid007124:126395:135774 [0] NCCL INFO Channel 04/0 : 28[0] -> 0[0] [send] via NET/AWS Libfabric/0
6: nid007123:108292:117868 [3] NCCL INFO Channel 06/0 : 27[3] -> 25[1] via P2P/CUMEM
4: nid007121:116204:125717 [3] NCCL INFO Channel 05/0 : 19[3] -> 18[2] via P2P/CUMEM
5: nid007122:66176:75723 [1] NCCL INFO Channel 00/0 : 21[1] -> 20[0] via P2P/CUMEM
0: nid007116:202903:212622 [3] NCCL INFO Channel 06/0 : 3[3] -> 1[1] via P2P/CUMEM
7: nid007124:126398:135772 [3] NCCL INFO Channel 07/0 : 31[3] -> 29[1] via P2P/CUMEM
5: nid007122:66178:75722 [3] NCCL INFO Channel 04/0 : 23[3] -> 22[2] via P2P/CUMEM
3: nid007120:197037:206405 [1] NCCL INFO Channel 00/0 : 13[1] -> 12[0] via P2P/CUMEM
1: nid007117:210408:220344 [3] NCCL INFO Channel 02/0 : 7[3] -> 6[2] via P2P/CUMEM
3: nid007120:197039:206406 [3] NCCL INFO Channel 04/0 : 15[3] -> 14[2] via P2P/CUMEM
6: nid007123:108292:117868 [3] NCCL INFO Channel 01/0 : 27[3] -> 26[2] via P2P/CUMEM
7: nid007124:126398:135772 [3] NCCL INFO Channel 00/0 : 31[3] -> 30[2] via P2P/CUMEM
4: nid007121:116202:125715 [1] NCCL INFO Channel 03/0 : 17[1] -> 16[0] via P2P/CUMEM
0: nid007116:202903:212622 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM
2: nid007119:146139:155519 [3] NCCL INFO Channel 05/0 : 11[3] -> 10[2] via P2P/CUMEM
5: nid007122:66176:75723 [1] NCCL INFO Channel 02/0 : 21[1] -> 20[0] via P2P/CUMEM
4: nid007121:116204:125717 [3] NCCL INFO Channel 07/0 : 19[3] -> 18[2] via P2P/CUMEM
2: nid007119:146137:155520 [1] NCCL INFO Channel 01/0 : 9[1] -> 8[0] via P2P/CUMEM
5: nid007122:66178:75722 [3] NCCL INFO Channel 06/0 : 23[3] -> 22[2] via P2P/CUMEM
1: nid007117:210406:220343 [1] NCCL INFO Channel 00/0 : 5[1] -> 4[0] via P2P/CUMEM
6: nid007123:108292:117868 [3] NCCL INFO Channel 03/0 : 27[3] -> 26[2] via P2P/CUMEM
3: nid007120:197037:206405 [1] NCCL INFO Channel 02/0 : 13[1] -> 12[0] via P2P/CUMEM
1: nid007117:210408:220344 [3] NCCL INFO Channel 04/0 : 7[3] -> 6[2] via P2P/CUMEM
3: nid007120:197039:206406 [3] NCCL INFO Channel 06/0 : 15[3] -> 14[2] via P2P/CUMEM
7: nid007124:126398:135772 [3] NCCL INFO Channel 02/0 : 31[3] -> 30[2] via P2P/CUMEM
0: nid007116:202903:212622 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM
4: nid007121:116202:125715 [1] NCCL INFO Channel 05/0 : 17[1] -> 16[0] via P2P/CUMEM
5: nid007122:66176:75723 [1] NCCL INFO Channel 04/0 : 21[1] -> 20[0] via P2P/CUMEM
2: nid007119:146139:155519 [3] NCCL INFO Channel 07/0 : 11[3] -> 10[2] via P2P/CUMEM
2: nid007119:146137:155520 [1] NCCL INFO Channel 03/0 : 9[1] -> 8[0] via P2P/CUMEM
1: nid007117:210406:220343 [1] NCCL INFO Channel 02/0 : 5[1] -> 4[0] via P2P/CUMEM
6: nid007123:108290:117867 [1] NCCL INFO Channel 01/0 : 25[1] -> 24[0] via P2P/CUMEM
6: nid007123:108292:117868 [3] NCCL INFO Channel 05/0 : 27[3] -> 26[2] via P2P/CUMEM
7: nid007124:126396:135771 [1] NCCL INFO Channel 00/0 : 29[1] -> 28[0] via P2P/CUMEM
7: nid007124:126398:135772 [3] NCCL INFO Channel 04/0 : 31[3] -> 30[2] via P2P/CUMEM
3: nid007120:197037:206405 [1] NCCL INFO Channel 04/0 : 13[1] -> 12[0] via P2P/CUMEM
1: nid007117:210408:220344 [3] NCCL INFO Channel 06/0 : 7[3] -> 6[2] via P2P/CUMEM
4: nid007121:116203:125716 [2] NCCL INFO Channel 01/0 : 18[2] -> 17[1] via P2P/CUMEM
4: nid007121:116202:125715 [1] NCCL INFO Channel 07/0 : 17[1] -> 16[0] via P2P/CUMEM
3: nid007120:197038:206403 [2] NCCL INFO Channel 00/0 : 14[2] -> 13[1] via P2P/CUMEM
5: nid007122:66176:75723 [1] NCCL INFO Channel 06/0 : 21[1] -> 20[0] via P2P/CUMEM
0: nid007116:202901:212620 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM
0: nid007116:202903:212622 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/CUMEM
3: nid007120:197037:206405 [1] NCCL INFO Channel 06/0 : 13[1] -> 12[0] via P2P/CUMEM
3: nid007120:197038:206403 [2] NCCL INFO Channel 04/0 : 14[2] -> 13[1] via P2P/CUMEM
4: nid007121:116203:125716 [2] NCCL INFO Channel 05/0 : 18[2] -> 17[1] via P2P/CUMEM
1: nid007117:210406:220343 [1] NCCL INFO Channel 04/0 : 5[1] -> 4[0] via P2P/CUMEM
2: nid007119:146137:155520 [1] NCCL INFO Channel 05/0 : 9[1] -> 8[0] via P2P/CUMEM
6: nid007123:108290:117867 [1] NCCL INFO Channel 03/0 : 25[1] -> 24[0] via P2P/CUMEM
6: nid007123:108292:117868 [3] NCCL INFO Channel 07/0 : 27[3] -> 26[2] via P2P/CUMEM
7: nid007124:126396:135771 [1] NCCL INFO Channel 02/0 : 29[1] -> 28[0] via P2P/CUMEM
5: nid007122:66177:75720 [2] NCCL INFO Channel 00/0 : 22[2] -> 21[1] via P2P/CUMEM
2: nid007119:146138:155521 [2] NCCL INFO Channel 01/0 : 10[2] -> 9[1] via P2P/CUMEM
7: nid007124:126398:135772 [3] NCCL INFO Channel 06/0 : 31[3] -> 30[2] via P2P/CUMEM
2: nid007119:146137:155520 [1] NCCL INFO Channel 07/0 : 9[1] -> 8[0] via P2P/CUMEM
5: nid007122:66177:75720 [2] NCCL INFO Channel 04/0 : 22[2] -> 21[1] via P2P/CUMEM
0: nid007116:202901:212620 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM
1: nid007117:210407:220345 [2] NCCL INFO Channel 00/0 : 6[2] -> 5[1] via P2P/CUMEM
1: nid007117:210406:220343 [1] NCCL INFO Channel 06/0 : 5[1] -> 4[0] via P2P/CUMEM
2: nid007119:146138:155521 [2] NCCL INFO Channel 05/0 : 10[2] -> 9[1] via P2P/CUMEM
0: nid007116:202903:212622 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/CUMEM
7: nid007124:126396:135771 [1] NCCL INFO Channel 04/0 : 29[1] -> 28[0] via P2P/CUMEM
6: nid007123:108290:117867 [1] NCCL INFO Channel 05/0 : 25[1] -> 24[0] via P2P/CUMEM
1: nid007117:210407:220345 [2] NCCL INFO Channel 04/0 : 6[2] -> 5[1] via P2P/CUMEM
0: nid007116:202901:212620 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM
7: nid007124:126396:135771 [1] NCCL INFO Channel 06/0 : 29[1] -> 28[0] via P2P/CUMEM
6: nid007123:108290:117867 [1] NCCL INFO Channel 07/0 : 25[1] -> 24[0] via P2P/CUMEM
6: nid007123:108291:117869 [2] NCCL INFO Channel 01/0 : 26[2] -> 25[1] via P2P/CUMEM
0: nid007116:202901:212620 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/CUMEM
6: nid007123:108291:117869 [2] NCCL INFO Channel 05/0 : 26[2] -> 25[1] via P2P/CUMEM
0: nid007116:202902:212621 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM
7: nid007124:126397:135773 [2] NCCL INFO Channel 00/0 : 30[2] -> 29[1] via P2P/CUMEM
7: nid007124:126397:135773 [2] NCCL INFO Channel 04/0 : 30[2] -> 29[1] via P2P/CUMEM
0: nid007116:202902:212621 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/CUMEM
2: nid007119:146139:155519 [3] NCCL INFO Connected all rings
2: nid007119:146136:155522 [0] NCCL INFO Connected all rings
2: nid007119:146138:155521 [2] NCCL INFO Connected all rings
2: nid007119:146137:155520 [1] NCCL INFO Connected all rings
1: nid007117:210405:220346 [0] NCCL INFO Connected all rings
1: nid007117:210408:220344 [3] NCCL INFO Connected all rings
0: nid007116:202900:212619 [0] NCCL INFO Connected all rings
0: nid007116:202903:212622 [3] NCCL INFO Connected all rings
1: nid007117:210407:220345 [2] NCCL INFO Connected all rings
1: nid007117:210406:220343 [1] NCCL INFO Connected all rings
0: nid007116:202901:212620 [1] NCCL INFO Connected all rings
0: nid007116:202902:212621 [2] NCCL INFO Connected all rings
4: nid007121:116201:125718 [0] NCCL INFO Connected all rings
4: nid007121:116204:125717 [3] NCCL INFO Connected all rings
3: nid007120:197036:206404 [0] NCCL INFO Connected all rings
3: nid007120:197039:206406 [3] NCCL INFO Connected all rings
4: nid007121:116203:125716 [2] NCCL INFO Connected all rings
4: nid007121:116202:125715 [1] NCCL INFO Connected all rings
3: nid007120:197038:206403 [2] NCCL INFO Connected all rings
3: nid007120:197037:206405 [1] NCCL INFO Connected all rings
7: nid007124:126398:135772 [3] NCCL INFO Connected all rings
7: nid007124:126395:135774 [0] NCCL INFO Connected all rings
6: nid007123:108292:117868 [3] NCCL INFO Connected all rings
6: nid007123:108289:117870 [0] NCCL INFO Connected all rings
5: nid007122:66175:75721 [0] NCCL INFO Connected all rings
5: nid007122:66178:75722 [3] NCCL INFO Connected all rings
7: nid007124:126397:135773 [2] NCCL INFO Connected all rings
7: nid007124:126396:135771 [1] NCCL INFO Connected all rings
6: nid007123:108291:117869 [2] NCCL INFO Connected all rings
6: nid007123:108290:117867 [1] NCCL INFO Connected all rings
5: nid007122:66177:75720 [2] NCCL INFO Connected all rings
5: nid007122:66176:75723 [1] NCCL INFO Connected all rings
0:   0%|          | 0/1112 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
0:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
7: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
7:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
7: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
7:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
1: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
1:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
1: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
1:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
4: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
4:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
3: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
3:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
3: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
3:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
2:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
3: nid007120:197036:206428 [0] NCCL INFO Channel 00/0 : 12[0] -> 13[1] via P2P/CUMEM
3: nid007120:197036:206428 [0] NCCL INFO Channel 02/0 : 12[0] -> 13[1] via P2P/CUMEM
3: nid007120:197036:206428 [0] NCCL INFO Channel 03/0 : 12[0] -> 13[1] via P2P/CUMEM
3: nid007120:197036:206428 [0] NCCL INFO Channel 04/0 : 12[0] -> 13[1] via P2P/CUMEM
3: nid007120:197036:206428 [0] NCCL INFO Channel 06/0 : 12[0] -> 13[1] via P2P/CUMEM
0: nid007116:202901:212647 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM
0: nid007116:202900:212648 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM
0: nid007116:202901:212647 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM
3: nid007120:197036:206428 [0] NCCL INFO Channel 07/0 : 12[0] -> 13[1] via P2P/CUMEM
0: nid007116:202900:212648 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM
0: nid007116:202900:212648 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM
2: nid007119:146136:155547 [0] NCCL INFO Channel 01/0 : 8[0] -> 9[1] via P2P/CUMEM
0: nid007116:202900:212648 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/CUMEM
0: nid007116:202900:212648 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM
2: nid007119:146136:155547 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[1] via P2P/CUMEM
0: nid007116:202900:212648 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM
3: nid007120:197038:206431 [2] NCCL INFO Channel 00/0 : 14[2] -> 15[3] via P2P/CUMEM
2: nid007119:146136:155547 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[1] via P2P/CUMEM
3: nid007120:197038:206431 [2] NCCL INFO Channel 02/0 : 14[2] -> 15[3] via P2P/CUMEM
0: nid007116:202902:212650 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM
0: nid007116:202900:212648 [0] NCCL INFO Channel 04/0 : 0[0] -> 4[0] [send] via NET/AWS Libfabric/0
3: nid007120:197038:206431 [2] NCCL INFO Channel 03/0 : 14[2] -> 15[3] via P2P/CUMEM
0: nid007116:202902:212650 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM
3: nid007120:197037:206430 [1] NCCL INFO Channel 00/0 : 13[1] -> 14[2] via P2P/CUMEM
2: nid007119:146136:155547 [0] NCCL INFO Channel 05/0 : 8[0] -> 9[1] via P2P/CUMEM
3: nid007120:197038:206431 [2] NCCL INFO Channel 04/0 : 14[2] -> 15[3] via P2P/CUMEM
0: nid007116:202902:212650 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/CUMEM
3: nid007120:197037:206430 [1] NCCL INFO Channel 04/0 : 13[1] -> 14[2] via P2P/CUMEM
3: nid007120:197038:206431 [2] NCCL INFO Channel 06/0 : 14[2] -> 15[3] via P2P/CUMEM
5: nid007122:66175:75769 [0] NCCL INFO Channel 00/0 : 20[0] -> 21[1] via P2P/CUMEM
0: nid007116:202902:212650 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/CUMEM
7: nid007124:126397:135800 [2] NCCL INFO Channel 00/0 : 30[2] -> 31[3] via P2P/CUMEM
0: nid007116:202902:212650 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM
2: nid007119:146138:155550 [2] NCCL INFO Channel 01/0 : 10[2] -> 11[3] via P2P/CUMEM
3: nid007120:197038:206431 [2] NCCL INFO Channel 07/0 : 14[2] -> 15[3] via P2P/CUMEM
0: nid007116:202902:212650 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/CUMEM
2: nid007119:146136:155547 [0] NCCL INFO Channel 06/0 : 8[0] -> 9[1] via P2P/CUMEM
7: nid007124:126397:135800 [2] NCCL INFO Channel 02/0 : 30[2] -> 31[3] via P2P/CUMEM
1: nid007117:210407:220353 [2] NCCL INFO Channel 00/0 : 6[2] -> 7[3] via P2P/CUMEM
3: nid007120:197036:206428 [0] NCCL INFO Channel 00/0 : 8[0] -> 12[0] [receive] via NET/AWS Libfabric/0
5: nid007122:66175:75769 [0] NCCL INFO Channel 02/0 : 20[0] -> 21[1] via P2P/CUMEM
7: nid007124:126397:135800 [2] NCCL INFO Channel 03/0 : 30[2] -> 31[3] via P2P/CUMEM
0: nid007116:202903:212649 [3] NCCL INFO Channel 02/0 : 3[3] -> 0[0] via P2P/CUMEM
3: nid007120:197039:206429 [3] NCCL INFO Channel 02/0 : 15[3] -> 12[0] via P2P/CUMEM
2: nid007119:146138:155550 [2] NCCL INFO Channel 02/0 : 10[2] -> 11[3] via P2P/CUMEM
1: nid007117:210405:220355 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[1] via P2P/CUMEM
0: nid007116:202902:212650 [2] NCCL INFO Channel 06/0 : 2[2] -> 6[2] [send] via NET/AWS Libfabric/2
1: nid007117:210407:220353 [2] NCCL INFO Channel 02/0 : 6[2] -> 7[3] via P2P/CUMEM
0: nid007116:202903:212649 [3] NCCL INFO Channel 03/0 : 3[3] -> 0[0] via P2P/CUMEM
3: nid007120:197038:206431 [2] NCCL INFO Channel 02/0 : 10[2] -> 14[2] [receive] via NET/AWS Libfabric/2
7: nid007124:126395:135803 [0] NCCL INFO Channel 00/0 : 28[0] -> 29[1] via P2P/CUMEM
3: nid007120:197039:206429 [3] NCCL INFO Channel 03/0 : 15[3] -> 12[0] via P2P/CUMEM
0: nid007116:202901:212647 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM
3: nid007120:197037:206430 [1] NCCL INFO Channel 01/0 : 13[1] -> 12[0] via P2P/CUMEM
5: nid007122:66175:75769 [0] NCCL INFO Channel 03/0 : 20[0] -> 21[1] via P2P/CUMEM
7: nid007124:126397:135800 [2] NCCL INFO Channel 04/0 : 30[2] -> 31[3] via P2P/CUMEM
0: nid007116:202903:212649 [3] NCCL INFO Channel 06/0 : 3[3] -> 0[0] via P2P/CUMEM
2: nid007119:146137:155548 [1] NCCL INFO Channel 01/0 : 9[1] -> 10[2] via P2P/CUMEM
2: nid007119:146138:155550 [2] NCCL INFO Channel 03/0 : 10[2] -> 11[3] via P2P/CUMEM
0: nid007116:202901:212647 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM
3: nid007120:197039:206429 [3] NCCL INFO Channel 06/0 : 15[3] -> 12[0] via P2P/CUMEM
3: nid007120:197037:206430 [1] NCCL INFO Channel 03/0 : 13[1] -> 12[0] via P2P/CUMEM
0: nid007116:202903:212649 [3] NCCL INFO Channel 07/0 : 3[3] -> 0[0] via P2P/CUMEM
0: nid007116:202901:212647 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/CUMEM
1: nid007117:210407:220353 [2] NCCL INFO Channel 03/0 : 6[2] -> 7[3] via P2P/CUMEM
2: nid007119:146136:155547 [0] NCCL INFO Channel 07/0 : 8[0] -> 9[1] via P2P/CUMEM
2: nid007119:146137:155548 [1] NCCL INFO Channel 05/0 : 9[1] -> 10[2] via P2P/CUMEM
2: nid007119:146138:155550 [2] NCCL INFO Channel 05/0 : 10[2] -> 11[3] via P2P/CUMEM
3: nid007120:197039:206429 [3] NCCL INFO Channel 07/0 : 15[3] -> 12[0] via P2P/CUMEM
5: nid007122:66175:75769 [0] NCCL INFO Channel 04/0 : 20[0] -> 21[1] via P2P/CUMEM
3: nid007120:197037:206430 [1] NCCL INFO Channel 05/0 : 13[1] -> 12[0] via P2P/CUMEM
0: nid007116:202901:212647 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/CUMEM
6: nid007123:108289:117900 [0] NCCL INFO Channel 01/0 : 24[0] -> 25[1] via P2P/CUMEM
6: nid007123:108290:117898 [1] NCCL INFO Channel 01/0 : 25[1] -> 26[2] via P2P/CUMEM
6: nid007123:108291:117899 [2] NCCL INFO Channel 01/0 : 26[2] -> 27[3] via P2P/CUMEM
7: nid007124:126397:135800 [2] NCCL INFO Channel 06/0 : 30[2] -> 31[3] via P2P/CUMEM
7: nid007124:126395:135803 [0] NCCL INFO Channel 02/0 : 28[0] -> 29[1] via P2P/CUMEM
3: nid007120:197037:206430 [1] NCCL INFO Channel 07/0 : 13[1] -> 12[0] via P2P/CUMEM
2: nid007119:146138:155550 [2] NCCL INFO Channel 06/0 : 10[2] -> 11[3] via P2P/CUMEM
1: nid007117:210407:220353 [2] NCCL INFO Channel 04/0 : 6[2] -> 7[3] via P2P/CUMEM
6: nid007123:108290:117898 [1] NCCL INFO Channel 05/0 : 25[1] -> 26[2] via P2P/CUMEM
5: nid007122:66177:75770 [2] NCCL INFO Channel 00/0 : 22[2] -> 23[3] via P2P/CUMEM
5: nid007122:66176:75767 [1] NCCL INFO Channel 00/0 : 21[1] -> 22[2] via P2P/CUMEM
2: nid007119:146138:155550 [2] NCCL INFO Channel 07/0 : 10[2] -> 11[3] via P2P/CUMEM
6: nid007123:108291:117899 [2] NCCL INFO Channel 02/0 : 26[2] -> 27[3] via P2P/CUMEM
5: nid007122:66175:75769 [0] NCCL INFO Channel 06/0 : 20[0] -> 21[1] via P2P/CUMEM
7: nid007124:126397:135800 [2] NCCL INFO Channel 07/0 : 30[2] -> 31[3] via P2P/CUMEM
2: nid007119:146136:155547 [0] NCCL INFO Channel 01/0 : 4[0] -> 8[0] [receive] via NET/AWS Libfabric/0
7: nid007124:126395:135803 [0] NCCL INFO Channel 03/0 : 28[0] -> 29[1] via P2P/CUMEM
2: nid007119:146136:155547 [0] NCCL INFO Channel 05/0 : 4[0] -> 8[0] [receive] via NET/AWS Libfabric/0
2: nid007119:146136:155547 [0] NCCL INFO Channel 00/0 : 8[0] -> 12[0] [send] via NET/AWS Libfabric/0
6: nid007123:108289:117900 [0] NCCL INFO Channel 02/0 : 24[0] -> 25[1] via P2P/CUMEM
6: nid007123:108291:117899 [2] NCCL INFO Channel 03/0 : 26[2] -> 27[3] via P2P/CUMEM
1: nid007117:210406:220352 [1] NCCL INFO Channel 00/0 : 5[1] -> 6[2] via P2P/CUMEM
2: nid007119:146139:155549 [3] NCCL INFO Channel 02/0 : 11[3] -> 8[0] via P2P/CUMEM
1: nid007117:210407:220353 [2] NCCL INFO Channel 06/0 : 6[2] -> 7[3] via P2P/CUMEM
5: nid007122:66177:75770 [2] NCCL INFO Channel 02/0 : 22[2] -> 23[3] via P2P/CUMEM
5: nid007122:66176:75767 [1] NCCL INFO Channel 04/0 : 21[1] -> 22[2] via P2P/CUMEM
7: nid007124:126395:135803 [0] NCCL INFO Channel 04/0 : 28[0] -> 29[1] via P2P/CUMEM
3: nid007120:197036:206428 [0] NCCL INFO Channel 04/0 : 4[0] -> 12[0] [receive] via NET/AWS Libfabric/0
2: nid007119:146138:155550 [2] NCCL INFO Channel 03/0 : 6[2] -> 10[2] [receive] via NET/AWS Libfabric/2
5: nid007122:66175:75769 [0] NCCL INFO Channel 07/0 : 20[0] -> 21[1] via P2P/CUMEM
6: nid007123:108291:117899 [2] NCCL INFO Channel 05/0 : 26[2] -> 27[3] via P2P/CUMEM
3: nid007120:197036:206428 [0] NCCL INFO Channel 05/0 : 4[0] -> 12[0] [receive] via NET/AWS Libfabric/0
6: nid007123:108289:117900 [0] NCCL INFO Channel 03/0 : 24[0] -> 25[1] via P2P/CUMEM
2: nid007119:146138:155550 [2] NCCL INFO Channel 07/0 : 6[2] -> 10[2] [receive] via NET/AWS Libfabric/2
3: nid007120:197036:206428 [0] NCCL INFO Channel 04/0 : 12[0] -> 20[0] [send] via NET/AWS Libfabric/0
1: nid007117:210405:220355 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[1] via P2P/CUMEM
2: nid007119:146139:155549 [3] NCCL INFO Channel 03/0 : 11[3] -> 8[0] via P2P/CUMEM
2: nid007119:146138:155550 [2] NCCL INFO Channel 02/0 : 10[2] -> 14[2] [send] via NET/AWS Libfabric/2
7: nid007124:126398:135801 [3] NCCL INFO Channel 02/0 : 31[3] -> 28[0] via P2P/CUMEM
3: nid007120:197036:206428 [0] NCCL INFO Channel 05/0 : 12[0] -> 20[0] [send] via NET/AWS Libfabric/0
7: nid007124:126395:135803 [0] NCCL INFO Channel 06/0 : 28[0] -> 29[1] via P2P/CUMEM
6: nid007123:108291:117899 [2] NCCL INFO Channel 06/0 : 26[2] -> 27[3] via P2P/CUMEM
1: nid007117:210406:220352 [1] NCCL INFO Channel 04/0 : 5[1] -> 6[2] via P2P/CUMEM
6: nid007123:108289:117900 [0] NCCL INFO Channel 05/0 : 24[0] -> 25[1] via P2P/CUMEM
1: nid007117:210407:220353 [2] NCCL INFO Channel 07/0 : 6[2] -> 7[3] via P2P/CUMEM
5: nid007122:66177:75770 [2] NCCL INFO Channel 03/0 : 22[2] -> 23[3] via P2P/CUMEM
2: nid007119:146139:155549 [3] NCCL INFO Channel 06/0 : 11[3] -> 8[0] via P2P/CUMEM
3: nid007120:197038:206431 [2] NCCL INFO Channel 06/0 : 6[2] -> 14[2] [receive] via NET/AWS Libfabric/2
2: nid007119:146137:155548 [1] NCCL INFO Channel 00/0 : 9[1] -> 8[0] via P2P/CUMEM
6: nid007123:108291:117899 [2] NCCL INFO Channel 07/0 : 26[2] -> 27[3] via P2P/CUMEM
3: nid007120:197038:206431 [2] NCCL INFO Channel 07/0 : 6[2] -> 14[2] [receive] via NET/AWS Libfabric/2
6: nid007123:108289:117900 [0] NCCL INFO Channel 06/0 : 24[0] -> 25[1] via P2P/CUMEM
7: nid007124:126398:135801 [3] NCCL INFO Channel 03/0 : 31[3] -> 28[0] via P2P/CUMEM
3: nid007120:197038:206431 [2] NCCL INFO Channel 06/0 : 14[2] -> 22[2] [send] via NET/AWS Libfabric/2
4: nid007121:116201:125747 [0] NCCL INFO Channel 01/0 : 16[0] -> 17[1] via P2P/CUMEM
7: nid007124:126395:135803 [0] NCCL INFO Channel 07/0 : 28[0] -> 29[1] via P2P/CUMEM
3: nid007120:197038:206431 [2] NCCL INFO Channel 07/0 : 14[2] -> 22[2] [send] via NET/AWS Libfabric/2
5: nid007122:66177:75770 [2] NCCL INFO Channel 04/0 : 22[2] -> 23[3] via P2P/CUMEM
2: nid007119:146139:155549 [3] NCCL INFO Channel 07/0 : 11[3] -> 8[0] via P2P/CUMEM
6: nid007123:108289:117900 [0] NCCL INFO Channel 07/0 : 24[0] -> 25[1] via P2P/CUMEM
1: nid007117:210405:220355 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[1] via P2P/CUMEM
2: nid007119:146137:155548 [1] NCCL INFO Channel 02/0 : 9[1] -> 8[0] via P2P/CUMEM
5: nid007122:66177:75770 [2] NCCL INFO Channel 06/0 : 22[2] -> 23[3] via P2P/CUMEM
5: nid007122:66175:75769 [0] NCCL INFO Channel 04/0 : 16[0] -> 20[0] [receive] via NET/AWS Libfabric/0
5: nid007122:66175:75769 [0] NCCL INFO Channel 01/0 : 20[0] -> 24[0] [send] via NET/AWS Libfabric/0
7: nid007124:126396:135802 [1] NCCL INFO Channel 00/0 : 29[1] -> 30[2] via P2P/CUMEM
5: nid007122:66177:75770 [2] NCCL INFO Channel 07/0 : 22[2] -> 23[3] via P2P/CUMEM
7: nid007124:126398:135801 [3] NCCL INFO Channel 06/0 : 31[3] -> 28[0] via P2P/CUMEM
6: nid007123:108292:117897 [3] NCCL INFO Channel 02/0 : 27[3] -> 24[0] via P2P/CUMEM
5: nid007122:66175:75769 [0] NCCL INFO Channel 05/0 : 20[0] -> 24[0] [send] via NET/AWS Libfabric/0
2: nid007119:146137:155548 [1] NCCL INFO Channel 04/0 : 9[1] -> 8[0] via P2P/CUMEM
1: nid007117:210405:220355 [0] NCCL INFO Channel 04/0 : 4[0] -> 5[1] via P2P/CUMEM
6: nid007123:108292:117897 [3] NCCL INFO Channel 03/0 : 27[3] -> 24[0] via P2P/CUMEM
6: nid007123:108291:117899 [2] NCCL INFO Channel 03/0 : 22[2] -> 26[2] [receive] via NET/AWS Libfabric/2
2: nid007119:146137:155548 [1] NCCL INFO Channel 06/0 : 9[1] -> 8[0] via P2P/CUMEM
6: nid007123:108291:117899 [2] NCCL INFO Channel 07/0 : 22[2] -> 26[2] [receive] via NET/AWS Libfabric/2
1: nid007117:210405:220355 [0] NCCL INFO Channel 06/0 : 4[0] -> 5[1] via P2P/CUMEM
7: nid007124:126396:135802 [1] NCCL INFO Channel 04/0 : 29[1] -> 30[2] via P2P/CUMEM
7: nid007124:126398:135801 [3] NCCL INFO Channel 07/0 : 31[3] -> 28[0] via P2P/CUMEM
6: nid007123:108291:117899 [2] NCCL INFO Channel 02/0 : 26[2] -> 30[2] [send] via NET/AWS Libfabric/2
5: nid007122:66178:75768 [3] NCCL INFO Channel 02/0 : 23[3] -> 20[0] via P2P/CUMEM
1: nid007117:210405:220355 [0] NCCL INFO Channel 07/0 : 4[0] -> 5[1] via P2P/CUMEM
1: nid007117:210408:220354 [3] NCCL INFO Channel 02/0 : 7[3] -> 4[0] via P2P/CUMEM
6: nid007123:108289:117900 [0] NCCL INFO Channel 01/0 : 20[0] -> 24[0] [receive] via NET/AWS Libfabric/0
6: nid007123:108292:117897 [3] NCCL INFO Channel 06/0 : 27[3] -> 24[0] via P2P/CUMEM
5: nid007122:66177:75770 [2] NCCL INFO Channel 06/0 : 18[2] -> 22[2] [receive] via NET/AWS Libfabric/2
5: nid007122:66177:75770 [2] NCCL INFO Channel 03/0 : 22[2] -> 26[2] [send] via NET/AWS Libfabric/2
6: nid007123:108289:117900 [0] NCCL INFO Channel 05/0 : 20[0] -> 24[0] [receive] via NET/AWS Libfabric/0
1: nid007117:210407:220353 [2] NCCL INFO Channel 06/0 : 2[2] -> 6[2] [receive] via NET/AWS Libfabric/2
1: nid007117:210408:220354 [3] NCCL INFO Channel 03/0 : 7[3] -> 4[0] via P2P/CUMEM
5: nid007122:66177:75770 [2] NCCL INFO Channel 07/0 : 22[2] -> 26[2] [send] via NET/AWS Libfabric/2
5: nid007122:66178:75768 [3] NCCL INFO Channel 03/0 : 23[3] -> 20[0] via P2P/CUMEM
6: nid007123:108289:117900 [0] NCCL INFO Channel 00/0 : 24[0] -> 28[0] [send] via NET/AWS Libfabric/0
1: nid007117:210407:220353 [2] NCCL INFO Channel 03/0 : 6[2] -> 10[2] [send] via NET/AWS Libfabric/2
6: nid007123:108290:117898 [1] NCCL INFO Channel 00/0 : 25[1] -> 24[0] via P2P/CUMEM
6: nid007123:108292:117897 [3] NCCL INFO Channel 07/0 : 27[3] -> 24[0] via P2P/CUMEM
1: nid007117:210408:220354 [3] NCCL INFO Channel 06/0 : 7[3] -> 4[0] via P2P/CUMEM
1: nid007117:210407:220353 [2] NCCL INFO Channel 07/0 : 6[2] -> 10[2] [send] via NET/AWS Libfabric/2
7: nid007124:126395:135803 [0] NCCL INFO Channel 00/0 : 24[0] -> 28[0] [receive] via NET/AWS Libfabric/0
7: nid007124:126397:135800 [2] NCCL INFO Channel 02/0 : 26[2] -> 30[2] [receive] via NET/AWS Libfabric/2
1: nid007117:210405:220355 [0] NCCL INFO Channel 04/0 : 0[0] -> 4[0] [receive] via NET/AWS Libfabric/0
4: nid007121:116201:125747 [0] NCCL INFO Channel 02/0 : 16[0] -> 17[1] via P2P/CUMEM
5: nid007122:66176:75767 [1] NCCL INFO Channel 01/0 : 21[1] -> 20[0] via P2P/CUMEM
0: nid007116:202902:212650 [2] NCCL INFO Channel 02/0 : 18[2] -> 2[2] [receive] via NET/AWS Libfabric/2
6: nid007123:108290:117898 [1] NCCL INFO Channel 02/0 : 25[1] -> 24[0] via P2P/CUMEM
1: nid007117:210405:220355 [0] NCCL INFO Channel 01/0 : 4[0] -> 8[0] [send] via NET/AWS Libfabric/0
5: nid007122:66178:75768 [3] NCCL INFO Channel 06/0 : 23[3] -> 20[0] via P2P/CUMEM
6: nid007123:108289:117900 [0] NCCL INFO Channel 00/0 : 16[0] -> 24[0] [receive] via NET/AWS Libfabric/0
7: nid007124:126395:135803 [0] NCCL INFO Channel 04/0 : 12[0] -> 28[0] [receive] via NET/AWS Libfabric/0
0: nid007116:202902:212650 [2] NCCL INFO Channel 03/0 : 18[2] -> 2[2] [receive] via NET/AWS Libfabric/2
6: nid007123:108289:117900 [0] NCCL INFO Channel 01/0 : 16[0] -> 24[0] [receive] via NET/AWS Libfabric/0
1: nid007117:210408:220354 [3] NCCL INFO Channel 07/0 : 7[3] -> 4[0] via P2P/CUMEM
4: nid007121:116203:125746 [2] NCCL INFO Channel 01/0 : 18[2] -> 19[3] via P2P/CUMEM
1: nid007117:210405:220355 [0] NCCL INFO Channel 05/0 : 4[0] -> 8[0] [send] via NET/AWS Libfabric/0
6: nid007123:108291:117899 [2] NCCL INFO Channel 02/0 : 18[2] -> 26[2] [receive] via NET/AWS Libfabric/2
0: nid007116:202902:212650 [2] NCCL INFO Channel 02/0 : 2[2] -> 18[2] [send] via NET/AWS Libfabric/2
2: nid007119:146138:155550 [2] NCCL INFO Channel 02/0 : 10[2] -> 18[2] [send] via NET/AWS Libfabric/2
1: nid007117:210407:220353 [2] NCCL INFO Channel 06/0 : 6[2] -> 14[2] [send] via NET/AWS Libfabric/2
7: nid007124:126397:135800 [2] NCCL INFO Channel 06/0 : 14[2] -> 30[2] [receive] via NET/AWS Libfabric/2
6: nid007123:108290:117898 [1] NCCL INFO Channel 04/0 : 25[1] -> 24[0] via P2P/CUMEM
7: nid007124:126395:135803 [0] NCCL INFO Channel 05/0 : 12[0] -> 28[0] [receive] via NET/AWS Libfabric/0
2: nid007119:146138:155550 [2] NCCL INFO Channel 03/0 : 10[2] -> 18[2] [send] via NET/AWS Libfabric/2
6: nid007123:108291:117899 [2] NCCL INFO Channel 03/0 : 18[2] -> 26[2] [receive] via NET/AWS Libfabric/2
1: nid007117:210407:220353 [2] NCCL INFO Channel 07/0 : 6[2] -> 14[2] [send] via NET/AWS Libfabric/2
0: nid007116:202902:212650 [2] NCCL INFO Channel 03/0 : 2[2] -> 18[2] [send] via NET/AWS Libfabric/2
5: nid007122:66176:75767 [1] NCCL INFO Channel 03/0 : 21[1] -> 20[0] via P2P/CUMEM
5: nid007122:66178:75768 [3] NCCL INFO Channel 07/0 : 23[3] -> 20[0] via P2P/CUMEM
6: nid007123:108290:117898 [1] NCCL INFO Channel 06/0 : 25[1] -> 24[0] via P2P/CUMEM
7: nid007124:126397:135800 [2] NCCL INFO Channel 07/0 : 14[2] -> 30[2] [receive] via NET/AWS Libfabric/2
7: nid007124:126395:135803 [0] NCCL INFO Channel 04/0 : 28[0] -> 12[0] [send] via NET/AWS Libfabric/0
0: nid007116:202900:212648 [0] NCCL INFO Channel 00/0 : 16[0] -> 0[0] [receive] via NET/AWS Libfabric/0
4: nid007121:116201:125747 [0] NCCL INFO Channel 03/0 : 16[0] -> 17[1] via P2P/CUMEM
7: nid007124:126396:135802 [1] NCCL INFO Channel 01/0 : 29[1] -> 28[0] via P2P/CUMEM
7: nid007124:126395:135803 [0] NCCL INFO Channel 05/0 : 28[0] -> 12[0] [send] via NET/AWS Libfabric/0
1: nid007117:210407:220353 [2] NCCL INFO Channel 06/0 : 14[2] -> 6[2] [receive] via NET/AWS Libfabric/2
7: nid007124:126397:135800 [2] NCCL INFO Channel 06/0 : 30[2] -> 14[2] [send] via NET/AWS Libfabric/2
1: nid007117:210405:220355 [0] NCCL INFO Channel 04/0 : 4[0] -> 12[0] [send] via NET/AWS Libfabric/0
0: nid007116:202900:212648 [0] NCCL INFO Channel 01/0 : 16[0] -> 0[0] [receive] via NET/AWS Libfabric/0
7: nid007124:126397:135800 [2] NCCL INFO Channel 07/0 : 30[2] -> 14[2] [send] via NET/AWS Libfabric/2
4: nid007121:116203:125746 [2] NCCL INFO Channel 02/0 : 18[2] -> 19[3] via P2P/CUMEM
2: nid007119:146136:155547 [0] NCCL INFO Channel 00/0 : 8[0] -> 16[0] [send] via NET/AWS Libfabric/0
1: nid007117:210406:220352 [1] NCCL INFO Channel 01/0 : 5[1] -> 4[0] via P2P/CUMEM
5: nid007122:66176:75767 [1] NCCL INFO Channel 05/0 : 21[1] -> 20[0] via P2P/CUMEM
7: nid007124:126396:135802 [1] NCCL INFO Channel 03/0 : 29[1] -> 28[0] via P2P/CUMEM
0: nid007116:202900:212648 [0] NCCL INFO Channel 00/0 : 0[0] -> 16[0] [send] via NET/AWS Libfabric/0
1: nid007117:210407:220353 [2] NCCL INFO Channel 07/0 : 14[2] -> 6[2] [receive] via NET/AWS Libfabric/2
2: nid007119:146136:155547 [0] NCCL INFO Channel 01/0 : 8[0] -> 16[0] [send] via NET/AWS Libfabric/0
1: nid007117:210405:220355 [0] NCCL INFO Channel 05/0 : 4[0] -> 12[0] [send] via NET/AWS Libfabric/0
1: nid007117:210406:220352 [1] NCCL INFO Channel 03/0 : 5[1] -> 4[0] via P2P/CUMEM
0: nid007116:202900:212648 [0] NCCL INFO Channel 01/0 : 0[0] -> 16[0] [send] via NET/AWS Libfabric/0
7: nid007124:126396:135802 [1] NCCL INFO Channel 05/0 : 29[1] -> 28[0] via P2P/CUMEM
1: nid007117:210405:220355 [0] NCCL INFO Channel 04/0 : 12[0] -> 4[0] [receive] via NET/AWS Libfabric/0
5: nid007122:66176:75767 [1] NCCL INFO Channel 07/0 : 21[1] -> 20[0] via P2P/CUMEM
1: nid007117:210406:220352 [1] NCCL INFO Channel 05/0 : 5[1] -> 4[0] via P2P/CUMEM
4: nid007121:116201:125747 [0] NCCL INFO Channel 05/0 : 16[0] -> 17[1] via P2P/CUMEM
1: nid007117:210405:220355 [0] NCCL INFO Channel 05/0 : 12[0] -> 4[0] [receive] via NET/AWS Libfabric/0
7: nid007124:126396:135802 [1] NCCL INFO Channel 07/0 : 29[1] -> 28[0] via P2P/CUMEM
1: nid007117:210406:220352 [1] NCCL INFO Channel 07/0 : 5[1] -> 4[0] via P2P/CUMEM
4: nid007121:116203:125746 [2] NCCL INFO Channel 03/0 : 18[2] -> 19[3] via P2P/CUMEM
4: nid007121:116202:125745 [1] NCCL INFO Channel 01/0 : 17[1] -> 18[2] via P2P/CUMEM
4: nid007121:116201:125747 [0] NCCL INFO Channel 06/0 : 16[0] -> 17[1] via P2P/CUMEM
4: nid007121:116203:125746 [2] NCCL INFO Channel 05/0 : 18[2] -> 19[3] via P2P/CUMEM
4: nid007121:116202:125745 [1] NCCL INFO Channel 05/0 : 17[1] -> 18[2] via P2P/CUMEM
4: nid007121:116201:125747 [0] NCCL INFO Channel 07/0 : 16[0] -> 17[1] via P2P/CUMEM
4: nid007121:116203:125746 [2] NCCL INFO Channel 06/0 : 18[2] -> 19[3] via P2P/CUMEM
4: nid007121:116203:125746 [2] NCCL INFO Channel 07/0 : 18[2] -> 19[3] via P2P/CUMEM
4: nid007121:116201:125747 [0] NCCL INFO Channel 04/0 : 16[0] -> 20[0] [send] via NET/AWS Libfabric/0
4: nid007121:116204:125744 [3] NCCL INFO Channel 02/0 : 19[3] -> 16[0] via P2P/CUMEM
5: nid007122:66175:75769 [0] NCCL INFO Channel 04/0 : 12[0] -> 20[0] [receive] via NET/AWS Libfabric/0
5: nid007122:66175:75769 [0] NCCL INFO Channel 05/0 : 12[0] -> 20[0] [receive] via NET/AWS Libfabric/0
4: nid007121:116203:125746 [2] NCCL INFO Channel 06/0 : 18[2] -> 22[2] [send] via NET/AWS Libfabric/2
4: nid007121:116201:125747 [0] NCCL INFO Channel 00/0 : 8[0] -> 16[0] [receive] via NET/AWS Libfabric/0
4: nid007121:116204:125744 [3] NCCL INFO Channel 03/0 : 19[3] -> 16[0] via P2P/CUMEM
4: nid007121:116202:125745 [1] NCCL INFO Channel 00/0 : 17[1] -> 16[0] via P2P/CUMEM
4: nid007121:116201:125747 [0] NCCL INFO Channel 01/0 : 8[0] -> 16[0] [receive] via NET/AWS Libfabric/0
5: nid007122:66175:75769 [0] NCCL INFO Channel 04/0 : 20[0] -> 12[0] [send] via NET/AWS Libfabric/0
4: nid007121:116201:125747 [0] NCCL INFO Channel 00/0 : 16[0] -> 24[0] [send] via NET/AWS Libfabric/0
5: nid007122:66175:75769 [0] NCCL INFO Channel 05/0 : 20[0] -> 12[0] [send] via NET/AWS Libfabric/0
3: nid007120:197036:206428 [0] NCCL INFO Channel 04/0 : 28[0] -> 12[0] [receive] via NET/AWS Libfabric/0
4: nid007121:116201:125747 [0] NCCL INFO Channel 01/0 : 16[0] -> 24[0] [send] via NET/AWS Libfabric/0
5: nid007122:66177:75770 [2] NCCL INFO Channel 06/0 : 14[2] -> 22[2] [receive] via NET/AWS Libfabric/2
4: nid007121:116203:125746 [2] NCCL INFO Channel 02/0 : 10[2] -> 18[2] [receive] via NET/AWS Libfabric/2
3: nid007120:197036:206428 [0] NCCL INFO Channel 05/0 : 28[0] -> 12[0] [receive] via NET/AWS Libfabric/0
5: nid007122:66177:75770 [2] NCCL INFO Channel 07/0 : 14[2] -> 22[2] [receive] via NET/AWS Libfabric/2
4: nid007121:116203:125746 [2] NCCL INFO Channel 03/0 : 10[2] -> 18[2] [receive] via NET/AWS Libfabric/2
3: nid007120:197036:206428 [0] NCCL INFO Channel 04/0 : 12[0] -> 28[0] [send] via NET/AWS Libfabric/0
4: nid007121:116202:125745 [1] NCCL INFO Channel 02/0 : 17[1] -> 16[0] via P2P/CUMEM
4: nid007121:116203:125746 [2] NCCL INFO Channel 02/0 : 18[2] -> 26[2] [send] via NET/AWS Libfabric/2
3: nid007120:197036:206428 [0] NCCL INFO Channel 05/0 : 12[0] -> 28[0] [send] via NET/AWS Libfabric/0
4: nid007121:116204:125744 [3] NCCL INFO Channel 06/0 : 19[3] -> 16[0] via P2P/CUMEM
2: nid007119:146136:155547 [0] NCCL INFO Channel 00/0 : 16[0] -> 8[0] [receive] via NET/AWS Libfabric/0
4: nid007121:116203:125746 [2] NCCL INFO Channel 03/0 : 18[2] -> 26[2] [send] via NET/AWS Libfabric/2
5: nid007122:66177:75770 [2] NCCL INFO Channel 06/0 : 22[2] -> 14[2] [send] via NET/AWS Libfabric/2
4: nid007121:116201:125747 [0] NCCL INFO Channel 00/0 : 0[0] -> 16[0] [receive] via NET/AWS Libfabric/0
2: nid007119:146136:155547 [0] NCCL INFO Channel 01/0 : 16[0] -> 8[0] [receive] via NET/AWS Libfabric/0
3: nid007120:197038:206431 [2] NCCL INFO Channel 06/0 : 30[2] -> 14[2] [receive] via NET/AWS Libfabric/2
4: nid007121:116202:125745 [1] NCCL INFO Channel 04/0 : 17[1] -> 16[0] via P2P/CUMEM
5: nid007122:66177:75770 [2] NCCL INFO Channel 07/0 : 22[2] -> 14[2] [send] via NET/AWS Libfabric/2
6: nid007123:108289:117900 [0] NCCL INFO Channel 00/0 : 24[0] -> 16[0] [send] via NET/AWS Libfabric/0
4: nid007121:116201:125747 [0] NCCL INFO Channel 01/0 : 0[0] -> 16[0] [receive] via NET/AWS Libfabric/0
6: nid007123:108289:117900 [0] NCCL INFO Channel 01/0 : 24[0] -> 16[0] [send] via NET/AWS Libfabric/0
4: nid007121:116201:125747 [0] NCCL INFO Channel 00/0 : 16[0] -> 0[0] [send] via NET/AWS Libfabric/0
7: nid007124:126395:135803 [0] NCCL INFO Channel 00/0 : 28[0] -> 24[0] [send] via NET/AWS Libfabric/0
3: nid007120:197038:206431 [2] NCCL INFO Channel 07/0 : 30[2] -> 14[2] [receive] via NET/AWS Libfabric/2
3: nid007120:197036:206428 [0] NCCL INFO Channel 04/0 : 20[0] -> 12[0] [receive] via NET/AWS Libfabric/0
4: nid007121:116201:125747 [0] NCCL INFO Channel 01/0 : 16[0] -> 0[0] [send] via NET/AWS Libfabric/0
4: nid007121:116204:125744 [3] NCCL INFO Channel 07/0 : 19[3] -> 16[0] via P2P/CUMEM
3: nid007120:197038:206431 [2] NCCL INFO Channel 06/0 : 14[2] -> 30[2] [send] via NET/AWS Libfabric/2
7: nid007124:126395:135803 [0] NCCL INFO Channel 01/0 : 28[0] -> 24[0] [send] via NET/AWS Libfabric/0
3: nid007120:197036:206428 [0] NCCL INFO Channel 05/0 : 20[0] -> 12[0] [receive] via NET/AWS Libfabric/0
2: nid007119:146138:155550 [2] NCCL INFO Channel 02/0 : 18[2] -> 10[2] [receive] via NET/AWS Libfabric/2
4: nid007121:116202:125745 [1] NCCL INFO Channel 06/0 : 17[1] -> 16[0] via P2P/CUMEM
3: nid007120:197038:206431 [2] NCCL INFO Channel 07/0 : 14[2] -> 30[2] [send] via NET/AWS Libfabric/2
2: nid007119:146138:155550 [2] NCCL INFO Channel 03/0 : 18[2] -> 10[2] [receive] via NET/AWS Libfabric/2
4: nid007121:116203:125746 [2] NCCL INFO Channel 02/0 : 2[2] -> 18[2] [receive] via NET/AWS Libfabric/2
3: nid007120:197036:206428 [0] NCCL INFO Channel 04/0 : 12[0] -> 4[0] [send] via NET/AWS Libfabric/0
4: nid007121:116201:125747 [0] NCCL INFO Channel 00/0 : 24[0] -> 16[0] [receive] via NET/AWS Libfabric/0
3: nid007120:197036:206428 [0] NCCL INFO Channel 05/0 : 12[0] -> 4[0] [send] via NET/AWS Libfabric/0
6: nid007123:108291:117899 [2] NCCL INFO Channel 02/0 : 26[2] -> 18[2] [send] via NET/AWS Libfabric/2
4: nid007121:116203:125746 [2] NCCL INFO Channel 03/0 : 2[2] -> 18[2] [receive] via NET/AWS Libfabric/2
0: nid007116:202900:212648 [0] NCCL INFO Channel 04/0 : 4[0] -> 0[0] [receive] via NET/AWS Libfabric/0
4: nid007121:116201:125747 [0] NCCL INFO Channel 01/0 : 24[0] -> 16[0] [receive] via NET/AWS Libfabric/0
6: nid007123:108291:117899 [2] NCCL INFO Channel 03/0 : 26[2] -> 18[2] [send] via NET/AWS Libfabric/2
3: nid007120:197038:206431 [2] NCCL INFO Channel 06/0 : 22[2] -> 14[2] [receive] via NET/AWS Libfabric/2
0: nid007116:202900:212648 [0] NCCL INFO Channel 05/0 : 4[0] -> 0[0] [receive] via NET/AWS Libfabric/0
4: nid007121:116203:125746 [2] NCCL INFO Channel 02/0 : 18[2] -> 2[2] [send] via NET/AWS Libfabric/2
5: nid007122:66175:75769 [0] NCCL INFO Channel 00/0 : 24[0] -> 20[0] [receive] via NET/AWS Libfabric/0
4: nid007121:116201:125747 [0] NCCL INFO Channel 00/0 : 16[0] -> 8[0] [send] via NET/AWS Libfabric/0
7: nid007124:126397:135800 [2] NCCL INFO Channel 02/0 : 30[2] -> 26[2] [send] via NET/AWS Libfabric/2
3: nid007120:197038:206431 [2] NCCL INFO Channel 07/0 : 22[2] -> 14[2] [receive] via NET/AWS Libfabric/2
5: nid007122:66175:75769 [0] NCCL INFO Channel 01/0 : 24[0] -> 20[0] [receive] via NET/AWS Libfabric/0
3: nid007120:197036:206428 [0] NCCL INFO Channel 00/0 : 12[0] -> 8[0] [send] via NET/AWS Libfabric/0
7: nid007124:126397:135800 [2] NCCL INFO Channel 03/0 : 30[2] -> 26[2] [send] via NET/AWS Libfabric/2
4: nid007121:116201:125747 [0] NCCL INFO Channel 01/0 : 16[0] -> 8[0] [send] via NET/AWS Libfabric/0
4: nid007121:116203:125746 [2] NCCL INFO Channel 03/0 : 18[2] -> 2[2] [send] via NET/AWS Libfabric/2
3: nid007120:197038:206431 [2] NCCL INFO Channel 06/0 : 14[2] -> 6[2] [send] via NET/AWS Libfabric/2
1: nid007117:210405:220355 [0] NCCL INFO Channel 00/0 : 8[0] -> 4[0] [receive] via NET/AWS Libfabric/0
3: nid007120:197036:206428 [0] NCCL INFO Channel 01/0 : 12[0] -> 8[0] [send] via NET/AWS Libfabric/0
5: nid007122:66175:75769 [0] NCCL INFO Channel 04/0 : 24[0] -> 20[0] [receive] via NET/AWS Libfabric/0
1: nid007117:210405:220355 [0] NCCL INFO Channel 01/0 : 8[0] -> 4[0] [receive] via NET/AWS Libfabric/0
5: nid007122:66175:75769 [0] NCCL INFO Channel 05/0 : 24[0] -> 20[0] [receive] via NET/AWS Libfabric/0
3: nid007120:197038:206431 [2] NCCL INFO Channel 07/0 : 14[2] -> 6[2] [send] via NET/AWS Libfabric/2
5: nid007122:66175:75769 [0] NCCL INFO Channel 04/0 : 20[0] -> 16[0] [send] via NET/AWS Libfabric/0
6: nid007123:108289:117900 [0] NCCL INFO Channel 00/0 : 28[0] -> 24[0] [receive] via NET/AWS Libfabric/0
1: nid007117:210405:220355 [0] NCCL INFO Channel 04/0 : 8[0] -> 4[0] [receive] via NET/AWS Libfabric/0
4: nid007121:116203:125746 [2] NCCL INFO Channel 02/0 : 26[2] -> 18[2] [receive] via NET/AWS Libfabric/2
1: nid007117:210405:220355 [0] NCCL INFO Channel 05/0 : 8[0] -> 4[0] [receive] via NET/AWS Libfabric/0
5: nid007122:66175:75769 [0] NCCL INFO Channel 05/0 : 20[0] -> 16[0] [send] via NET/AWS Libfabric/0
6: nid007123:108289:117900 [0] NCCL INFO Channel 01/0 : 28[0] -> 24[0] [receive] via NET/AWS Libfabric/0
1: nid007117:210405:220355 [0] NCCL INFO Channel 04/0 : 4[0] -> 0[0] [send] via NET/AWS Libfabric/0
0: nid007116:202902:212650 [2] NCCL INFO Channel 06/0 : 6[2] -> 2[2] [receive] via NET/AWS Libfabric/2
4: nid007121:116201:125747 [0] NCCL INFO Channel 04/0 : 20[0] -> 16[0] [receive] via NET/AWS Libfabric/0
5: nid007122:66177:75770 [2] NCCL INFO Channel 02/0 : 26[2] -> 22[2] [receive] via NET/AWS Libfabric/2
2: nid007119:146136:155547 [0] NCCL INFO Channel 00/0 : 12[0] -> 8[0] [receive] via NET/AWS Libfabric/0
6: nid007123:108289:117900 [0] NCCL INFO Channel 00/0 : 24[0] -> 20[0] [send] via NET/AWS Libfabric/0
4: nid007121:116203:125746 [2] NCCL INFO Channel 03/0 : 26[2] -> 18[2] [receive] via NET/AWS Libfabric/2
0: nid007116:202902:212650 [2] NCCL INFO Channel 07/0 : 6[2] -> 2[2] [receive] via NET/AWS Libfabric/2
3: nid007120:197038:206431 [2] NCCL INFO Channel 02/0 : 14[2] -> 10[2] [send] via NET/AWS Libfabric/2
5: nid007122:66177:75770 [2] NCCL INFO Channel 03/0 : 26[2] -> 22[2] [receive] via NET/AWS Libfabric/2
1: nid007117:210405:220355 [0] NCCL INFO Channel 05/0 : 4[0] -> 0[0] [send] via NET/AWS Libfabric/0
4: nid007121:116201:125747 [0] NCCL INFO Channel 05/0 : 20[0] -> 16[0] [receive] via NET/AWS Libfabric/0
2: nid007119:146136:155547 [0] NCCL INFO Channel 01/0 : 12[0] -> 8[0] [receive] via NET/AWS Libfabric/0
6: nid007123:108289:117900 [0] NCCL INFO Channel 01/0 : 24[0] -> 20[0] [send] via NET/AWS Libfabric/0
4: nid007121:116203:125746 [2] NCCL INFO Channel 02/0 : 18[2] -> 10[2] [send] via NET/AWS Libfabric/2
1: nid007117:210407:220353 [2] NCCL INFO Channel 02/0 : 10[2] -> 6[2] [receive] via NET/AWS Libfabric/2
2: nid007119:146136:155547 [0] NCCL INFO Channel 00/0 : 8[0] -> 4[0] [send] via NET/AWS Libfabric/0
6: nid007123:108289:117900 [0] NCCL INFO Channel 04/0 : 24[0] -> 20[0] [send] via NET/AWS Libfabric/0
4: nid007121:116203:125746 [2] NCCL INFO Channel 03/0 : 18[2] -> 10[2] [send] via NET/AWS Libfabric/2
3: nid007120:197038:206431 [2] NCCL INFO Channel 03/0 : 14[2] -> 10[2] [send] via NET/AWS Libfabric/2
5: nid007122:66177:75770 [2] NCCL INFO Channel 06/0 : 26[2] -> 22[2] [receive] via NET/AWS Libfabric/2
1: nid007117:210407:220353 [2] NCCL INFO Channel 03/0 : 10[2] -> 6[2] [receive] via NET/AWS Libfabric/2
2: nid007119:146136:155547 [0] NCCL INFO Channel 01/0 : 8[0] -> 4[0] [send] via NET/AWS Libfabric/0
5: nid007122:66177:75770 [2] NCCL INFO Channel 07/0 : 26[2] -> 22[2] [receive] via NET/AWS Libfabric/2
6: nid007123:108289:117900 [0] NCCL INFO Channel 05/0 : 24[0] -> 20[0] [send] via NET/AWS Libfabric/0
2: nid007119:146136:155547 [0] NCCL INFO Channel 04/0 : 8[0] -> 4[0] [send] via NET/AWS Libfabric/0
1: nid007117:210407:220353 [2] NCCL INFO Channel 06/0 : 10[2] -> 6[2] [receive] via NET/AWS Libfabric/2
5: nid007122:66177:75770 [2] NCCL INFO Channel 06/0 : 22[2] -> 18[2] [send] via NET/AWS Libfabric/2
5: nid007122:66177:75770 [2] NCCL INFO Channel 07/0 : 22[2] -> 18[2] [send] via NET/AWS Libfabric/2
2: nid007119:146136:155547 [0] NCCL INFO Channel 05/0 : 8[0] -> 4[0] [send] via NET/AWS Libfabric/0
1: nid007117:210407:220353 [2] NCCL INFO Channel 07/0 : 10[2] -> 6[2] [receive] via NET/AWS Libfabric/2
1: nid007117:210407:220353 [2] NCCL INFO Channel 06/0 : 6[2] -> 2[2] [send] via NET/AWS Libfabric/2
6: nid007123:108291:117899 [2] NCCL INFO Channel 02/0 : 30[2] -> 26[2] [receive] via NET/AWS Libfabric/2
1: nid007117:210407:220353 [2] NCCL INFO Channel 07/0 : 6[2] -> 2[2] [send] via NET/AWS Libfabric/2
6: nid007123:108291:117899 [2] NCCL INFO Channel 03/0 : 30[2] -> 26[2] [receive] via NET/AWS Libfabric/2
4: nid007121:116203:125746 [2] NCCL INFO Channel 06/0 : 22[2] -> 18[2] [receive] via NET/AWS Libfabric/2
2: nid007119:146138:155550 [2] NCCL INFO Channel 02/0 : 14[2] -> 10[2] [receive] via NET/AWS Libfabric/2
6: nid007123:108291:117899 [2] NCCL INFO Channel 02/0 : 26[2] -> 22[2] [send] via NET/AWS Libfabric/2
4: nid007121:116203:125746 [2] NCCL INFO Channel 07/0 : 22[2] -> 18[2] [receive] via NET/AWS Libfabric/2
2: nid007119:146138:155550 [2] NCCL INFO Channel 03/0 : 14[2] -> 10[2] [receive] via NET/AWS Libfabric/2
6: nid007123:108291:117899 [2] NCCL INFO Channel 03/0 : 26[2] -> 22[2] [send] via NET/AWS Libfabric/2
2: nid007119:146138:155550 [2] NCCL INFO Channel 02/0 : 10[2] -> 6[2] [send] via NET/AWS Libfabric/2
6: nid007123:108291:117899 [2] NCCL INFO Channel 06/0 : 26[2] -> 22[2] [send] via NET/AWS Libfabric/2
2: nid007119:146138:155550 [2] NCCL INFO Channel 03/0 : 10[2] -> 6[2] [send] via NET/AWS Libfabric/2
6: nid007123:108291:117899 [2] NCCL INFO Channel 07/0 : 26[2] -> 22[2] [send] via NET/AWS Libfabric/2
2: nid007119:146138:155550 [2] NCCL INFO Channel 06/0 : 10[2] -> 6[2] [send] via NET/AWS Libfabric/2
2: nid007119:146138:155550 [2] NCCL INFO Channel 07/0 : 10[2] -> 6[2] [send] via NET/AWS Libfabric/2
4: nid007121:116204:125744 [3] NCCL INFO Channel 00/0 : 19[3] -> 18[2] via P2P/CUMEM
7: nid007124:126398:135801 [3] NCCL INFO Channel 01/0 : 31[3] -> 30[2] via P2P/CUMEM
6: nid007123:108292:117897 [3] NCCL INFO Channel 00/0 : 27[3] -> 26[2] via P2P/CUMEM
5: nid007122:66178:75768 [3] NCCL INFO Channel 01/0 : 23[3] -> 22[2] via P2P/CUMEM
3: nid007120:197039:206429 [3] NCCL INFO Channel 01/0 : 15[3] -> 14[2] via P2P/CUMEM
0: nid007116:202903:212649 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM
2: nid007119:146139:155549 [3] NCCL INFO Channel 00/0 : 11[3] -> 10[2] via P2P/CUMEM
1: nid007117:210408:220354 [3] NCCL INFO Channel 01/0 : 7[3] -> 6[2] via P2P/CUMEM
0: nid007116:202902:212650 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM
4: nid007121:116204:125744 [3] NCCL INFO Channel 02/0 : 19[3] -> 18[2] via P2P/CUMEM
6: nid007123:108292:117897 [3] NCCL INFO Channel 02/0 : 27[3] -> 26[2] via P2P/CUMEM
7: nid007124:126398:135801 [3] NCCL INFO Channel 03/0 : 31[3] -> 30[2] via P2P/CUMEM
0: nid007116:202903:212649 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM
5: nid007122:66178:75768 [3] NCCL INFO Channel 03/0 : 23[3] -> 22[2] via P2P/CUMEM
3: nid007120:197039:206429 [3] NCCL INFO Channel 03/0 : 15[3] -> 14[2] via P2P/CUMEM
0: nid007116:202902:212650 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/CUMEM
2: nid007119:146139:155549 [3] NCCL INFO Channel 02/0 : 11[3] -> 10[2] via P2P/CUMEM
1: nid007117:210408:220354 [3] NCCL INFO Channel 03/0 : 7[3] -> 6[2] via P2P/CUMEM
6: nid007123:108292:117897 [3] NCCL INFO Channel 04/0 : 27[3] -> 26[2] via P2P/CUMEM
4: nid007121:116204:125744 [3] NCCL INFO Channel 04/0 : 19[3] -> 18[2] via P2P/CUMEM
4: nid007121:116203:125746 [2] NCCL INFO Channel 00/0 : 18[2] -> 17[1] via P2P/CUMEM
7: nid007124:126398:135801 [3] NCCL INFO Channel 05/0 : 31[3] -> 30[2] via P2P/CUMEM
7: nid007124:126397:135800 [2] NCCL INFO Channel 01/0 : 30[2] -> 29[1] via P2P/CUMEM
6: nid007123:108291:117899 [2] NCCL INFO Channel 00/0 : 26[2] -> 25[1] via P2P/CUMEM
5: nid007122:66178:75768 [3] NCCL INFO Channel 05/0 : 23[3] -> 22[2] via P2P/CUMEM
0: nid007116:202903:212649 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/CUMEM
5: nid007122:66177:75770 [2] NCCL INFO Channel 01/0 : 22[2] -> 21[1] via P2P/CUMEM
3: nid007120:197039:206429 [3] NCCL INFO Channel 05/0 : 15[3] -> 14[2] via P2P/CUMEM
2: nid007119:146139:155549 [3] NCCL INFO Channel 04/0 : 11[3] -> 10[2] via P2P/CUMEM
6: nid007123:108292:117897 [3] NCCL INFO Channel 06/0 : 27[3] -> 26[2] via P2P/CUMEM
3: nid007120:197038:206431 [2] NCCL INFO Channel 01/0 : 14[2] -> 13[1] via P2P/CUMEM
1: nid007117:210408:220354 [3] NCCL INFO Channel 05/0 : 7[3] -> 6[2] via P2P/CUMEM
2: nid007119:146138:155550 [2] NCCL INFO Channel 00/0 : 10[2] -> 9[1] via P2P/CUMEM
1: nid007117:210407:220353 [2] NCCL INFO Channel 01/0 : 6[2] -> 5[1] via P2P/CUMEM
4: nid007121:116204:125744 [3] NCCL INFO Channel 06/0 : 19[3] -> 18[2] via P2P/CUMEM
4: nid007121:116203:125746 [2] NCCL INFO Channel 04/0 : 18[2] -> 17[1] via P2P/CUMEM
7: nid007124:126398:135801 [3] NCCL INFO Channel 07/0 : 31[3] -> 30[2] via P2P/CUMEM
0: nid007116:202903:212649 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/CUMEM
7: nid007124:126397:135800 [2] NCCL INFO Channel 05/0 : 30[2] -> 29[1] via P2P/CUMEM
6: nid007123:108291:117899 [2] NCCL INFO Channel 04/0 : 26[2] -> 25[1] via P2P/CUMEM
5: nid007122:66178:75768 [3] NCCL INFO Channel 07/0 : 23[3] -> 22[2] via P2P/CUMEM
3: nid007120:197039:206429 [3] NCCL INFO Channel 07/0 : 15[3] -> 14[2] via P2P/CUMEM
5: nid007122:66177:75770 [2] NCCL INFO Channel 05/0 : 22[2] -> 21[1] via P2P/CUMEM
3: nid007120:197038:206431 [2] NCCL INFO Channel 05/0 : 14[2] -> 13[1] via P2P/CUMEM
2: nid007119:146139:155549 [3] NCCL INFO Channel 06/0 : 11[3] -> 10[2] via P2P/CUMEM
1: nid007117:210408:220354 [3] NCCL INFO Channel 07/0 : 7[3] -> 6[2] via P2P/CUMEM
2: nid007119:146138:155550 [2] NCCL INFO Channel 04/0 : 10[2] -> 9[1] via P2P/CUMEM
1: nid007117:210407:220353 [2] NCCL INFO Channel 05/0 : 6[2] -> 5[1] via P2P/CUMEM
1: nid007117:210405:220355 [0] NCCL INFO Connected all trees
4: nid007121:116201:125747 [0] NCCL INFO Connected all trees
5: nid007122:66175:75769 [0] NCCL INFO Connected all trees
2: nid007119:146136:155547 [0] NCCL INFO Connected all trees
7: nid007124:126395:135803 [0] NCCL INFO Connected all trees
6: nid007123:108289:117900 [0] NCCL INFO Connected all trees
0: nid007116:202900:212648 [0] NCCL INFO Connected all trees
3: nid007120:197036:206428 [0] NCCL INFO Connected all trees
0: nid007116:202901:212647 [1] NCCL INFO Connected all trees
4: nid007121:116202:125745 [1] NCCL INFO Connected all trees
3: nid007120:197037:206430 [1] NCCL INFO Connected all trees
0: nid007116:202903:212649 [3] NCCL INFO Connected all trees
5: nid007122:66176:75767 [1] NCCL INFO Connected all trees
0: nid007116:202902:212650 [2] NCCL INFO Connected all trees
6: nid007123:108290:117898 [1] NCCL INFO Connected all trees
1: nid007117:210406:220352 [1] NCCL INFO Connected all trees
7: nid007124:126396:135802 [1] NCCL INFO Connected all trees
4: nid007121:116204:125744 [3] NCCL INFO Connected all trees
5: nid007122:66178:75768 [3] NCCL INFO Connected all trees
3: nid007120:197039:206429 [3] NCCL INFO Connected all trees
6: nid007123:108292:117897 [3] NCCL INFO Connected all trees
1: nid007117:210408:220354 [3] NCCL INFO Connected all trees
4: nid007121:116203:125746 [2] NCCL INFO Connected all trees
2: nid007119:146137:155548 [1] NCCL INFO Connected all trees
2: nid007119:146139:155549 [3] NCCL INFO Connected all trees
3: nid007120:197038:206431 [2] NCCL INFO Connected all trees
5: nid007122:66177:75770 [2] NCCL INFO Connected all trees
7: nid007124:126398:135801 [3] NCCL INFO Connected all trees
7: nid007124:126397:135800 [2] NCCL INFO Connected all trees
6: nid007123:108291:117899 [2] NCCL INFO Connected all trees
1: nid007117:210407:220353 [2] NCCL INFO Connected all trees
2: nid007119:146138:155550 [2] NCCL INFO Connected all trees
3: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
3:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
2:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
0:   0%|          | 1/1112 [00:14<4:24:12, 14.27s/it]  0%|          | 2/1112 [00:18<2:37:22,  8.51s/it]/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
0:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
6: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
6:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
2:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
7: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
7:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
1: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
1:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
1: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
1:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
6: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
6:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
5: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
5:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
5: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
5:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
4: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
4:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
6: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
6:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
5: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
5:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
2:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
3: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
3:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
6: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
6:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
4: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
4:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
5: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
5:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
7: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
7:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
0:   0%|          | 3/1112 [00:24<2:13:31,  7.22s/it]  0%|          | 4/1112 [00:30<2:01:20,  6.57s/it]  0%|          | 5/1112 [00:35<1:54:37,  6.21s/it]  1%|          | 6/1112 [00:41<1:54:39,  6.22s/it]  1%|          | 7/1112 [00:47<1:50:50,  6.02s/it]  1%|          | 8/1112 [00:53<1:48:15,  5.88s/it]  1%|          | 9/1112 [00:58<1:46:18,  5.78s/it]/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
0:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
0: {'loss': 1.0865, 'grad_norm': 5.95046560851166, 'learning_rate': 8.035714285714287e-07, 'epoch': 0.01}
4: /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
4:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
0:   1%|          | 10/1112 [01:04<1:45:00,  5.72s/it]                                                     1%|          | 10/1112 [01:04<1:45:00,  5.72s/it]  1%|          | 11/1112 [01:09<1:43:20,  5.63s/it]  1%|          | 12/1112 [01:15<1:42:09,  5.57s/it]  1%|          | 13/1112 [01:20<1:42:10,  5.58s/it]  1%|▏         | 14/1112 [01:26<1:41:42,  5.56s/it]/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/cudnn/MHA.cpp:667.)
0:   return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
0: {'loss': 0.9672, 'grad_norm': 2.933785789702049, 'learning_rate': 1.6964285714285717e-06, 'epoch': 0.02}
0:   1%|▏         | 15/1112 [01:31<1:41:07,  5.53s/it]  1%|▏         | 16/1112 [01:37<1:41:14,  5.54s/it]  2%|▏         | 17/1112 [01:41<1:35:12,  5.22s/it]  2%|▏         | 18/1112 [01:47<1:37:09,  5.33s/it]  2%|▏         | 19/1112 [01:52<1:39:03,  5.44s/it]  2%|▏         | 20/1112 [01:58<1:40:56,  5.55s/it]                                                     2%|▏         | 20/1112 [01:58<1:40:56,  5.55s/it]  2%|▏         | 21/1112 [02:04<1:41:52,  5.60s/it]  2%|▏         | 22/1112 [02:10<1:43:05,  5.67s/it]  2%|▏         | 23/1112 [02:16<1:43:27,  5.70s/it]  2%|▏         | 24/1112 [02:20<1:38:15,  5.42s/it]  2%|▏         | 25/1112 [02:26<1:40:17,  5.54s/it]  2%|▏         | 26/1112 [02:32<1:42:03,  5.64s/it]  2%|▏         | 27/1112 [02:38<1:42:54,  5.69s/it]  3%|▎         | 28/1112 [02:44<1:43:36,  5.73s/it]  3%|▎         | 29/1112 [02:49<1:43:50,  5.75s/it]  3%|▎         | 30/1112 [02:55<1:43:20,  5.73s/it]                                                   
0: {'loss': 0.7632, 'grad_norm': 3.298611006408538, 'learning_rate': 2.5892857142857148e-06, 'epoch': 0.03}
0: {'loss': 0.6396, 'grad_norm': 1.5797029725066696, 'learning_rate': 3.482142857142857e-06, 'epoch': 0.04}
0:   3%|▎         | 30/1112 [02:55<1:43:20,  5.73s/it]  3%|▎         | 31/1112 [03:01<1:44:19,  5.79s/it]  3%|▎         | 32/1112 [03:07<1:44:23,  5.80s/it]  3%|▎         | 33/1112 [03:13<1:44:43,  5.82s/it]  3%|▎         | 34/1112 [03:19<1:44:35,  5.82s/it]  3%|▎         | 35/1112 [03:24<1:44:06,  5.80s/it]  3%|▎         | 36/1112 [03:30<1:43:34,  5.78s/it]  3%|▎         | 37/1112 [03:36<1:43:26,  5.77s/it]  3%|▎         | 38/1112 [03:42<1:43:13,  5.77s/it]  4%|▎         | 39/1112 [03:47<1:43:23,  5.78s/it]  4%|▎         | 40/1112 [03:53<1:43:33,  5.80s/it]                                                     4%|▎         | 40/1112 [03:53<1:43:33,  5.80s/it]  4%|▎         | 41/1112 [03:59<1:43:44,  5.81s/it]  4%|▍         | 42/1112 [04:05<1:44:05,  5.84s/it]  4%|▍         | 43/1112 [04:11<1:43:38,  5.82s/it]  4%|▍         | 44/1112 [04:17<1:43:32,  5.82s/it]  4%|▍         | 45/1112 [04:22<1:43:15,  5.81s/it]  4%|▍         | 46/1112 [04:28<1:43:09,  5.81s/it
0: {'loss': 0.5669, 'grad_norm': 2.4966479045739813, 'learning_rate': 4.3750000000000005e-06, 'epoch': 0.04}
0: {'loss': 0.5258, 'grad_norm': 2.611574891594703, 'learning_rate': 5.267857142857144e-06, 'epoch': 0.05}
0: ]  4%|▍         | 47/1112 [04:34<1:42:45,  5.79s/it]  4%|▍         | 48/1112 [04:40<1:42:50,  5.80s/it]  4%|▍         | 49/1112 [04:46<1:42:51,  5.81s/it]  4%|▍         | 50/1112 [04:51<1:42:51,  5.81s/it]                                                     4%|▍         | 50/1112 [04:51<1:42:51,  5.81s/it]  5%|▍         | 51/1112 [04:57<1:42:37,  5.80s/it]  5%|▍         | 52/1112 [05:03<1:42:01,  5.77s/it]  5%|▍         | 53/1112 [05:09<1:41:53,  5.77s/it]  5%|▍         | 54/1112 [05:14<1:42:14,  5.80s/it]  5%|▍         | 55/1112 [05:20<1:41:40,  5.77s/it]  5%|▌         | 56/1112 [05:26<1:41:49,  5.79s/it]  5%|▌         | 57/1112 [05:32<1:42:04,  5.81s/it]  5%|▌         | 58/1112 [05:37<1:36:26,  5.49s/it]  5%|▌         | 59/1112 [05:42<1:38:06,  5.59s/it]  5%|▌         | 60/1112 [05:48<1:39:52,  5.70s/it]                                                     5%|▌         | 60/1112 [05:48<1:39:52,  5.70s/it]  5%|▌         | 61/1112 [05:54<1:40:29,  5.74s/it
0: {'loss': 0.4939, 'grad_norm': 1.4650019865123485, 'learning_rate': 6.160714285714286e-06, 'epoch': 0.06}
0: ]  6%|▌         | 62/1112 [05:59<1:35:26,  5.45s/it]  6%|▌         | 63/1112 [06:05<1:37:01,  5.55s/it]  6%|▌         | 64/1112 [06:11<1:38:25,  5.64s/it]  6%|▌         | 65/1112 [06:15<1:33:51,  5.38s/it]  6%|▌         | 66/1112 [06:21<1:35:49,  5.50s/it]  6%|▌         | 67/1112 [06:27<1:37:30,  5.60s/it]  6%|▌         | 68/1112 [06:33<1:38:09,  5.64s/it]  6%|▌         | 69/1112 [06:39<1:39:06,  5.70s/it]  6%|▋         | 70/1112 [06:44<1:39:34,  5.73s/it]                                                     6%|▋         | 70/1112 [06:44<1:39:34,  5.73s/it]  6%|▋         | 71/1112 [06:50<1:39:35,  5.74s/it]  6%|▋         | 72/1112 [06:56<1:39:58,  5.77s/it]  7%|▋         | 73/1112 [07:02<1:39:45,  5.76s/it]  7%|▋         | 74/1112 [07:06<1:34:22,  5.46s/it]  7%|▋         | 75/1112 [07:12<1:36:21,  5.57s/it]  7%|▋         | 76/1112 [07:18<1:37:18,  5.64s/it]  7%|▋         | 77/1112 [07:24<1:38:53,  5.73s/it]  7%|▋         | 78/1112 [07:30<1:39:01,  5.75s/i
0: {'loss': 0.4757, 'grad_norm': 1.3922393994844384, 'learning_rate': 7.053571428571429e-06, 'epoch': 0.07}
0: {'loss': 0.4591, 'grad_norm': 1.834939434700146, 'learning_rate': 7.946428571428571e-06, 'epoch': 0.08}
0: t]  7%|▋         | 79/1112 [07:36<1:39:20,  5.77s/it]  7%|▋         | 80/1112 [07:41<1:39:14,  5.77s/it]                                                     7%|▋         | 80/1112 [07:41<1:39:14,  5.77s/it]  7%|▋         | 81/1112 [07:47<1:39:23,  5.78s/it]  7%|▋         | 82/1112 [07:53<1:39:08,  5.78s/it]  7%|▋         | 83/1112 [07:59<1:39:18,  5.79s/it]  8%|▊         | 84/1112 [08:03<1:33:33,  5.46s/it]  8%|▊         | 85/1112 [08:09<1:35:08,  5.56s/it]  8%|▊         | 86/1112 [08:15<1:36:15,  5.63s/it]  8%|▊         | 87/1112 [08:21<1:36:53,  5.67s/it]  8%|▊         | 88/1112 [08:27<1:37:47,  5.73s/it]  8%|▊         | 89/1112 [08:33<1:38:02,  5.75s/it]  8%|▊         | 90/1112 [08:38<1:38:24,  5.78s/it]                                                     8%|▊         | 90/1112 [08:38<1:38:24,  5.78s/it]  8%|▊         | 91/1112 [08:44<1:38:09,  5.77s/it]  8%|▊         | 92/1112 [08:49<1:32:49,  5.46s/it]  8%|▊         | 93/1112 [08:55<1:34:58,  5.59s/i
0: {'loss': 0.4485, 'grad_norm': 1.7970638755802748, 'learning_rate': 8.839285714285714e-06, 'epoch': 0.09}
0: t]  8%|▊         | 94/1112 [09:00<1:35:38,  5.64s/it]  9%|▊         | 95/1112 [09:06<1:36:09,  5.67s/it]  9%|▊         | 96/1112 [09:12<1:36:15,  5.68s/it]  9%|▊         | 97/1112 [09:18<1:37:44,  5.78s/it]  9%|▉         | 98/1112 [09:24<1:37:31,  5.77s/it]  9%|▉         | 99/1112 [09:30<1:38:04,  5.81s/it]  9%|▉         | 100/1112 [09:35<1:37:40,  5.79s/it]                                                      9%|▉         | 100/1112 [09:35<1:37:40,  5.79s/it]  9%|▉         | 101/1112 [09:41<1:37:24,  5.78s/it]  9%|▉         | 102/1112 [09:47<1:37:15,  5.78s/it]  9%|▉         | 103/1112 [09:53<1:37:37,  5.80s/it]  9%|▉         | 104/1112 [09:59<1:37:31,  5.81s/it]  9%|▉         | 105/1112 [10:03<1:32:04,  5.49s/it] 10%|▉         | 106/1112 [10:09<1:33:40,  5.59s/it] 10%|▉         | 107/1112 [10:15<1:35:09,  5.68s/it] 10%|▉         | 108/1112 [10:20<1:30:33,  5.41s/it] 10%|▉         | 109/1112 [10:26<1:32:10,  5.51s/it] 10%|▉         | 110/1112 [10:31<1:3
0: {'loss': 0.4486, 'grad_norm': 10.687789064527077, 'learning_rate': 9.732142857142858e-06, 'epoch': 0.1}
0: {'loss': 0.439, 'grad_norm': 6.117694977923916, 'learning_rate': 9.998791022184921e-06, 'epoch': 0.11}
0: 3:34,  5.60s/it]                                                     10%|▉         | 110/1112 [10:31<1:33:34,  5.60s/it] 10%|▉         | 111/1112 [10:36<1:29:24,  5.36s/it] 10%|█         | 112/1112 [10:42<1:31:55,  5.52s/it] 10%|█         | 113/1112 [10:48<1:33:07,  5.59s/it] 10%|█         | 114/1112 [10:54<1:34:18,  5.67s/it] 10%|█         | 115/1112 [10:58<1:29:42,  5.40s/it] 10%|█         | 116/1112 [11:04<1:31:19,  5.50s/it] 11%|█         | 117/1112 [11:10<1:33:00,  5.61s/it] 11%|█         | 118/1112 [11:16<1:34:30,  5.71s/it] 11%|█         | 119/1112 [11:22<1:34:47,  5.73s/it] 11%|█         | 120/1112 [11:26<1:29:53,  5.44s/it]                                                     11%|█         | 120/1112 [11:26<1:29:53,  5.44s/it] 11%|█         | 121/1112 [11:31<1:26:31,  5.24s/it] 11%|█         | 122/1112 [11:37<1:29:24,  5.42s/it] 11%|█         | 123/1112 [11:42<1:26:06,  5.22s/it] 11%|█         | 124/1112 [11:48<1:28:49,  5.39s/it] 11%|█         | 
0: {'loss': 0.4322, 'grad_norm': 1.2776631624707226, 'learning_rate': 9.992870905597549e-06, 'epoch': 0.12}
0: 125/1112 [11:54<1:31:23,  5.56s/it] 11%|█▏        | 126/1112 [12:00<1:35:56,  5.84s/it] 11%|█▏        | 127/1112 [12:06<1:38:04,  5.97s/it] 12%|█▏        | 128/1112 [12:13<1:39:46,  6.08s/it] 12%|█▏        | 129/1112 [12:20<1:44:11,  6.36s/it] 12%|█▏        | 130/1112 [12:26<1:43:51,  6.35s/it]                                                     12%|█▏        | 130/1112 [12:26<1:43:51,  6.35s/it] 12%|█▏        | 131/1112 [12:32<1:43:12,  6.31s/it] 12%|█▏        | 132/1112 [12:38<1:41:27,  6.21s/it] 12%|█▏        | 133/1112 [12:44<1:39:18,  6.09s/it] 12%|█▏        | 134/1112 [12:50<1:37:40,  5.99s/it] 12%|█▏        | 135/1112 [12:56<1:36:52,  5.95s/it] 12%|█▏        | 136/1112 [13:01<1:35:54,  5.90s/it] 12%|█▏        | 137/1112 [13:06<1:30:16,  5.56s/it] 12%|█▏        | 138/1112 [13:11<1:26:06,  5.30s/it] 12%|█▎        | 139/1112 [13:17<1:28:19,  5.45s/it] 13%|█▎        | 140/1112 [13:22<1:29:53,  5.55s/it]                      
0: {'loss': 0.4228, 'grad_norm': 1.4325882765509277, 'learning_rate': 9.982023428222963e-06, 'epoch': 0.13}
0: {'loss': 0.4148, 'grad_norm': 1.4794841641404273, 'learning_rate': 9.966259295211698e-06, 'epoch': 0.13}
0:                                13%|█▎        | 140/1112 [13:22<1:29:53,  5.55s/it] 13%|█▎        | 141/1112 [13:28<1:30:37,  5.60s/it] 13%|█▎        | 142/1112 [13:34<1:31:10,  5.64s/it] 13%|█▎        | 143/1112 [13:39<1:26:39,  5.37s/it] 13%|█▎        | 144/1112 [13:44<1:28:21,  5.48s/it] 13%|█▎        | 145/1112 [13:50<1:29:32,  5.56s/it] 13%|█▎        | 146/1112 [13:56<1:30:06,  5.60s/it] 13%|█▎        | 147/1112 [14:02<1:30:35,  5.63s/it] 13%|█▎        | 148/1112 [14:07<1:32:01,  5.73s/it] 13%|█▎        | 149/1112 [14:13<1:32:03,  5.74s/it] 13%|█▎        | 150/1112 [14:19<1:32:03,  5.74s/it]                                                     13%|█▎        | 150/1112 [14:19<1:32:03,  5.74s/it] 14%|█▎        | 151/1112 [14:24<1:26:55,  5.43s/it] 14%|█▎        | 152/1112 [14:29<1:28:20,  5.52s/it] 14%|█▍        | 153/1112 [14:35<1:29:44,  5.61s/it] 14%|█▍        | 154/1112 [14:41<1:30:11,  5.65s/it] 14%|█▍        | 155/1
0: {'loss': 0.4159, 'grad_norm': 1.1936607633322938, 'learning_rate': 9.94559406385981e-06, 'epoch': 0.14}
0: 112 [14:46<1:25:48,  5.38s/it] 14%|█▍        | 156/1112 [14:52<1:28:16,  5.54s/it] 14%|█▍        | 157/1112 [14:58<1:29:39,  5.63s/it] 14%|█▍        | 158/1112 [15:02<1:25:21,  5.37s/it] 14%|█▍        | 159/1112 [15:08<1:27:19,  5.50s/it] 14%|█▍        | 160/1112 [15:14<1:28:26,  5.57s/it]                                                     14%|█▍        | 160/1112 [15:14<1:28:26,  5.57s/it] 14%|█▍        | 161/1112 [15:19<1:24:40,  5.34s/it] 15%|█▍        | 162/1112 [15:23<1:21:54,  5.17s/it] 15%|█▍        | 163/1112 [15:28<1:20:03,  5.06s/it] 15%|█▍        | 164/1112 [15:33<1:18:07,  4.94s/it] 15%|█▍        | 165/1112 [15:38<1:17:29,  4.91s/it] 15%|█▍        | 166/1112 [15:43<1:21:22,  5.16s/it] 15%|█▌        | 167/1112 [15:49<1:24:01,  5.33s/it] 15%|█▌        | 168/1112 [15:55<1:25:40,  5.45s/it] 15%|█▌        | 169/1112 [15:59<1:17:25,  4.93s/it] 15%|█▌        | 170/1112 [16:03<1:16:31,  4.87s/it]                           
0: {'loss': 0.4112, 'grad_norm': 1.2304618205136189, 'learning_rate': 9.920048128255699e-06, 'epoch': 0.15}
0: {'loss': 0.4052, 'grad_norm': 1.1112296304855394, 'learning_rate': 9.88964669915361e-06, 'epoch': 0.16}
0:                           15%|█▌        | 170/1112 [16:03<1:16:31,  4.87s/it] 15%|█▌        | 171/1112 [16:09<1:20:49,  5.15s/it] 15%|█▌        | 172/1112 [16:15<1:23:44,  5.34s/it] 16%|█▌        | 173/1112 [16:21<1:25:35,  5.47s/it] 16%|█▌        | 174/1112 [16:26<1:26:30,  5.53s/it] 16%|█▌        | 175/1112 [16:32<1:27:18,  5.59s/it] 16%|█▌        | 176/1112 [16:38<1:28:25,  5.67s/it] 16%|█▌        | 177/1112 [16:43<1:24:09,  5.40s/it] 16%|█▌        | 178/1112 [16:49<1:25:42,  5.51s/it] 16%|█▌        | 179/1112 [16:54<1:26:49,  5.58s/it] 16%|█▌        | 180/1112 [17:00<1:27:14,  5.62s/it]                                                     16%|█▌        | 180/1112 [17:00<1:27:14,  5.62s/it] 16%|█▋        | 181/1112 [17:05<1:23:12,  5.36s/it] 16%|█▋        | 182/1112 [17:09<1:20:08,  5.17s/it] 16%|█▋        | 183/1112 [17:15<1:22:46,  5.35s/it] 17%|█▋        | 184/1112 [17:21<1:24:37,  5.47s/it] 17%|█▋        | 185/1112 [
0: {'loss': 0.3999, 'grad_norm': 1.18479026833682, 'learning_rate': 9.854419779093656e-06, 'epoch': 0.17}
0: 17:26<1:20:55,  5.24s/it] 17%|█▋        | 186/1112 [17:31<1:22:57,  5.37s/it] 17%|█▋        | 187/1112 [17:36<1:20:27,  5.22s/it] 17%|█▋        | 188/1112 [17:42<1:22:48,  5.38s/it] 17%|█▋        | 189/1112 [17:48<1:24:05,  5.47s/it] 17%|█▋        | 190/1112 [17:53<1:25:01,  5.53s/it]                                                     17%|█▋        | 190/1112 [17:53<1:25:01,  5.53s/it] 17%|█▋        | 191/1112 [17:59<1:26:23,  5.63s/it] 17%|█▋        | 192/1112 [18:05<1:26:50,  5.66s/it] 17%|█▋        | 193/1112 [18:11<1:27:40,  5.72s/it] 17%|█▋        | 194/1112 [18:16<1:22:58,  5.42s/it] 18%|█▊        | 195/1112 [18:21<1:24:17,  5.52s/it] 18%|█▊        | 196/1112 [18:27<1:25:00,  5.57s/it] 18%|█▊        | 197/1112 [18:32<1:21:09,  5.32s/it] 18%|█▊        | 198/1112 [18:37<1:22:57,  5.45s/it] 18%|█▊        | 199/1112 [18:42<1:19:33,  5.23s/it] 18%|█▊        | 200/1112 [18:48<1:21:44,  5.38s/it]                                
0: {'loss': 0.4013, 'grad_norm': 1.1803221689691885, 'learning_rate': 9.814402132792939e-06, 'epoch': 0.18}
0: {'loss': 0.3924, 'grad_norm': 1.315557097393816, 'learning_rate': 9.769633252836969e-06, 'epoch': 0.19}
0:                      18%|█▊        | 200/1112 [18:48<1:21:44,  5.38s/it] 18%|█▊        | 201/1112 [18:53<1:18:37,  5.18s/it] 18%|█▊        | 202/1112 [18:58<1:21:07,  5.35s/it] 18%|█▊        | 203/1112 [19:04<1:23:05,  5.48s/it] 18%|█▊        | 204/1112 [19:08<1:15:00,  4.96s/it] 18%|█▊        | 205/1112 [19:13<1:13:54,  4.89s/it] 19%|█▊        | 206/1112 [19:18<1:17:48,  5.15s/it] 19%|█▊        | 207/1112 [19:24<1:20:42,  5.35s/it] 19%|█▊        | 208/1112 [19:30<1:22:14,  5.46s/it] 19%|█▉        | 209/1112 [19:36<1:23:26,  5.54s/it] 19%|█▉        | 210/1112 [19:41<1:24:29,  5.62s/it]                                                     19%|█▉        | 210/1112 [19:41<1:24:29,  5.62s/it] 19%|█▉        | 211/1112 [19:47<1:25:05,  5.67s/it] 19%|█▉        | 212/1112 [19:52<1:20:47,  5.39s/it] 19%|█▉        | 213/1112 [19:57<1:17:43,  5.19s/it] 19%|█▉        | 214/1112 [20:03<1:20:42,  5.39s/it] 19%|█▉        | 215/1112 [20:07
0: {'loss': 0.3847, 'grad_norm': 1.2899888699780486, 'learning_rate': 9.72015732070525e-06, 'epoch': 0.2}
0: <1:17:43,  5.20s/it] 19%|█▉        | 216/1112 [20:13<1:19:50,  5.35s/it] 20%|█▉        | 217/1112 [20:18<1:17:03,  5.17s/it] 20%|█▉        | 218/1112 [20:23<1:15:29,  5.07s/it] 20%|█▉        | 219/1112 [20:28<1:18:40,  5.29s/it] 20%|█▉        | 220/1112 [20:34<1:20:47,  5.43s/it]                                                     20%|█▉        | 220/1112 [20:34<1:20:47,  5.43s/it] 20%|█▉        | 221/1112 [20:40<1:22:03,  5.53s/it] 20%|█▉        | 222/1112 [20:46<1:23:18,  5.62s/it] 20%|██        | 223/1112 [20:52<1:24:11,  5.68s/it] 20%|██        | 224/1112 [20:56<1:19:52,  5.40s/it] 20%|██        | 225/1112 [21:02<1:21:18,  5.50s/it] 20%|██        | 226/1112 [21:08<1:22:33,  5.59s/it] 20%|██        | 227/1112 [21:14<1:23:30,  5.66s/it] 21%|██        | 228/1112 [21:19<1:24:01,  5.70s/it] 21%|██        | 229/1112 [21:24<1:19:30,  5.40s/it] 21%|██        | 230/1112 [21:30<1:21:22,  5.54s/it]                                     
0: {'loss': 0.3854, 'grad_norm': 2.2271985150741855, 'learning_rate': 9.666023163169493e-06, 'epoch': 0.21}
0: {'loss': 0.3907, 'grad_norm': 1.1166267083697772, 'learning_rate': 9.607284204107493e-06, 'epoch': 0.22}
0:                 21%|██        | 230/1112 [21:30<1:21:22,  5.54s/it] 21%|██        | 231/1112 [21:36<1:22:02,  5.59s/it] 21%|██        | 232/1112 [21:41<1:22:43,  5.64s/it] 21%|██        | 233/1112 [21:46<1:18:49,  5.38s/it] 21%|██        | 234/1112 [21:50<1:11:27,  4.88s/it] 21%|██        | 235/1112 [21:56<1:15:12,  5.15s/it] 21%|██        | 236/1112 [22:01<1:17:36,  5.32s/it] 21%|██▏       | 237/1112 [22:07<1:19:22,  5.44s/it] 21%|██▏       | 238/1112 [22:13<1:20:40,  5.54s/it] 21%|██▏       | 239/1112 [22:19<1:21:38,  5.61s/it] 22%|██▏       | 240/1112 [22:24<1:22:07,  5.65s/it]                                                     22%|██▏       | 240/1112 [22:24<1:22:07,  5.65s/it] 22%|██▏       | 241/1112 [22:30<1:22:40,  5.70s/it] 22%|██▏       | 242/1112 [22:36<1:22:48,  5.71s/it] 22%|██▏       | 243/1112 [22:42<1:22:50,  5.72s/it] 22%|██▏       | 244/1112 [22:47<1:22:36,  5.71s/it] 22%|██▏       | 
0: {'loss': 0.3796, 'grad_norm': 1.0650687790111126, 'learning_rate': 9.543998411780202e-06, 'epoch': 0.22}
0: 245/1112 [22:53<1:22:45,  5.73s/it] 22%|██▏       | 246/1112 [22:58<1:18:07,  5.41s/it] 22%|██▏       | 247/1112 [23:03<1:14:57,  5.20s/it] 22%|██▏       | 248/1112 [23:07<1:12:50,  5.06s/it] 22%|██▏       | 249/1112 [23:13<1:15:46,  5.27s/it] 22%|██▏       | 250/1112 [23:18<1:13:14,  5.10s/it]                                                     22%|██▏       | 250/1112 [23:18<1:13:14,  5.10s/it] 23%|██▎       | 251/1112 [23:23<1:15:49,  5.28s/it] 23%|██▎       | 252/1112 [23:28<1:13:09,  5.10s/it] 23%|██▎       | 253/1112 [23:33<1:11:27,  4.99s/it] 23%|██▎       | 254/1112 [23:39<1:14:37,  5.22s/it] 23%|██▎       | 255/1112 [23:43<1:12:19,  5.06s/it] 23%|██▎       | 256/1112 [23:49<1:15:13,  5.27s/it] 23%|██▎       | 257/1112 [23:54<1:12:36,  5.10s/it] 23%|██▎       | 258/1112 [24:00<1:15:47,  5.32s/it] 23%|██▎       | 259/1112 [24:05<1:17:54,  5.48s/it] 23%|██▎       | 260/1112 [24:11<1:19:10,  
0: {'loss': 0.3823, 'grad_norm': 0.9658240525902696, 'learning_rate': 9.476228241624059e-06, 'epoch': 0.23}
0: {'loss': 0.3811, 'grad_norm': 1.3805949565906221, 'learning_rate': 9.404040574615018e-06, 'epoch': 0.24}
0: 5.58s/it]                                                     23%|██▎       | 260/1112 [24:11<1:19:10,  5.58s/it] 23%|██▎       | 261/1112 [24:16<1:15:28,  5.32s/it] 24%|██▎       | 262/1112 [24:22<1:16:55,  5.43s/it] 24%|██▎       | 263/1112 [24:27<1:18:00,  5.51s/it] 24%|██▎       | 264/1112 [24:33<1:18:59,  5.59s/it] 24%|██▍       | 265/1112 [24:39<1:19:36,  5.64s/it] 24%|██▍       | 266/1112 [24:45<1:20:32,  5.71s/it] 24%|██▍       | 267/1112 [24:51<1:21:51,  5.81s/it] 24%|██▍       | 268/1112 [24:57<1:21:16,  5.78s/it] 24%|██▍       | 269/1112 [25:01<1:16:29,  5.44s/it] 24%|██▍       | 270/1112 [25:08<1:20:10,  5.71s/it]                                                     24%|██▍       | 270/1112 [25:08<1:20:10,  5.71s/it] 24%|██▍       | 271/1112 [25:13<1:17:36,  5.54s/it] 24%|██▍       | 272/1112 [25:18<1:16:07,  5.44s/it] 25%|██▍       | 273/1112 [25:23<1:15:36,  5.41s/it] 25%|██▍       
0: {'loss': 0.38, 'grad_norm': 1.2504147035005475, 'learning_rate': 9.327506651265096e-06, 'epoch': 0.25}
0: | 274/1112 [25:29<1:19:00,  5.66s/it] 25%|██▍       | 275/1112 [25:33<1:10:48,  5.08s/it] 25%|██▍       | 276/1112 [25:38<1:11:35,  5.14s/it] 25%|██▍       | 277/1112 [25:45<1:16:51,  5.52s/it] 25%|██▌       | 278/1112 [25:51<1:18:04,  5.62s/it] 25%|██▌       | 279/1112 [25:56<1:14:59,  5.40s/it] 25%|██▌       | 280/1112 [26:00<1:11:51,  5.18s/it]                                                     25%|██▌       | 280/1112 [26:00<1:11:51,  5.18s/it] 25%|██▌       | 281/1112 [26:06<1:13:44,  5.32s/it] 25%|██▌       | 282/1112 [26:11<1:12:46,  5.26s/it] 25%|██▌       | 283/1112 [26:17<1:14:24,  5.38s/it] 26%|██▌       | 284/1112 [26:21<1:11:37,  5.19s/it] 26%|██▌       | 285/1112 [26:27<1:13:44,  5.35s/it] 26%|██▌       | 286/1112 [26:33<1:15:27,  5.48s/it] 26%|██▌       | 287/1112 [26:39<1:16:35,  5.57s/it] 26%|██▌       | 288/1112 [26:44<1:17:14,  5.62s/it] 26%|██▌       | 289/1112 [26:50<1:17:30,
0: {'loss': 0.3686, 'grad_norm': 1.0301774432060482, 'learning_rate': 9.246702001316584e-06, 'epoch': 0.26}
0: {'loss': 0.3692, 'grad_norm': 1.2566782015883715, 'learning_rate': 9.161706369203319e-06, 'epoch': 0.27}
0:   5.65s/it] 26%|██▌       | 290/1112 [26:56<1:18:19,  5.72s/it]                                                     26%|██▌       | 290/1112 [26:56<1:18:19,  5.72s/it] 26%|██▌       | 291/1112 [27:02<1:18:02,  5.70s/it] 26%|██▋       | 292/1112 [27:08<1:18:31,  5.75s/it] 26%|██▋       | 293/1112 [27:12<1:14:08,  5.43s/it] 26%|██▋       | 294/1112 [27:16<1:06:59,  4.91s/it] 27%|██▋       | 295/1112 [27:22<1:10:19,  5.17s/it] 27%|██▋       | 296/1112 [27:26<1:08:16,  5.02s/it] 27%|██▋       | 297/1112 [27:32<1:11:04,  5.23s/it] 27%|██▋       | 298/1112 [27:37<1:09:06,  5.09s/it] 27%|██▋       | 299/1112 [27:42<1:07:33,  4.99s/it] 27%|██▋       | 300/1112 [27:46<1:06:50,  4.94s/it]                                                     27%|██▋       | 300/1112 [27:46<1:06:50,  4.94s/it] 27%|██▋       | 301/1112 [27:52<1:10:02,  5.18s/it] 27%|██▋       | 302/1112 [27:58<1:12:01,  5.34s/it] 27%|██▋     
0: {'loss': 0.3711, 'grad_norm': 1.0885037854128483, 'learning_rate': 9.072603635352548e-06, 'epoch': 0.28}
0:   | 303/1112 [28:04<1:13:20,  5.44s/it] 27%|██▋       | 304/1112 [28:09<1:14:53,  5.56s/it] 27%|██▋       | 305/1112 [28:15<1:15:35,  5.62s/it] 28%|██▊       | 306/1112 [28:21<1:16:04,  5.66s/it] 28%|██▊       | 307/1112 [28:26<1:12:04,  5.37s/it] 28%|██▊       | 308/1112 [28:31<1:13:04,  5.45s/it] 28%|██▊       | 309/1112 [28:37<1:14:23,  5.56s/it] 28%|██▊       | 310/1112 [28:42<1:10:48,  5.30s/it]                                                     28%|██▊       | 310/1112 [28:42<1:10:48,  5.30s/it] 28%|██▊       | 311/1112 [28:47<1:08:48,  5.15s/it] 28%|██▊       | 312/1112 [28:52<1:10:56,  5.32s/it] 28%|██▊       | 313/1112 [28:57<1:08:17,  5.13s/it] 28%|██▊       | 314/1112 [29:03<1:10:44,  5.32s/it] 28%|██▊       | 315/1112 [29:09<1:12:44,  5.48s/it] 28%|██▊       | 316/1112 [29:12<1:05:42,  4.95s/it] 29%|██▊       | 317/1112 [29:17<1:04:49,  4.89s/it] 29%|██▊       | 318/1112 [29:22<1:04:0
0: {'loss': 0.3601, 'grad_norm': 1.0564223790913418, 'learning_rate': 8.97948173340508e-06, 'epoch': 0.29}
2: [nid007119:146137:0:155493] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x460)
2: ==== backtrace (tid: 155493) ====
2:  0 0x00000000009820fc nvrtcCreateProgram()  ???:0
2: =================================
2: W0711 12:18:19.631000 70369729251424 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 146136 closing signal SIGTERM
2: W0711 12:18:19.634000 70369729251424 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 146138 closing signal SIGTERM
2: W0711 12:18:19.641000 70369729251424 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 146139 closing signal SIGTERM
2: E0711 12:18:19.656000 70369729251424 torch/distributed/elastic/multiprocessing/api.py:863] failed (exitcode: -11) local_rank: 1 (pid: 146137) of binary: /usr/bin/python
2: Traceback (most recent call last):
2:   File "/usr/local/bin/torchrun", line 33, in <module>
2:     sys.exit(load_entry_point('torch==2.5.0a0+872d972e41.nv24.8', 'console_scripts', 'torchrun')())
2:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
2:     return f(*args, **kwargs)
2:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 919, in main
2:     run(args)
2:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 910, in run
2:     elastic_launch(
2:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
2:     return launch_agent(self._config, self._entrypoint, list(args))
2:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
2:     raise ChildFailedError(
2: torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
2: ============================================================
2: /workspace/LLaMA-Factory/src/llamafactory/launcher.py FAILED
2: ------------------------------------------------------------
2: Failures:
2:   <NO_OTHER_FAILURES>
2: ------------------------------------------------------------
2: Root Cause (first observed failure):
2: [0]:
2:   time      : 2025-07-11_12:18:19
2:   host      : nid007119
2:   rank      : 9 (local_rank: 1)
2:   exitcode  : -11 (pid: 146137)
2:   error_file: <N/A>
2:   traceback : Signal 11 (SIGSEGV) received by PID 146137
2: ============================================================
2: Traceback (most recent call last):
2:   File "/usr/local/bin/llamafactory-cli", line 8, in <module>
2:     sys.exit(main())
2:   File "/workspace/LLaMA-Factory/src/llamafactory/cli.py", line 130, in main
2:     process = subprocess.run(
2:   File "/usr/lib/python3.10/subprocess.py", line 526, in run
2:     raise CalledProcessError(retcode, process.args,
2: subprocess.CalledProcessError: Command '['torchrun', '--nnodes', '8', '--node_rank', '2', '--nproc_per_node', '4', '--master_addr', 'nid007116', '--master_port', '29500', '/workspace/LLaMA-Factory/src/llamafactory/launcher.py', 'examples/train_full/qwen2_5_vl_full_sft.yaml']' returned non-zero exit status 1.
2: Exception ignored in atexit callback: <function dump_compile_times at 0x40017dc839a0>
2: Traceback (most recent call last):
2:   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 337, in dump_compile_times
2:   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 324, in compile_times
2:   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 129, in tabulate
2:   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
2:   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
2:   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
2:   File "<frozen importlib._bootstrap_external>", line 879, in exec_module
2:   File "<frozen importlib._bootstrap_external>", line 1016, in get_code
2:   File "<frozen importlib._bootstrap_external>", line 1073, in get_data
2: OSError: [Errno 107] Transport endpoint is not connected: '/usr/local/lib/python3.10/dist-packages/tabulate/__init__.py'
srun: error: nid007119: task 2: Exited with exit code 1
srun: Terminating StepId=553151.0
0: 2,  4.84s/it] 29%|██▊       | 319/1112 [29:27<1:03:37,  4.81s/it] 29%|██▉       | 320/1112 [29:32<1:07:14,  5.09s/it]                                                     29%|██▉       | 320/1112 [29:32<1:07:14,  5.09s/it] 29%|██▉       | 321/1112 [29:38<1:09:46,  5.29s/it] 29%|██▉       | 322/1112 [29:44<1:11:41,  5.44s/it] 29%|██▉       | 323/1112 [29:49<1:08:53,  5.24s/it] 29%|██▉       | 324/1112 [29:53<1:06:35,  5.07s/it] 29%|██▉       | 325/1112 [29:58<1:05:21,  4.98s/it]slurmstepd: error: *** STEP 553151.0 ON nid007116 CANCELLED AT 2025-07-11T12:18:21 ***
0: W0711 12:18:22.004000 70368977946720 torch/distributed/elastic/agent/server/api.py:704] Received Signals.SIGHUP death signal, shutting down workers
0: W0711 12:18:22.005000 70368977946720 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 202900 closing signal SIGHUP
0: W0711 12:18:22.005000 70368977946720 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 202901 closing signal SIGHUP
0: W0711 12:18:22.005000 70368977946720 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 202902 closing signal SIGHUP
0: W0711 12:18:22.005000 70368977946720 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 202903 closing signal SIGHUP
3: W0711 12:18:22.005000 70369175865440 torch/distributed/elastic/agent/server/api.py:704] Received Signals.SIGHUP death signal, shutting down workers
3: W0711 12:18:22.006000 70369175865440 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 197036 closing signal SIGHUP
3: W0711 12:18:22.006000 70369175865440 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 197037 closing signal SIGHUP
5: W0711 12:18:22.006000 70369139099744 torch/distributed/elastic/agent/server/api.py:704] Received Signals.SIGHUP death signal, shutting down workers
5: W0711 12:18:22.006000 70369139099744 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 66175 closing signal SIGHUP
3: W0711 12:18:22.006000 70369175865440 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 197038 closing signal SIGHUP
6: W0711 12:18:22.006000 70369706707040 torch/distributed/elastic/agent/server/api.py:704] Received Signals.SIGHUP death signal, shutting down workers
5: W0711 12:18:22.006000 70369139099744 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 66176 closing signal SIGHUP
6: W0711 12:18:22.006000 70369706707040 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 108289 closing signal SIGHUP
3: W0711 12:18:22.007000 70369175865440 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 197039 closing signal SIGHUP
5: W0711 12:18:22.007000 70369139099744 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 66177 closing signal SIGHUP
6: W0711 12:18:22.007000 70369706707040 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 108290 closing signal SIGHUP
5: W0711 12:18:22.007000 70369139099744 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 66178 closing signal SIGHUP
6: W0711 12:18:22.007000 70369706707040 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 108291 closing signal SIGHUP
6: W0711 12:18:22.007000 70369706707040 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 108292 closing signal SIGHUP
0: Exception ignored in atexit callback: <function dump_compile_times at 0x4001597c39a0>
0: Traceback (most recent call last):
0:   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 337, in dump_compile_times
0:   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 324, in compile_times
0:   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 129, in tabulate
0:   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
3: Exception ignored in atexit callback: <function dump_compile_times at 0x400185a8f9a0>
3: Traceback (most recent call last):
3:   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 337, in dump_compile_times
0:   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
3:   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 324, in compile_times
3:   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 129, in tabulate
0:   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
3:   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
5: Exception ignored in atexit callback: <function dump_compile_times at 0x4001784739a0>
5: Traceback (most recent call last):
5:   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 337, in dump_compile_times
5:   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 324, in compile_times
5:   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 129, in tabulate
0:   File "<frozen importlib._bootstrap_external>", line 879, in exec_module
5:   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
3:   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
0:   File "<frozen importlib._bootstrap_external>", line 1016, in get_code
3:   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
0:   File "<frozen importlib._bootstrap_external>", line 1073, in get_data
5:   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
3:   File "<frozen importlib._bootstrap_external>", line 879, in exec_module
0: OSError: [Errno 107] Transport endpoint is not connected: '/usr/local/lib/python3.10/dist-packages/tabulate/__init__.py'
5:   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
6: Exception ignored in atexit callback: <function dump_compile_times at 0x40014df739a0>
6: Traceback (most recent call last):
6:   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 337, in dump_compile_times
3:   File "<frozen importlib._bootstrap_external>", line 1016, in get_code
6:   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 324, in compile_times
6:   File "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/utils.py", line 129, in tabulate
5:   File "<frozen importlib._bootstrap_external>", line 879, in exec_module
3:   File "<frozen importlib._bootstrap_external>", line 1073, in get_data
6:   File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
5:   File "<frozen importlib._bootstrap_external>", line 1016, in get_code
3: OSError: [Errno 107] Transport endpoint is not connected: '/usr/local/lib/python3.10/dist-packages/tabulate/__init__.py'
5:   File "<frozen importlib._bootstrap_external>", line 1073, in get_data
6:   File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
5: OSError: [Errno 107] Transport endpoint is not connected: '/usr/local/lib/python3.10/dist-packages/tabulate/__init__.py'
6:   File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
6:   File "<frozen importlib._bootstrap_external>", line 879, in exec_module
6:   File "<frozen importlib._bootstrap_external>", line 1016, in get_code
6:   File "<frozen importlib._bootstrap_external>", line 1073, in get_data
6: OSError: [Errno 107] Transport endpoint is not connected: '/usr/local/lib/python3.10/dist-packages/tabulate/__init__.py'
4: W0711 12:18:22.030000 70369684949088 torch/distributed/elastic/agent/server/api.py:704] Received Signals.SIGHUP death signal, shutting down workers
4: W0711 12:18:22.031000 70369684949088 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 116201 closing signal SIGHUP
4: W0711 12:18:22.031000 70369684949088 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 116202 closing signal SIGHUP
4: W0711 12:18:22.031000 70369684949088 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 116203 closing signal SIGHUP
4: W0711 12:18:22.031000 70369684949088 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 116204 closing signal SIGHUP
7: W0711 12:18:22.033000 70369193887840 torch/distributed/elastic/agent/server/api.py:704] Received Signals.SIGHUP death signal, shutting down workers
7: W0711 12:18:22.034000 70369193887840 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 126395 closing signal SIGHUP
7: W0711 12:18:22.034000 70369193887840 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 126396 closing signal SIGHUP
7: W0711 12:18:22.034000 70369193887840 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 126397 closing signal SIGHUP
7: W0711 12:18:22.034000 70369193887840 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 126398 closing signal SIGHUP
1: W0711 12:18:22.035000 70369164920928 torch/distributed/elastic/agent/server/api.py:704] Received Signals.SIGHUP death signal, shutting down workers
1: W0711 12:18:22.035000 70369164920928 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 210405 closing signal SIGHUP
1: W0711 12:18:22.036000 70369164920928 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 210406 closing signal SIGHUP
1: W0711 12:18:22.036000 70369164920928 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 210407 closing signal SIGHUP
1: W0711 12:18:22.036000 70369164920928 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 210408 closing signal SIGHUP
srun: error: nid007122: task 5: Terminated
srun: error: nid007124: task 7: Terminated
srun: error: nid007120: task 3: Terminated
srun: error: nid007123: task 6: Terminated
srun: error: nid007116: task 0: Terminated
srun: error: nid007117: task 1: Terminated
srun: error: nid007121: task 4: Terminated
srun: Force Terminated StepId=553151.0
